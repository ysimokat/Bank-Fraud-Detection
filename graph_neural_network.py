#!/usr/bin/env python3
"""
Graph Neural Networks for Credit Card Fraud Detection
====================================================

This implements GNN to analyze transaction networks and detect fraud patterns
through relationship analysis between cardholders, merchants, and transactions.
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GCNConv, GATConv, global_mean_pool, global_max_pool
from torch_geometric.data import Data, DataLoader
import networkx as nx
from sklearn.preprocessing import StandardScaler
import joblib
from datetime import datetime, timedelta

# Import GPU configuration
from gpu_config import gpu_config, get_device

class TransactionGraphBuilder:
    """Build graph representation of transactions."""
    
    def __init__(self, time_window_hours=24):
        # OPTIMIZATION: Drastically reduce time window for faster processing
        self.time_window = min(time_window_hours, 0.5) * 3600  # Max 30 minutes
        self.scaler = StandardScaler()
        
    def build_transaction_graph(self, df, target_idx):
        """
        Build a graph centered around a target transaction.
        
        Nodes represent:
        - Transactions
        - Cards (anonymized)
        - Merchants (derived from patterns)
        
        Edges represent:
        - Card-Transaction relationships
        - Temporal relationships
        - Amount similarity relationships
        """
        # Use label-based indexing for consistency
        target_row = df.loc[target_idx]
        target_time = target_row['Time']
        
        # Get transactions within time window
        time_mask = (df['Time'] >= target_time - self.time_window) & \
                   (df['Time'] <= target_time + self.time_window)
        local_df = df[time_mask].copy()
        
        # OPTIMIZATION: Limit number of nodes
        max_nodes = 100
        if len(local_df) > max_nodes:
            # Sample most relevant transactions
            large_amounts = local_df.nlargest(max_nodes//2, 'Amount')
            small_amounts = local_df.nsmallest(max_nodes//2, 'Amount')
            local_df = pd.concat([large_amounts, small_amounts]).drop_duplicates()
        
        # Create graph
        G = nx.Graph()
        
        # Add transaction nodes
        for idx, row in local_df.iterrows():
            node_features = self._extract_node_features(row)
            G.add_node(f'txn_{idx}', 
                      type='transaction',
                      features=node_features,
                      is_fraud=row['Class'],
                      is_target=(idx == target_idx))
        
        # Add edges based on relationships
        self._add_similarity_edges(G, local_df)
        self._add_temporal_edges(G, local_df)
        self._add_pattern_edges(G, local_df)
        
        return G
    
    def _extract_node_features(self, row):
        """Extract features for a transaction node."""
        features = []
        
        # Basic features
        features.append(row['Amount'])
        features.append(np.log(row['Amount'] + 1))
        
        # Time features
        hour = (row['Time'] % (24 * 3600)) // 3600
        features.append(np.sin(2 * np.pi * hour / 24))
        features.append(np.cos(2 * np.pi * hour / 24))
        
        # PCA features (top 10)
        for i in range(1, 11):
            if f'V{i}' in row:
                features.append(row[f'V{i}'])
        
        return np.array(features)
    
    def _add_similarity_edges(self, G, df):
        """Add edges between similar transactions."""
        amounts = df['Amount'].values
        indices = df.index.tolist()
        
        # OPTIMIZATION: Limit comparisons to avoid O(n¬≤) complexity
        max_edges_per_node = 5
        
        # Sort by amount for efficient similarity search
        sorted_idx = np.argsort(amounts)
        
        for i in range(len(sorted_idx)):
            edges_added = 0
            # Only check nearby transactions in sorted order
            for j in range(i + 1, min(i + 10, len(sorted_idx))):
                if edges_added >= max_edges_per_node:
                    break
                    
                idx_i = indices[sorted_idx[i]]
                idx_j = indices[sorted_idx[j]]
                
                # Similar amounts (within 10%)
                if abs(amounts[sorted_idx[i]] - amounts[sorted_idx[j]]) / (max(amounts[sorted_idx[i]], amounts[sorted_idx[j]]) + 1e-6) < 0.1:
                    G.add_edge(f'txn_{idx_i}', f'txn_{idx_j}', 
                              weight=1.0, type='amount_similarity')
                    edges_added += 1
    
    def _add_temporal_edges(self, G, df):
        """Add edges between temporally close transactions."""
        times = df['Time'].values
        indices = df.index.tolist()
        
        # OPTIMIZATION: Use sorted time indices for efficient search
        sorted_time_idx = np.argsort(times)
        max_edges_per_node = 5
        
        for i in range(len(sorted_time_idx)):
            edges_added = 0
            # Only check transactions close in time
            for j in range(i + 1, min(i + 20, len(sorted_time_idx))):
                if edges_added >= max_edges_per_node:
                    break
                    
                idx_i = indices[sorted_time_idx[i]]
                idx_j = indices[sorted_time_idx[j]]
                
                time_diff = abs(times[sorted_time_idx[i]] - times[sorted_time_idx[j]])
                if time_diff < 300:  # Within 5 minutes
                    weight = 1.0 - (time_diff / 300)
                    G.add_edge(f'txn_{idx_i}', f'txn_{idx_j}', 
                              weight=weight, type='temporal')
                    edges_added += 1
                else:
                    # Times are sorted, so no more close transactions
                    break
    
    def _add_pattern_edges(self, G, df):
        """Add edges based on transaction patterns."""
        # OPTIMIZATION: Only use top 2 PCA components and limit edges
        pca_cols = [col for col in df.columns if col.startswith('V')][:2]
        
        for col in pca_cols:
            # Discretize into bins
            try:
                bins = pd.qcut(df[col], q=3, duplicates='drop')  # Fewer bins
            except:
                continue
            
            for bin_label in bins.unique():
                nodes_in_bin = df[bins == bin_label].index
                
                # Limit connections per bin
                max_connections = min(10, len(nodes_in_bin))
                
                # Connect only a few transactions in same bin
                for i in range(min(5, len(nodes_in_bin))):
                    for j in range(i + 1, min(i + 2, max_connections)):
                        if j < len(nodes_in_bin):
                            G.add_edge(f'txn_{nodes_in_bin[i]}', f'txn_{nodes_in_bin[j]}',
                                      weight=0.5, type='pattern')

class GraphAttentionFraudDetector(nn.Module):
    """
    Graph Attention Network for fraud detection.
    
    Uses attention mechanisms to focus on important transaction relationships.
    """
    
    def __init__(self, input_dim, hidden_dim=64, num_heads=4, dropout=0.2):
        super().__init__()
        
        # Graph attention layers
        self.gat1 = GATConv(input_dim, hidden_dim, heads=num_heads, dropout=dropout)
        self.gat2 = GATConv(hidden_dim * num_heads, hidden_dim, heads=num_heads, dropout=dropout)
        self.gat3 = GATConv(hidden_dim * num_heads, hidden_dim, heads=1, dropout=dropout)
        
        # Global graph features
        self.global_attention = nn.Linear(hidden_dim, 1)
        
        # Classification head
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim * 3, 128),  # node + global features
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(64, 2)  # Binary classification
        )
    
    def forward(self, x, edge_index, batch, target_mask):
        # Graph attention layers
        x = F.relu(self.gat1(x, edge_index))
        x = F.relu(self.gat2(x, edge_index))
        x = self.gat3(x, edge_index)
        
        # Global graph representation
        global_max = global_max_pool(x, batch)
        global_mean = global_mean_pool(x, batch)
        
        # Attention-weighted global features
        attention_weights = torch.softmax(self.global_attention(x), dim=0)
        global_attention = torch.sum(x * attention_weights, dim=0, keepdim=True)
        
        # Get target node features
        target_features = x[target_mask]
        
        # Combine features
        combined = torch.cat([target_features, global_max, global_mean, global_attention], dim=1)
        
        # Classification
        output = self.classifier(combined)
        
        return output

class HybridGNNFraudDetector:
    """
    Hybrid system combining GNN with traditional models.
    """
    
    def __init__(self):
        self.graph_builder = TransactionGraphBuilder()
        self.gnn_model = None
        self.traditional_models = {}
        self.ensemble_weights = None
        
    def train(self, df, traditional_models):
        """Train the hybrid system."""
        print("üåê Training Graph Neural Network for Fraud Detection...")
        
        # Print GPU configuration
        gpu_config.print_config()
        
        self.traditional_models = traditional_models
        
        # Prepare graph data
        graph_data = self._prepare_graph_data(df)
        
        # Train GNN
        self.gnn_model = self._train_gnn(graph_data)
        
        # Learn ensemble weights
        self._learn_ensemble_weights(df)
        
        print("‚úÖ Hybrid GNN system trained successfully")
        
        # Generate insights from top transactions
        self._generate_graph_insights(df)
    
    def _prepare_graph_data(self, df, sample_size=10000):
        """Prepare graph data for training."""
        print("üìä Analyzing top transactions with Graph Neural Network...")
        
        # OPTIMIZATION: Only analyze most important transactions
        top_transactions = 20  # Just show top fraud/normal examples
        
        graphs = []
        
        # Get highest amount frauds and normal transactions
        fraud_df = df[df['Class'] == 1].nlargest(top_transactions//2, 'Amount')
        normal_df = df[df['Class'] == 0].nlargest(top_transactions//2, 'Amount')
        
        print(f"\nüéØ Analyzing {top_transactions} most significant transactions:")
        print(f"   - Top {top_transactions//2} fraud transactions (by amount)")
        print(f"   - Top {top_transactions//2} normal transactions (by amount)")
        
        # Build graphs for top fraud transactions
        print("\nüìä Building fraud transaction graphs...")
        for i, (idx, row) in enumerate(fraud_df.iterrows()):
            print(f"   Fraud #{i+1}: Amount=${row['Amount']:.2f} at Time={row['Time']:.0f}s")
            G = self.graph_builder.build_transaction_graph(df, idx)
            data = self._networkx_to_geometric(G, idx)
            graphs.append(data)
            
            # Show graph statistics
            print(f"      ‚Üí Graph: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges")
        
        # Build graphs for top normal transactions  
        print("\nüìä Building normal transaction graphs...")
        for i, (idx, row) in enumerate(normal_df.iterrows()):
            print(f"   Normal #{i+1}: Amount=${row['Amount']:.2f} at Time={row['Time']:.0f}s")
            G = self.graph_builder.build_transaction_graph(df, idx)
            data = self._networkx_to_geometric(G, idx)
            graphs.append(data)
            
            # Show graph statistics
            print(f"      ‚Üí Graph: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges")
        
        print(f"\n‚úÖ Built {len(graphs)} transaction graphs for analysis")
        
        return graphs
    
    def _networkx_to_geometric(self, G, target_idx):
        """Convert NetworkX graph to PyTorch Geometric format."""
        # Node features
        node_features = []
        node_to_idx = {}
        target_mask = []
        
        for i, (node, attrs) in enumerate(G.nodes(data=True)):
            node_to_idx[node] = i
            if 'features' in attrs:
                node_features.append(attrs['features'])
            target_mask.append(attrs.get('is_target', False))
        
        x = torch.tensor(np.array(node_features), dtype=torch.float)
        
        # Edges
        edge_index = []
        edge_attr = []
        
        for u, v, attrs in G.edges(data=True):
            edge_index.append([node_to_idx[u], node_to_idx[v]])
            edge_index.append([node_to_idx[v], node_to_idx[u]])  # Undirected
            
            weight = attrs.get('weight', 1.0)
            edge_attr.extend([weight, weight])
        
        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()
        edge_attr = torch.tensor(edge_attr, dtype=torch.float)
        
        # Target
        target_node = list(G.nodes())[target_mask.index(True)]
        y = torch.tensor([G.nodes[target_node].get('is_fraud', 0)], dtype=torch.long)
        
        # Create data object
        data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)
        data.target_mask = torch.tensor(target_mask, dtype=torch.bool)
        
        return data
    
    def _train_gnn(self, graph_data):
        """Train the GNN model."""
        print("üß† Training Graph Attention Network...")
        
        # Split data
        train_size = int(0.8 * len(graph_data))
        train_data = graph_data[:train_size]
        val_data = graph_data[train_size:]
        
        # Get optimal batch size based on GPU
        batch_size = gpu_config.get_optimal_batch_size('deep')
        
        # Create data loaders
        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)
        
        # Initialize model
        input_dim = graph_data[0].x.shape[1]
        model = GraphAttentionFraudDetector(input_dim).to(gpu_config.get_device())
        
        # Training setup
        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
        criterion = nn.CrossEntropyLoss()
        
        # Move data to device
        device = gpu_config.get_device()
        
        # Training loop - OPTIMIZATION: Reduce epochs
        model.train()
        num_epochs = 20  # Reduced from 50
        
        for epoch in range(num_epochs):
            total_loss = 0
            
            for batch in train_loader:
                optimizer.zero_grad()
                
                # Move batch to device
                batch = batch.to(device)
                
                out = model(batch.x, batch.edge_index, batch.batch, batch.target_mask)
                loss = criterion(out, batch.y)
                
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()
            
            if (epoch + 1) % 5 == 0:
                print(f"   Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader):.4f}")
        
        return model
    
    def predict_with_explanation(self, transaction_df, idx):
        """Make prediction with graph-based explanation."""
        # Build transaction graph
        G = self.graph_builder.build_transaction_graph(transaction_df, idx)
        
        # Get GNN prediction
        data = self._networkx_to_geometric(G, idx)
        self.gnn_model.eval()
        
        with torch.no_grad():
            gnn_output = self.gnn_model(data.x, data.edge_index, 
                                       torch.zeros(data.x.shape[0], dtype=torch.long),
                                       data.target_mask)
            gnn_pred = torch.softmax(gnn_output, dim=1)[0, 1].item()
        
        # Get traditional model predictions
        traditional_preds = {}
        for name, model in self.traditional_models.items():
            # Simplified - would need proper feature extraction
            traditional_preds[name] = 0.5  # Placeholder
        
        # Ensemble prediction
        final_pred = self._ensemble_predict(gnn_pred, traditional_preds)
        
        # Generate explanation
        explanation = self._generate_graph_explanation(G, gnn_pred, final_pred)
        
        return final_pred, explanation
    
    def _ensemble_predict(self, gnn_pred, traditional_preds):
        """Combine predictions using learned weights."""
        if self.ensemble_weights is None:
            # Simple average if weights not learned
            all_preds = [gnn_pred] + list(traditional_preds.values())
            return np.mean(all_preds)
        
        # Weighted combination
        weighted_sum = self.ensemble_weights['gnn'] * gnn_pred
        for name, pred in traditional_preds.items():
            weighted_sum += self.ensemble_weights.get(name, 0) * pred
        
        return weighted_sum
    
    def _generate_graph_insights(self, df):
        """Generate insights from graph analysis of top transactions."""
        print("\n" + "="*60)
        print("üîç GRAPH NEURAL NETWORK INSIGHTS")
        print("="*60)
        
        # Analyze fraud network patterns
        fraud_df = df[df['Class'] == 1]
        normal_df = df[df['Class'] == 0]
        
        print("\nüìä Transaction Network Patterns:")
        
        # Time clustering analysis
        fraud_times = fraud_df['Time'].values
        fraud_time_diffs = np.diff(np.sort(fraud_times))
        burst_threshold = 300  # 5 minutes
        fraud_bursts = np.sum(fraud_time_diffs < burst_threshold)
        
        print(f"\n‚è∞ Temporal Patterns:")
        print(f"   - Fraud transactions in bursts (within 5 min): {fraud_bursts}")
        print(f"   - Average time between frauds: {np.mean(fraud_time_diffs):.1f} seconds")
        print(f"   - Fraud concentration hours: ", end="")
        
        # Hour of day analysis
        fraud_hours = ((fraud_df['Time'] % (24 * 3600)) // 3600).value_counts().sort_index()
        top_fraud_hours = fraud_hours.nlargest(3).index.tolist()
        print(f"{top_fraud_hours}")
        
        # Amount patterns
        print(f"\nüí∞ Amount Patterns:")
        print(f"   - Fraud amount range: ${fraud_df['Amount'].min():.2f} - ${fraud_df['Amount'].max():.2f}")
        print(f"   - Most common fraud amounts: ", end="")
        common_amounts = fraud_df['Amount'].value_counts().head(3)
        for amt, count in common_amounts.items():
            print(f"${amt:.2f} ({count}x), ", end="")
        print()
        
        # Network density analysis
        print(f"\nüåê Network Characteristics:")
        print(f"   - Frauds often occur in clusters with similar:")
        print(f"     ‚Ä¢ Transaction amounts (¬±10%)")
        print(f"     ‚Ä¢ Time windows (5-minute bursts)")
        print(f"     ‚Ä¢ PCA feature patterns")
        
        # Recommendations
        print(f"\nüí° GNN Detection Strategy:")
        print(f"   - Focus on transactions with dense local networks")
        print(f"   - Monitor burst patterns in 5-minute windows")
        print(f"   - Flag amount repetitions and round numbers")
        print(f"   - Track PCA feature similarities in transaction clusters")
        
        print("\n" + "="*60)
    
    def _learn_ensemble_weights(self, df):
        """Learn optimal ensemble weights."""
        # Simplified - in practice, use validation set
        self.ensemble_weights = {
            'gnn': 0.3,
            'random_forest': 0.4,
            'xgboost': 0.3
        }
    
    def _generate_graph_explanation(self, G, gnn_pred, final_pred):
        """Generate explanation based on graph structure."""
        explanation = {
            'prediction': 'Fraud' if final_pred > 0.5 else 'Normal',
            'confidence': final_pred,
            'gnn_contribution': gnn_pred,
            'graph_stats': {
                'num_nodes': G.number_of_nodes(),
                'num_edges': G.number_of_edges(),
                'connected_frauds': sum(1 for n, d in G.nodes(data=True) 
                                      if d.get('is_fraud', 0) == 1)
            },
            'important_connections': []
        }
        
        # Find important connections
        target_node = [n for n, d in G.nodes(data=True) if d.get('is_target', False)][0]
        
        for neighbor in G.neighbors(target_node):
            if G.nodes[neighbor].get('is_fraud', 0) == 1:
                explanation['important_connections'].append({
                    'type': 'connected_to_fraud',
                    'edge_type': G.edges[target_node, neighbor].get('type', 'unknown'),
                    'weight': G.edges[target_node, neighbor].get('weight', 1.0)
                })
        
        return explanation

def demonstrate_gnn():
    """Demonstrate GNN capabilities."""
    print("üåê Graph Neural Network Fraud Detection Demo")
    print("=" * 50)
    
    # Load data
    df = pd.read_csv('/mnt/c/Users/Helen/Desktop/CodeMonkey/AI_Practice/Bank_Fraud_Detection/creditcard.csv')
    
    # Build sample graph
    builder = TransactionGraphBuilder()
    
    # Find a fraud transaction
    fraud_idx = df[df['Class'] == 1].index[0]
    
    print(f"\nüìä Building graph for transaction {fraud_idx}...")
    G = builder.build_transaction_graph(df, fraud_idx)
    
    print(f"‚úÖ Graph built with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges")
    
    # Analyze graph properties
    print("\nüîç Graph Analysis:")
    print(f"   ‚Ä¢ Connected fraud transactions: {sum(1 for n, d in G.nodes(data=True) if d.get('is_fraud', 0) == 1)}")
    print(f"   ‚Ä¢ Average degree: {np.mean([d for n, d in G.degree()]):.2f}")
    print(f"   ‚Ä¢ Graph density: {nx.density(G):.4f}")
    
    # Show relationships
    target_node = [n for n, d in G.nodes(data=True) if d.get('is_target', False)][0]
    neighbors = list(G.neighbors(target_node))
    
    print(f"\nüîó Target transaction connections:")
    for neighbor in neighbors[:5]:  # Show first 5
        edge_data = G.edges[target_node, neighbor]
        print(f"   ‚Ä¢ Connected to {neighbor}: {edge_data.get('type', 'unknown')} (weight: {edge_data.get('weight', 1.0):.2f})")
    
    print("\nüí° GNN Advantages:")
    print("   ‚Ä¢ Captures transaction relationships and patterns")
    print("   ‚Ä¢ Identifies fraud rings and coordinated attacks")
    print("   ‚Ä¢ Learns from network topology")
    print("   ‚Ä¢ Provides explainable connections")

if __name__ == "__main__":
    demonstrate_gnn()