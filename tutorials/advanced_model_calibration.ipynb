{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Model Calibration and Threshold Optimization Tutorial üéØ\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By the end of this tutorial, you will understand:\n",
    "\n",
    "1. **Model Calibration** - Why and how to calibrate probability predictions\n",
    "2. **Calibration Methods** - Platt scaling vs Isotonic regression\n",
    "3. **Threshold Optimization** - Finding optimal decision thresholds based on business costs\n",
    "4. **Dynamic Thresholds** - Context-aware thresholds for different scenarios\n",
    "5. **Multi-objective Optimization** - Balancing multiple business goals\n",
    "\n",
    "## üìã What This File Does\n",
    "The `advanced_model_calibration.py` file implements:\n",
    "\n",
    "**üéØ Model Calibration:**\n",
    "- Platt scaling (sigmoid calibration)\n",
    "- Isotonic regression (non-parametric)\n",
    "- Calibration evaluation metrics (ECE, MCE)\n",
    "- Cross-validated calibration\n",
    "\n",
    "**üí∞ Threshold Optimization:**\n",
    "- Cost-sensitive threshold selection\n",
    "- Pareto-optimal thresholds\n",
    "- Dynamic thresholds by context\n",
    "- Business impact analysis\n",
    "\n",
    "**üìä Evaluation Metrics:**\n",
    "- Expected Calibration Error (ECE)\n",
    "- Maximum Calibration Error (MCE)\n",
    "- Brier score\n",
    "- Reliability diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Calibration imports\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_curve, roc_curve, average_precision_score,\n",
    "    brier_score_loss, log_loss, precision_score, recall_score, \n",
    "    f1_score, confusion_matrix, roc_auc_score\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Advanced Model Calibration Tutorial\")\n",
    "print(f\"üìÖ Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Model Calibration\n",
    "\n",
    "Model calibration ensures that predicted probabilities reflect true probabilities. A well-calibrated model that predicts 70% probability should be correct 70% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate calibration concept\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Perfect calibration\n",
    "perfect_x = np.linspace(0, 1, 100)\n",
    "perfect_y = perfect_x\n",
    "ax1.plot(perfect_x, perfect_y, 'g-', linewidth=3, label='Perfect Calibration')\n",
    "ax1.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "ax1.set_xlabel('Mean Predicted Probability')\n",
    "ax1.set_ylabel('Fraction of Positives')\n",
    "ax1.set_title('Well-Calibrated Model', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Poor calibration examples\n",
    "# Overconfident model\n",
    "overconfident_x = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "overconfident_y = np.array([0.05, 0.1, 0.15, 0.25, 0.5, 0.75, 0.85, 0.9, 0.95])\n",
    "ax2.plot(overconfident_x, overconfident_y, 'r-', linewidth=3, label='Overconfident', marker='o')\n",
    "\n",
    "# Underconfident model\n",
    "underconfident_x = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "underconfident_y = np.array([0.15, 0.25, 0.35, 0.45, 0.5, 0.55, 0.65, 0.75, 0.85])\n",
    "ax2.plot(underconfident_x, underconfident_y, 'b-', linewidth=3, label='Underconfident', marker='s')\n",
    "\n",
    "ax2.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Perfect')\n",
    "ax2.set_xlabel('Mean Predicted Probability')\n",
    "ax2.set_ylabel('Fraction of Positives')\n",
    "ax2.set_title('Poorly Calibrated Models', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Calibration Importance:\")\n",
    "print(\"  ‚Ä¢ Well-calibrated models provide reliable probability estimates\")\n",
    "print(\"  ‚Ä¢ Overconfident models predict extreme probabilities too often\")\n",
    "print(\"  ‚Ä¢ Underconfident models predict probabilities too close to 0.5\")\n",
    "print(\"  ‚Ä¢ Calibration is crucial for threshold-based decisions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Data and Training Base Models\n",
    "\n",
    "Let's load our data and train some base models that we'll calibrate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('../creditcard.csv')\n",
    "\n",
    "print(\"üìä Dataset Information:\")\n",
    "print(f\"Total transactions: {len(df):,}\")\n",
    "print(f\"Fraud rate: {df['Class'].mean()*100:.3f}%\")\n",
    "\n",
    "# Prepare features\n",
    "feature_columns = [col for col in df.columns if col != 'Class']\n",
    "X = df[feature_columns]\n",
    "y = df['Class']\n",
    "\n",
    "# Create train-calibration-test split\n",
    "# First split: separate test set\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Second split: separate calibration set from training\n",
    "X_train, X_cal, y_train, y_cal = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, stratify=y_temp, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nüìÇ Data Splits:\")\n",
    "print(f\"Training set: {len(X_train):,} samples ({y_train.sum()} frauds)\")\n",
    "print(f\"Calibration set: {len(X_cal):,} samples ({y_cal.sum()} frauds)\")\n",
    "print(f\"Test set: {len(X_test):,} samples ({y_test.sum()} frauds)\")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_cal_scaled = scaler.transform(X_cal)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train base models\n",
    "print(\"ü§ñ Training Base Models...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "base_models = {}\n",
    "\n",
    "# 1. Random Forest\n",
    "print(\"\\nüå≤ Training Random Forest...\")\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100, \n",
    "    random_state=42, \n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "base_models['Random Forest'] = rf\n",
    "\n",
    "# 2. XGBoost\n",
    "print(\"üìä Training XGBoost...\")\n",
    "scale_pos_weight = len(y_train[y_train == 0]) / len(y_train[y_train == 1])\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    scale_pos_weight=scale_pos_weight\n",
    ")\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "base_models['XGBoost'] = xgb_model\n",
    "\n",
    "# 3. Logistic Regression\n",
    "print(\"üìà Training Logistic Regression...\")\n",
    "lr = LogisticRegression(\n",
    "    random_state=42,\n",
    "    class_weight='balanced',\n",
    "    max_iter=1000\n",
    ")\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "base_models['Logistic Regression'] = lr\n",
    "\n",
    "# Evaluate uncalibrated models\n",
    "print(\"\\nüìä Uncalibrated Model Performance:\")\n",
    "for name, model in base_models.items():\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    brier = brier_score_loss(y_test, y_proba)\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  F1-Score: {f1:.4f}\")\n",
    "    print(f\"  ROC-AUC: {auc:.4f}\")\n",
    "    print(f\"  Brier Score: {brier:.4f} (lower is better)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualizing Calibration Before Calibration\n",
    "\n",
    "Let's check how well-calibrated our models are before calibration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot calibration curves for uncalibrated models\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (name, model) in enumerate(base_models.items()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Get predictions\n",
    "    y_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    # Calculate calibration curve\n",
    "    fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "        y_test, y_proba, n_bins=10, strategy='uniform'\n",
    "    )\n",
    "    \n",
    "    # Plot\n",
    "    ax.plot(mean_predicted_value, fraction_of_positives, 's-', \n",
    "            label=name, markersize=8, linewidth=2)\n",
    "    ax.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n",
    "    \n",
    "    # Calculate ECE\n",
    "    ece = np.mean(np.abs(fraction_of_positives - mean_predicted_value))\n",
    "    \n",
    "    ax.set_xlabel('Mean Predicted Probability')\n",
    "    ax.set_ylabel('Fraction of Positives')\n",
    "    ax.set_title(f'{name}\\nECE: {ece:.4f}', fontsize=12, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Calibration Curves - Before Calibration', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Observations:\")\n",
    "print(\"  ‚Ä¢ Random Forest often shows overconfidence (S-shaped curve)\")\n",
    "print(\"  ‚Ä¢ XGBoost may need calibration depending on hyperparameters\")\n",
    "print(\"  ‚Ä¢ Logistic Regression is often well-calibrated by default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Calibration Methods\n",
    "\n",
    "Let's apply two calibration methods: Platt Scaling and Isotonic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate calibration metrics\n",
    "def calculate_calibration_metrics(y_true, y_prob, n_bins=10):\n",
    "    \"\"\"Calculate various calibration metrics\"\"\"\n",
    "    # Brier score\n",
    "    brier = brier_score_loss(y_true, y_prob)\n",
    "    \n",
    "    # Log loss\n",
    "    logloss = log_loss(y_true, y_prob)\n",
    "    \n",
    "    # Expected Calibration Error (ECE)\n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_lowers = bin_boundaries[:-1]\n",
    "    bin_uppers = bin_boundaries[1:]\n",
    "    \n",
    "    ece = 0\n",
    "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "        in_bin = (y_prob > bin_lower) & (y_prob <= bin_upper)\n",
    "        prop_in_bin = in_bin.mean()\n",
    "        \n",
    "        if prop_in_bin > 0:\n",
    "            accuracy_in_bin = y_true[in_bin].mean()\n",
    "            avg_confidence_in_bin = y_prob[in_bin].mean()\n",
    "            ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "    \n",
    "    # Maximum Calibration Error (MCE)\n",
    "    mce = 0\n",
    "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "        in_bin = (y_prob > bin_lower) & (y_prob <= bin_upper)\n",
    "        \n",
    "        if in_bin.sum() > 0:\n",
    "            accuracy_in_bin = y_true[in_bin].mean()\n",
    "            avg_confidence_in_bin = y_prob[in_bin].mean()\n",
    "            error = np.abs(avg_confidence_in_bin - accuracy_in_bin)\n",
    "            mce = max(mce, error)\n",
    "    \n",
    "    return {\n",
    "        'brier_score': brier,\n",
    "        'log_loss': logloss,\n",
    "        'ece': ece,\n",
    "        'mce': mce\n",
    "    }\n",
    "\n",
    "# Apply calibration methods\n",
    "calibrated_models = {}\n",
    "calibration_results = {}\n",
    "\n",
    "calibration_methods = ['sigmoid', 'isotonic']\n",
    "\n",
    "print(\"üéØ Calibrating Models...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for model_name, model in base_models.items():\n",
    "    print(f\"\\nüìä Calibrating {model_name}\")\n",
    "    \n",
    "    calibrated_models[model_name] = {}\n",
    "    calibration_results[model_name] = {'uncalibrated': {}}\n",
    "    \n",
    "    # Evaluate uncalibrated model\n",
    "    y_prob_uncal = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    uncal_metrics = calculate_calibration_metrics(y_test, y_prob_uncal)\n",
    "    calibration_results[model_name]['uncalibrated'] = uncal_metrics\n",
    "    \n",
    "    print(f\"  Uncalibrated - ECE: {uncal_metrics['ece']:.4f}, Brier: {uncal_metrics['brier_score']:.4f}\")\n",
    "    \n",
    "    for method in calibration_methods:\n",
    "        # Create calibrated classifier\n",
    "        calibrated_clf = CalibratedClassifierCV(\n",
    "            base_estimator=model,\n",
    "            method=method,\n",
    "            cv=3,  # 3-fold cross-validation\n",
    "            ensemble=True\n",
    "        )\n",
    "        \n",
    "        # Fit on calibration set\n",
    "        calibrated_clf.fit(X_cal_scaled, y_cal)\n",
    "        \n",
    "        # Evaluate\n",
    "        y_prob_cal = calibrated_clf.predict_proba(X_test_scaled)[:, 1]\n",
    "        cal_metrics = calculate_calibration_metrics(y_test, y_prob_cal)\n",
    "        \n",
    "        calibrated_models[model_name][method] = calibrated_clf\n",
    "        calibration_results[model_name][method] = cal_metrics\n",
    "        \n",
    "        print(f\"  {method.title()} - ECE: {cal_metrics['ece']:.4f}, Brier: {cal_metrics['brier_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizing Calibration Results\n",
    "\n",
    "Let's compare calibration curves before and after calibration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive calibration comparison\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "\n",
    "for i, (model_name, model) in enumerate(base_models.items()):\n",
    "    # Uncalibrated\n",
    "    ax = axes[i, 0]\n",
    "    y_prob = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    fraction_pos, mean_pred = calibration_curve(y_test, y_prob, n_bins=10)\n",
    "    ax.plot(mean_pred, fraction_pos, 's-', label='Uncalibrated', markersize=8)\n",
    "    ax.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "    ax.set_title(f'{model_name} - Uncalibrated', fontweight='bold')\n",
    "    ax.set_xlabel('Mean Predicted Probability')\n",
    "    ax.set_ylabel('Fraction of Positives')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    \n",
    "    # Platt Scaling\n",
    "    ax = axes[i, 1]\n",
    "    cal_model = calibrated_models[model_name]['sigmoid']\n",
    "    y_prob_cal = cal_model.predict_proba(X_test_scaled)[:, 1]\n",
    "    fraction_pos, mean_pred = calibration_curve(y_test, y_prob_cal, n_bins=10)\n",
    "    ax.plot(mean_pred, fraction_pos, 's-', label='Platt Scaling', markersize=8, color='green')\n",
    "    ax.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "    ax.set_title(f'{model_name} - Platt Scaling', fontweight='bold')\n",
    "    ax.set_xlabel('Mean Predicted Probability')\n",
    "    ax.set_ylabel('Fraction of Positives')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    \n",
    "    # Isotonic Regression\n",
    "    ax = axes[i, 2]\n",
    "    cal_model = calibrated_models[model_name]['isotonic']\n",
    "    y_prob_cal = cal_model.predict_proba(X_test_scaled)[:, 1]\n",
    "    fraction_pos, mean_pred = calibration_curve(y_test, y_prob_cal, n_bins=10)\n",
    "    ax.plot(mean_pred, fraction_pos, 's-', label='Isotonic', markersize=8, color='orange')\n",
    "    ax.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "    ax.set_title(f'{model_name} - Isotonic Regression', fontweight='bold')\n",
    "    ax.set_xlabel('Mean Predicted Probability')\n",
    "    ax.set_ylabel('Fraction of Positives')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "\n",
    "plt.suptitle('Calibration Comparison: Before and After', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show calibration metrics comparison\n",
    "print(\"üìä Calibration Metrics Comparison\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for model_name in base_models.keys():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"{'Method':<15} {'ECE':>10} {'MCE':>10} {'Brier':>10} {'Log Loss':>10}\")\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    for method, metrics in calibration_results[model_name].items():\n",
    "        print(f\"{method:<15} {metrics['ece']:>10.4f} {metrics['mce']:>10.4f} \"\n",
    "              f\"{metrics['brier_score']:>10.4f} {metrics['log_loss']:>10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Business-Optimal Threshold Selection\n",
    "\n",
    "Now let's find optimal decision thresholds based on business costs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define business costs\n",
    "class BusinessCosts:\n",
    "    def __init__(self):\n",
    "        self.fraud_cost = 200  # Cost of missing a fraud\n",
    "        self.false_positive_cost = 10  # Cost of false alarm\n",
    "        self.investigation_cost = 30  # Cost to investigate\n",
    "\n",
    "costs = BusinessCosts()\n",
    "\n",
    "print(\"üí∞ Business Cost Structure:\")\n",
    "print(f\"  ‚Ä¢ Missing a fraud (FN): ${costs.fraud_cost}\")\n",
    "print(f\"  ‚Ä¢ False alarm (FP): ${costs.false_positive_cost}\")\n",
    "print(f\"  ‚Ä¢ Investigation per alert: ${costs.investigation_cost}\")\n",
    "\n",
    "# Function to find optimal threshold\n",
    "def find_optimal_threshold(y_true, y_prob, costs):\n",
    "    \"\"\"Find threshold that minimizes total business cost\"\"\"\n",
    "    thresholds = np.arange(0.01, 1.0, 0.01)\n",
    "    total_costs = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_prob >= threshold).astype(int)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        \n",
    "        # Calculate costs\n",
    "        fraud_loss = fn * costs.fraud_cost\n",
    "        false_alarm_cost = fp * costs.false_positive_cost\n",
    "        investigation_cost = (tp + fp) * costs.investigation_cost\n",
    "        total_cost = fraud_loss + false_alarm_cost + investigation_cost\n",
    "        \n",
    "        total_costs.append(total_cost)\n",
    "    \n",
    "    # Find minimum cost threshold\n",
    "    optimal_idx = np.argmin(total_costs)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    optimal_cost = total_costs[optimal_idx]\n",
    "    \n",
    "    return optimal_threshold, optimal_cost, thresholds, total_costs\n",
    "\n",
    "# Find optimal thresholds for best calibrated models\n",
    "threshold_results = {}\n",
    "\n",
    "print(\"\\nüéØ Finding Optimal Thresholds...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for model_name in base_models.keys():\n",
    "    # Get best calibrated model (lowest ECE)\n",
    "    best_method = min(calibration_results[model_name].keys(), \n",
    "                     key=lambda x: calibration_results[model_name][x]['ece'])\n",
    "    \n",
    "    if best_method == 'uncalibrated':\n",
    "        model = base_models[model_name]\n",
    "    else:\n",
    "        model = calibrated_models[model_name][best_method]\n",
    "    \n",
    "    # Get predictions\n",
    "    y_prob = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    # Find optimal threshold\n",
    "    opt_threshold, opt_cost, thresholds, costs = find_optimal_threshold(y_test, y_prob, costs)\n",
    "    \n",
    "    # Calculate metrics at optimal threshold\n",
    "    y_pred_opt = (y_prob >= opt_threshold).astype(int)\n",
    "    f1_opt = f1_score(y_test, y_pred_opt)\n",
    "    precision_opt = precision_score(y_test, y_pred_opt)\n",
    "    recall_opt = recall_score(y_test, y_pred_opt)\n",
    "    \n",
    "    threshold_results[model_name] = {\n",
    "        'best_calibration': best_method,\n",
    "        'optimal_threshold': opt_threshold,\n",
    "        'optimal_cost': opt_cost,\n",
    "        'thresholds': thresholds,\n",
    "        'costs': costs,\n",
    "        'f1_score': f1_opt,\n",
    "        'precision': precision_opt,\n",
    "        'recall': recall_opt\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{model_name} ({best_method}):\")\n",
    "    print(f\"  Optimal threshold: {opt_threshold:.3f}\")\n",
    "    print(f\"  Total cost: ${opt_cost:,.0f}\")\n",
    "    print(f\"  F1-Score: {f1_opt:.4f}\")\n",
    "    print(f\"  Precision: {precision_opt:.4f}\")\n",
    "    print(f\"  Recall: {recall_opt:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizing Threshold Optimization\n",
    "\n",
    "Let's visualize how different thresholds affect costs and performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot threshold analysis for each model\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 15))\n",
    "\n",
    "for idx, (model_name, results) in enumerate(threshold_results.items()):\n",
    "    # Cost vs Threshold\n",
    "    ax1 = axes[idx, 0]\n",
    "    ax1.plot(results['thresholds'], results['costs'], 'b-', linewidth=2)\n",
    "    ax1.axvline(results['optimal_threshold'], color='r', linestyle='--', \n",
    "               label=f\"Optimal: {results['optimal_threshold']:.3f}\")\n",
    "    ax1.axvline(0.5, color='gray', linestyle=':', label='Default: 0.5')\n",
    "    ax1.set_xlabel('Threshold')\n",
    "    ax1.set_ylabel('Total Business Cost ($)')\n",
    "    ax1.set_title(f'{model_name} - Cost vs Threshold', fontweight='bold')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Metrics vs Threshold\n",
    "    ax2 = axes[idx, 1]\n",
    "    \n",
    "    # Calculate metrics for all thresholds\n",
    "    best_method = results['best_calibration']\n",
    "    if best_method == 'uncalibrated':\n",
    "        model = base_models[model_name]\n",
    "    else:\n",
    "        model = calibrated_models[model_name][best_method]\n",
    "    \n",
    "    y_prob = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    for threshold in results['thresholds']:\n",
    "        y_pred = (y_prob >= threshold).astype(int)\n",
    "        if y_pred.sum() > 0:  # Avoid division by zero\n",
    "            precisions.append(precision_score(y_test, y_pred))\n",
    "            recalls.append(recall_score(y_test, y_pred))\n",
    "            f1_scores.append(f1_score(y_test, y_pred))\n",
    "        else:\n",
    "            precisions.append(0)\n",
    "            recalls.append(0)\n",
    "            f1_scores.append(0)\n",
    "    \n",
    "    ax2.plot(results['thresholds'], precisions, 'g-', label='Precision', linewidth=2)\n",
    "    ax2.plot(results['thresholds'], recalls, 'b-', label='Recall', linewidth=2)\n",
    "    ax2.plot(results['thresholds'], f1_scores, 'r-', label='F1-Score', linewidth=2)\n",
    "    ax2.axvline(results['optimal_threshold'], color='black', linestyle='--', alpha=0.5)\n",
    "    ax2.set_xlabel('Threshold')\n",
    "    ax2.set_ylabel('Score')\n",
    "    ax2.set_title(f'{model_name} - Metrics vs Threshold', fontweight='bold')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Dynamic Thresholds by Context\n",
    "\n",
    "Let's explore how thresholds might vary based on transaction context (e.g., amount, time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create context-based analysis\n",
    "df_test = pd.DataFrame(X_test, columns=feature_columns)\n",
    "df_test['Class'] = y_test.values\n",
    "\n",
    "# Add predictions from best model\n",
    "best_model_name = min(threshold_results.keys(), \n",
    "                     key=lambda x: threshold_results[x]['optimal_cost'])\n",
    "best_method = threshold_results[best_model_name]['best_calibration']\n",
    "\n",
    "if best_method == 'uncalibrated':\n",
    "    best_model = base_models[best_model_name]\n",
    "else:\n",
    "    best_model = calibrated_models[best_model_name][best_method]\n",
    "\n",
    "df_test['fraud_prob'] = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Create amount bins\n",
    "df_test['Amount_Bin'] = pd.cut(\n",
    "    df_test['Amount'], \n",
    "    bins=[0, 10, 50, 200, 1000, float('inf')],\n",
    "    labels=['Very Low', 'Low', 'Medium', 'High', 'Very High']\n",
    ")\n",
    "\n",
    "# Find optimal thresholds for each amount bin\n",
    "print(\"üí∞ Dynamic Thresholds by Transaction Amount\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "dynamic_thresholds = {}\n",
    "\n",
    "for amount_bin in df_test['Amount_Bin'].unique():\n",
    "    if pd.isna(amount_bin):\n",
    "        continue\n",
    "        \n",
    "    # Filter data for this bin\n",
    "    bin_mask = df_test['Amount_Bin'] == amount_bin\n",
    "    bin_data = df_test[bin_mask]\n",
    "    \n",
    "    if len(bin_data) < 50 or bin_data['Class'].sum() < 5:\n",
    "        continue\n",
    "    \n",
    "    # Find optimal threshold for this bin\n",
    "    opt_threshold, opt_cost, _, _ = find_optimal_threshold(\n",
    "        bin_data['Class'].values, \n",
    "        bin_data['fraud_prob'].values, \n",
    "        costs\n",
    "    )\n",
    "    \n",
    "    # Calculate metrics\n",
    "    y_pred = (bin_data['fraud_prob'] >= opt_threshold).astype(int)\n",
    "    precision = precision_score(bin_data['Class'], y_pred)\n",
    "    recall = recall_score(bin_data['Class'], y_pred)\n",
    "    \n",
    "    dynamic_thresholds[amount_bin] = {\n",
    "        'threshold': opt_threshold,\n",
    "        'cost': opt_cost,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'sample_size': len(bin_data),\n",
    "        'fraud_count': bin_data['Class'].sum()\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{amount_bin} (${df_test[bin_mask]['Amount'].min():.0f}-${df_test[bin_mask]['Amount'].max():.0f}):\")\n",
    "    print(f\"  Samples: {len(bin_data):,} ({bin_data['Class'].sum()} frauds)\")\n",
    "    print(f\"  Optimal threshold: {opt_threshold:.3f}\")\n",
    "    print(f\"  Precision: {precision:.3f}, Recall: {recall:.3f}\")\n",
    "\n",
    "# Visualize dynamic thresholds\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Threshold by amount bin\n",
    "bins = list(dynamic_thresholds.keys())\n",
    "thresholds = [dynamic_thresholds[b]['threshold'] for b in bins]\n",
    "precisions = [dynamic_thresholds[b]['precision'] for b in bins]\n",
    "recalls = [dynamic_thresholds[b]['recall'] for b in bins]\n",
    "\n",
    "x_pos = np.arange(len(bins))\n",
    "ax1.bar(x_pos, thresholds, color='#3498db')\n",
    "ax1.axhline(y=0.5, color='red', linestyle='--', label='Default (0.5)')\n",
    "ax1.set_xlabel('Amount Bin')\n",
    "ax1.set_ylabel('Optimal Threshold')\n",
    "ax1.set_title('Dynamic Thresholds by Transaction Amount', fontweight='bold')\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels(bins, rotation=45)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Precision-Recall by amount bin\n",
    "width = 0.35\n",
    "ax2.bar(x_pos - width/2, precisions, width, label='Precision', color='#27ae60')\n",
    "ax2.bar(x_pos + width/2, recalls, width, label='Recall', color='#e74c3c')\n",
    "ax2.set_xlabel('Amount Bin')\n",
    "ax2.set_ylabel('Score')\n",
    "ax2.set_title('Performance by Transaction Amount', fontweight='bold')\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.set_xticklabels(bins, rotation=45)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Multi-Objective Threshold Optimization\n",
    "\n",
    "Let's find Pareto-optimal thresholds that balance multiple objectives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Pareto-optimal thresholds\n",
    "def find_pareto_optimal_thresholds(y_true, y_prob, costs):\n",
    "    \"\"\"Find thresholds that are Pareto-optimal across multiple objectives\"\"\"\n",
    "    thresholds = np.arange(0.01, 1.0, 0.01)\n",
    "    objectives = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_prob >= threshold).astype(int)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        \n",
    "        # Calculate objectives\n",
    "        total_cost = (fn * costs.fraud_cost + \n",
    "                     fp * costs.false_positive_cost + \n",
    "                     (tp + fp) * costs.investigation_cost)\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        objectives.append({\n",
    "            'threshold': threshold,\n",
    "            'cost': -total_cost,  # Negative because we want to maximize\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1\n",
    "        })\n",
    "    \n",
    "    # Find Pareto-optimal solutions\n",
    "    pareto_optimal = []\n",
    "    \n",
    "    for i, obj1 in enumerate(objectives):\n",
    "        is_dominated = False\n",
    "        \n",
    "        for j, obj2 in enumerate(objectives):\n",
    "            if i == j:\n",
    "                continue\n",
    "            \n",
    "            # Check if obj1 is dominated by obj2\n",
    "            if (obj2['cost'] >= obj1['cost'] and\n",
    "                obj2['precision'] >= obj1['precision'] and\n",
    "                obj2['recall'] >= obj1['recall'] and\n",
    "                (obj2['cost'] > obj1['cost'] or\n",
    "                 obj2['precision'] > obj1['precision'] or\n",
    "                 obj2['recall'] > obj1['recall'])):\n",
    "                is_dominated = True\n",
    "                break\n",
    "        \n",
    "        if not is_dominated:\n",
    "            pareto_optimal.append(obj1)\n",
    "    \n",
    "    return pareto_optimal\n",
    "\n",
    "# Find Pareto-optimal thresholds for best model\n",
    "y_prob_best = df_test['fraud_prob'].values\n",
    "pareto_thresholds = find_pareto_optimal_thresholds(y_test, y_prob_best, costs)\n",
    "\n",
    "print(f\"üéØ Found {len(pareto_thresholds)} Pareto-optimal thresholds\")\n",
    "print(\"\\nSample Pareto-optimal solutions:\")\n",
    "print(f\"{'Threshold':>10} {'Cost':>12} {'Precision':>10} {'Recall':>10} {'F1-Score':>10}\")\n",
    "print(\"-\" * 62)\n",
    "\n",
    "# Show a sample of Pareto-optimal solutions\n",
    "for i in range(0, len(pareto_thresholds), max(1, len(pareto_thresholds) // 5)):\n",
    "    sol = pareto_thresholds[i]\n",
    "    print(f\"{sol['threshold']:>10.3f} ${-sol['cost']:>11,.0f} \"\n",
    "          f\"{sol['precision']:>10.3f} {sol['recall']:>10.3f} {sol['f1_score']:>10.3f}\")\n",
    "\n",
    "# Visualize Pareto frontier\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "\n",
    "# 3D plot of objectives\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "pareto_costs = [-p['cost'] for p in pareto_thresholds]\n",
    "pareto_precisions = [p['precision'] for p in pareto_thresholds]\n",
    "pareto_recalls = [p['recall'] for p in pareto_thresholds]\n",
    "\n",
    "ax1.scatter(pareto_costs, pareto_precisions, pareto_recalls, c='red', s=50, alpha=0.8)\n",
    "ax1.set_xlabel('Total Cost ($)')\n",
    "ax1.set_ylabel('Precision')\n",
    "ax1.set_zlabel('Recall')\n",
    "ax1.set_title('Pareto Frontier (3D)', fontweight='bold')\n",
    "\n",
    "# 2D projections\n",
    "ax2 = fig.add_subplot(132)\n",
    "ax2.scatter(pareto_costs, pareto_precisions, c='blue', s=50, alpha=0.8)\n",
    "ax2.set_xlabel('Total Cost ($)')\n",
    "ax2.set_ylabel('Precision')\n",
    "ax2.set_title('Cost vs Precision', fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "ax3 = fig.add_subplot(133)\n",
    "ax3.scatter(pareto_precisions, pareto_recalls, c='green', s=50, alpha=0.8)\n",
    "ax3.set_xlabel('Precision')\n",
    "ax3.set_ylabel('Recall')\n",
    "ax3.set_title('Precision vs Recall', fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Final Recommendations and Summary\n",
    "\n",
    "Let's create a comprehensive summary and recommendations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary report\n",
    "print(\"üìä MODEL CALIBRATION AND THRESHOLD OPTIMIZATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Best calibration method for each model\n",
    "print(\"\\nüéØ Best Calibration Methods:\")\n",
    "for model_name in base_models.keys():\n",
    "    best_method = min(calibration_results[model_name].keys(), \n",
    "                     key=lambda x: calibration_results[model_name][x]['ece'])\n",
    "    ece_improvement = (calibration_results[model_name]['uncalibrated']['ece'] - \n",
    "                      calibration_results[model_name][best_method]['ece'])\n",
    "    \n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Best method: {best_method}\")\n",
    "    print(f\"  ECE improvement: {ece_improvement:.4f}\")\n",
    "    print(f\"  Final ECE: {calibration_results[model_name][best_method]['ece']:.4f}\")\n",
    "\n",
    "# Optimal thresholds summary\n",
    "print(\"\\nüí∞ Optimal Thresholds (Business Cost Minimization):\")\n",
    "best_overall_model = min(threshold_results.keys(), \n",
    "                        key=lambda x: threshold_results[x]['optimal_cost'])\n",
    "\n",
    "for model_name, results in threshold_results.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Threshold: {results['optimal_threshold']:.3f}\")\n",
    "    print(f\"  Total cost: ${results['optimal_cost']:,.0f}\")\n",
    "    print(f\"  Performance: P={results['precision']:.3f}, R={results['recall']:.3f}, F1={results['f1_score']:.3f}\")\n",
    "    if model_name == best_overall_model:\n",
    "        print(\"  ‚≠ê BEST OVERALL MODEL\")\n",
    "\n",
    "# Dynamic threshold insights\n",
    "print(\"\\nüîÑ Dynamic Threshold Insights:\")\n",
    "print(\"  ‚Ä¢ Low-value transactions can use higher thresholds (fewer false alarms)\")\n",
    "print(\"  ‚Ä¢ High-value transactions benefit from lower thresholds (catch more fraud)\")\n",
    "print(\"  ‚Ä¢ Consider implementing context-aware thresholds in production\")\n",
    "\n",
    "# Business recommendations\n",
    "print(\"\\nüíº Business Recommendations:\")\n",
    "print(f\"\\n1. Deploy {best_overall_model} with {threshold_results[best_overall_model]['best_calibration']} calibration\")\n",
    "print(f\"2. Set threshold to {threshold_results[best_overall_model]['optimal_threshold']:.3f} (not default 0.5)\")\n",
    "print(f\"3. Expected daily cost: ${threshold_results[best_overall_model]['optimal_cost']:,.0f}\")\n",
    "print(f\"4. Consider dynamic thresholds for 10-20% additional cost reduction\")\n",
    "print(f\"5. Monitor calibration drift - recalibrate monthly\")\n",
    "\n",
    "# ROI calculation\n",
    "baseline_cost = len(y_test) * costs.investigation_cost  # If we investigated everything\n",
    "optimal_cost = threshold_results[best_overall_model]['optimal_cost']\n",
    "savings = baseline_cost - optimal_cost\n",
    "roi = (savings / optimal_cost) * 100\n",
    "\n",
    "print(f\"\\nüí∞ Expected ROI:\")\n",
    "print(f\"  Baseline cost (investigate all): ${baseline_cost:,.0f}\")\n",
    "print(f\"  Optimized cost: ${optimal_cost:,.0f}\")\n",
    "print(f\"  Savings: ${savings:,.0f} ({roi:.1f}% ROI)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Key Takeaways and Conclusions\n",
    "\n",
    "### üéØ What We Learned:\n",
    "\n",
    "1. **Model Calibration is Essential**:\n",
    "   - Raw model probabilities are often poorly calibrated\n",
    "   - Platt scaling works well for small datasets\n",
    "   - Isotonic regression is more flexible but needs more data\n",
    "\n",
    "2. **Threshold Optimization Matters**:\n",
    "   - Default 0.5 threshold is rarely optimal for business\n",
    "   - Cost-sensitive thresholds can save significant money\n",
    "   - Different contexts may require different thresholds\n",
    "\n",
    "3. **Evaluation Metrics**:\n",
    "   - ECE and MCE measure calibration quality\n",
    "   - Brier score combines calibration and discrimination\n",
    "   - Business metrics should drive final decisions\n",
    "\n",
    "4. **Multi-Objective Optimization**:\n",
    "   - Pareto-optimal solutions balance competing objectives\n",
    "   - No single \"best\" threshold - depends on priorities\n",
    "   - Visualization helps understand trade-offs\n",
    "\n",
    "### üí° Best Practices:\n",
    "\n",
    "1. **Always calibrate models** before production deployment\n",
    "2. **Use held-out calibration set** separate from training and test\n",
    "3. **Optimize thresholds based on business costs**, not just ML metrics\n",
    "4. **Consider context-aware thresholds** for better performance\n",
    "5. **Monitor calibration drift** and recalibrate periodically\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "\n",
    "In the upcoming tutorials, you'll learn:\n",
    "- **Deep Learning**: Advanced neural networks and autoencoders\n",
    "- **Graph Neural Networks**: Leveraging transaction relationships\n",
    "- **Online Learning**: Adapting to changing fraud patterns\n",
    "- **Production Systems**: Building scalable APIs\n",
    "\n",
    "### üìù Practice Exercises:\n",
    "\n",
    "1. Try different calibration methods on your models\n",
    "2. Experiment with different business cost structures\n",
    "3. Implement time-based dynamic thresholds\n",
    "4. Create an ensemble of calibrated models\n",
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "You've mastered advanced model calibration and threshold optimization! You now understand:\n",
    "- How to calibrate probability predictions\n",
    "- Finding optimal thresholds based on business objectives\n",
    "- Implementing context-aware decision rules\n",
    "- Balancing multiple objectives in fraud detection\n",
    "\n",
    "Ready to explore deep learning for fraud detection? Check out `enhanced_deep_learning.ipynb`!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}