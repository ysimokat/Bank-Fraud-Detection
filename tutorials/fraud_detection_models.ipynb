{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": "# Building Your First Fraud Detection Models ü§ñ\n\n## üéØ Learning Objectives\nBy the end of this tutorial, you will:\n- Build multiple machine learning models for fraud detection\n- Understand the ML pipeline from data prep to evaluation\n- Handle class imbalance with proper techniques\n- Evaluate models using appropriate metrics for fraud detection\n- Compare different algorithms and choose the best one\n\n## üìã What This File Does\nThe `fraud_detection_models.py` file implements a complete machine learning pipeline including:\n\n**üîß Data Preprocessing:**\n- Feature scaling and normalization\n- Train/validation/test splits with stratification\n- Class imbalance handling\n\n**ü§ñ Multiple ML Algorithms:**\n- Logistic Regression (baseline)\n- Random Forest (ensemble)\n- Neural Networks (deep learning)\n- Isolation Forest (anomaly detection)\n- One-Class SVM (anomaly detection)\n\n**‚öñÔ∏è Imbalance Handling:**\n- Class weights adjustment\n- SMOTE (Synthetic Minority Over-sampling)\n- Anomaly detection approaches\n\n**üìä Comprehensive Evaluation:**\n- Confusion matrices\n- ROC curves and AUC scores\n- Precision-Recall curves\n- F1-scores and business metrics"
  },
  {
   "cell_type": "markdown",
   "id": "gwkikudw15r",
   "source": "## 1. Setting Up the Environment\n\nLet's start by importing all necessary libraries and setting up our environment:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "6se9icpnbe9",
   "source": "# Import essential libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Scikit-learn imports\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, IsolationForest\nfrom sklearn.svm import OneClassSVM\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import (\n    classification_report, confusion_matrix, \n    roc_auc_score, precision_recall_curve, roc_curve,\n    average_precision_score, f1_score, precision_score, recall_score\n)\n\n# Imbalanced-learn for SMOTE\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline as ImbPipeline\n\n# For saving models\nimport joblib\n\n# Set visualization style\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")\n\nprint(\"‚úÖ Libraries imported successfully!\")\nprint(f\"üìÖ Current time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ojspbvelurc",
   "source": "## 2. Loading and Preparing the Data\n\nWe'll load the data and create proper train/test splits while maintaining the class distribution:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ry59qckfsr8",
   "source": "# Load the dataset\ndf = pd.read_csv('../creditcard.csv')\n\nprint(\"üìä Dataset Overview:\")\nprint(f\"Total transactions: {len(df):,}\")\nprint(f\"Features: {df.shape[1]}\")\nprint(f\"Fraud transactions: {df['Class'].sum():,} ({df['Class'].mean()*100:.3f}%)\")\nprint(f\"Normal transactions: {(1-df['Class']).sum():,} ({(1-df['Class']).mean()*100:.3f}%)\")\n\n# Feature engineering\nprint(\"\\nüîß Feature Engineering...\")\n# Add log-transformed amount (helps with skewed distribution)\ndf['Amount_log'] = np.log(df['Amount'] + 1)\n\n# Add hour of day (cyclical pattern)\ndf['Hour'] = (df['Time'] % (24 * 3600)) // 3600\n\n# Create time-based features\ndf['Time_sin'] = np.sin(2 * np.pi * df['Hour'] / 24)\ndf['Time_cos'] = np.cos(2 * np.pi * df['Hour'] / 24)\n\nprint(\"‚úÖ Added engineered features: Amount_log, Hour, Time_sin, Time_cos\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "hw62zzpynrl",
   "source": "# Prepare features and target\n# Select features (exclude Time and Class, include engineered features)\nfeature_cols = [col for col in df.columns if col not in ['Class', 'Time']]\nX = df[feature_cols]\ny = df['Class']\n\nprint(f\"\\nüìä Feature matrix shape: {X.shape}\")\nprint(f\"Selected features ({len(feature_cols)}): {', '.join(feature_cols[:5])}...\")\n\n# Train-test split with stratification to maintain class distribution\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\nprint(f\"\\nüìÇ Train set: {len(X_train):,} samples\")\nprint(f\"   - Fraud: {y_train.sum():,} ({y_train.mean()*100:.3f}%)\")\nprint(f\"üìÇ Test set: {len(X_test):,} samples\")\nprint(f\"   - Fraud: {y_test.sum():,} ({y_test.mean()*100:.3f}%)\")\n\n# Verify stratification worked\nprint(f\"\\n‚úÖ Stratification check: Train fraud rate = {y_train.mean():.4f}, Test fraud rate = {y_test.mean():.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5d8v5wsxwak",
   "source": "## 3. Feature Scaling\n\nMost ML algorithms perform better with scaled features. Let's standardize our data:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "6pla8ybrgjo",
   "source": "# Initialize and fit the scaler\nscaler = StandardScaler()\n\n# Fit on training data and transform both sets\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Convert back to DataFrames for easier handling\nX_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_cols, index=X_train.index)\nX_test_scaled = pd.DataFrame(X_test_scaled, columns=feature_cols, index=X_test.index)\n\nprint(\"‚úÖ Features scaled using StandardScaler\")\nprint(f\"Mean of scaled training features: {X_train_scaled.mean().mean():.6f} (should be ~0)\")\nprint(f\"Std of scaled training features: {X_train_scaled.std().mean():.6f} (should be ~1)\")\n\n# Visualize the effect of scaling\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n\n# Before scaling\nax1.boxplot([X_train['Amount'].values, X_train['V1'].values, X_train['V2'].values], \n            labels=['Amount', 'V1', 'V2'])\nax1.set_title('Before Scaling')\nax1.set_ylabel('Value')\n\n# After scaling\nax2.boxplot([X_train_scaled['Amount'].values, X_train_scaled['V1'].values, X_train_scaled['V2'].values], \n            labels=['Amount', 'V1', 'V2'])\nax2.set_title('After Scaling')\nax2.set_ylabel('Standardized Value')\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "e6ocl5v3lbw",
   "source": "## 4. Building Baseline Models\n\nLet's start with simple models to establish baselines. We'll use class weights to handle imbalance:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "s29plp54ma",
   "source": "# Initialize models dictionary to store results\nmodels = {}\nresults = {}\n\nprint(\"ü§ñ Training Baseline Models...\")\nprint(\"=\" * 50)\n\n# 1. Logistic Regression\nprint(\"\\n1Ô∏è‚É£ Logistic Regression\")\nlr = LogisticRegression(\n    random_state=42, \n    max_iter=1000, \n    class_weight='balanced'  # Automatically adjust weights\n)\nlr.fit(X_train_scaled, y_train)\nmodels['Logistic Regression'] = lr\n\n# Make predictions\ny_pred_lr = lr.predict(X_test_scaled)\ny_proba_lr = lr.predict_proba(X_test_scaled)[:, 1]\n\n# Calculate metrics\nlr_metrics = {\n    'precision': precision_score(y_test, y_pred_lr),\n    'recall': recall_score(y_test, y_pred_lr),\n    'f1': f1_score(y_test, y_pred_lr),\n    'roc_auc': roc_auc_score(y_test, y_proba_lr)\n}\nresults['Logistic Regression'] = lr_metrics\n\nprint(f\"‚úÖ Precision: {lr_metrics['precision']:.4f}\")\nprint(f\"‚úÖ Recall: {lr_metrics['recall']:.4f}\")\nprint(f\"‚úÖ F1-Score: {lr_metrics['f1']:.4f}\")\nprint(f\"‚úÖ ROC-AUC: {lr_metrics['roc_auc']:.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "gcb3jofb6m9",
   "source": "# 2. Random Forest\nprint(\"\\n2Ô∏è‚É£ Random Forest\")\nrf = RandomForestClassifier(\n    n_estimators=100,\n    random_state=42,\n    class_weight='balanced',\n    n_jobs=-1  # Use all CPU cores\n)\nrf.fit(X_train_scaled, y_train)\nmodels['Random Forest'] = rf\n\n# Make predictions\ny_pred_rf = rf.predict(X_test_scaled)\ny_proba_rf = rf.predict_proba(X_test_scaled)[:, 1]\n\n# Calculate metrics\nrf_metrics = {\n    'precision': precision_score(y_test, y_pred_rf),\n    'recall': recall_score(y_test, y_pred_rf),\n    'f1': f1_score(y_test, y_pred_rf),\n    'roc_auc': roc_auc_score(y_test, y_proba_rf)\n}\nresults['Random Forest'] = rf_metrics\n\nprint(f\"‚úÖ Precision: {rf_metrics['precision']:.4f}\")\nprint(f\"‚úÖ Recall: {rf_metrics['recall']:.4f}\")\nprint(f\"‚úÖ F1-Score: {rf_metrics['f1']:.4f}\")\nprint(f\"‚úÖ ROC-AUC: {rf_metrics['roc_auc']:.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "n74fxmrs40j",
   "source": "# 3. Neural Network\nprint(\"\\n3Ô∏è‚É£ Neural Network\")\nnn = MLPClassifier(\n    hidden_layer_sizes=(100, 50),  # Two hidden layers\n    random_state=42,\n    max_iter=500,\n    early_stopping=True,\n    validation_fraction=0.1\n)\nnn.fit(X_train_scaled, y_train)\nmodels['Neural Network'] = nn\n\n# Make predictions\ny_pred_nn = nn.predict(X_test_scaled)\ny_proba_nn = nn.predict_proba(X_test_scaled)[:, 1]\n\n# Calculate metrics\nnn_metrics = {\n    'precision': precision_score(y_test, y_pred_nn),\n    'recall': recall_score(y_test, y_pred_nn),\n    'f1': f1_score(y_test, y_pred_nn),\n    'roc_auc': roc_auc_score(y_test, y_proba_nn)\n}\nresults['Neural Network'] = nn_metrics\n\nprint(f\"‚úÖ Precision: {nn_metrics['precision']:.4f}\")\nprint(f\"‚úÖ Recall: {nn_metrics['recall']:.4f}\")\nprint(f\"‚úÖ F1-Score: {nn_metrics['f1']:.4f}\")\nprint(f\"‚úÖ ROC-AUC: {nn_metrics['roc_auc']:.4f}\")\n\nprint(\"\\nüìä Summary of Baseline Models:\")\nbaseline_df = pd.DataFrame(results).T\nprint(baseline_df.round(4))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "lcf6vivcv3o",
   "source": "## 5. Anomaly Detection Approaches\n\nSince fraud is rare, we can treat it as an anomaly detection problem. These models learn what \"normal\" looks like:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "z2spsx738sa",
   "source": "print(\"üîç Training Anomaly Detection Models...\")\nprint(\"=\" * 50)\n\n# Get only normal transactions for training\nX_train_normal = X_train_scaled[y_train == 0]\nprint(f\"Training on {len(X_train_normal):,} normal transactions only\")\n\n# 1. Isolation Forest\nprint(\"\\n1Ô∏è‚É£ Isolation Forest\")\niso_forest = IsolationForest(\n    contamination=0.002,  # Expected proportion of outliers\n    random_state=42,\n    n_jobs=-1\n)\niso_forest.fit(X_train_normal)\nmodels['Isolation Forest'] = iso_forest\n\n# Predict (-1 for anomaly, 1 for normal)\ny_pred_iso = iso_forest.predict(X_test_scaled)\n# Convert to binary (0 for normal, 1 for fraud)\ny_pred_iso_binary = np.where(y_pred_iso == -1, 1, 0)\n\n# Get anomaly scores\ny_scores_iso = -iso_forest.score_samples(X_test_scaled)  # Higher score = more anomalous\n\n# Calculate metrics\niso_metrics = {\n    'precision': precision_score(y_test, y_pred_iso_binary),\n    'recall': recall_score(y_test, y_pred_iso_binary),\n    'f1': f1_score(y_test, y_pred_iso_binary),\n    'roc_auc': roc_auc_score(y_test, y_scores_iso)\n}\nresults['Isolation Forest'] = iso_metrics\n\nprint(f\"‚úÖ Precision: {iso_metrics['precision']:.4f}\")\nprint(f\"‚úÖ Recall: {iso_metrics['recall']:.4f}\")\nprint(f\"‚úÖ F1-Score: {iso_metrics['f1']:.4f}\")\nprint(f\"‚úÖ ROC-AUC: {iso_metrics['roc_auc']:.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "mddehczadc",
   "source": "## 6. Handling Imbalance with SMOTE\n\nSMOTE (Synthetic Minority Over-sampling Technique) creates synthetic fraud examples:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "6ikg69vbzfq",
   "source": "print(\"‚öñÔ∏è Balancing Data with SMOTE...\")\nprint(\"=\" * 50)\n\n# Apply SMOTE\nsmote = SMOTE(random_state=42)\nX_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n\nprint(f\"Original training set:\")\nprint(f\"  - Total: {len(y_train):,}\")\nprint(f\"  - Fraud: {y_train.sum():,} ({y_train.mean()*100:.3f}%)\")\n\nprint(f\"\\nAfter SMOTE:\")\nprint(f\"  - Total: {len(y_train_smote):,}\")\nprint(f\"  - Fraud: {y_train_smote.sum():,} ({y_train_smote.mean()*100:.3f}%)\")\n\n# Visualize the effect of SMOTE\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n# Original distribution\nax1.bar(['Normal', 'Fraud'], [len(y_train) - y_train.sum(), y_train.sum()], \n        color=['#2ecc71', '#e74c3c'])\nax1.set_title('Original Training Set')\nax1.set_ylabel('Count')\n\n# After SMOTE\nax2.bar(['Normal', 'Fraud'], [len(y_train_smote) - y_train_smote.sum(), y_train_smote.sum()], \n        color=['#2ecc71', '#e74c3c'])\nax2.set_title('After SMOTE')\nax2.set_ylabel('Count')\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "d5n3zzw5r0u",
   "source": "# Train models on SMOTE-balanced data\nprint(\"\\nü§ñ Training Models with SMOTE Data...\")\n\n# Random Forest with SMOTE\nprint(\"\\n1Ô∏è‚É£ Random Forest (SMOTE)\")\nrf_smote = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\nrf_smote.fit(X_train_smote, y_train_smote)\nmodels['Random Forest SMOTE'] = rf_smote\n\ny_pred_rf_smote = rf_smote.predict(X_test_scaled)\ny_proba_rf_smote = rf_smote.predict_proba(X_test_scaled)[:, 1]\n\nrf_smote_metrics = {\n    'precision': precision_score(y_test, y_pred_rf_smote),\n    'recall': recall_score(y_test, y_pred_rf_smote),\n    'f1': f1_score(y_test, y_pred_rf_smote),\n    'roc_auc': roc_auc_score(y_test, y_proba_rf_smote)\n}\nresults['Random Forest SMOTE'] = rf_smote_metrics\n\nprint(f\"‚úÖ Precision: {rf_smote_metrics['precision']:.4f}\")\nprint(f\"‚úÖ Recall: {rf_smote_metrics['recall']:.4f}\")\nprint(f\"‚úÖ F1-Score: {rf_smote_metrics['f1']:.4f}\")\nprint(f\"‚úÖ ROC-AUC: {rf_smote_metrics['roc_auc']:.4f}\")\n\n# Logistic Regression with SMOTE\nprint(\"\\n2Ô∏è‚É£ Logistic Regression (SMOTE)\")\nlr_smote = LogisticRegression(random_state=42, max_iter=1000)\nlr_smote.fit(X_train_smote, y_train_smote)\nmodels['Logistic Regression SMOTE'] = lr_smote\n\ny_pred_lr_smote = lr_smote.predict(X_test_scaled)\ny_proba_lr_smote = lr_smote.predict_proba(X_test_scaled)[:, 1]\n\nlr_smote_metrics = {\n    'precision': precision_score(y_test, y_pred_lr_smote),\n    'recall': recall_score(y_test, y_pred_lr_smote),\n    'f1': f1_score(y_test, y_pred_lr_smote),\n    'roc_auc': roc_auc_score(y_test, y_proba_lr_smote)\n}\nresults['Logistic Regression SMOTE'] = lr_smote_metrics\n\nprint(f\"‚úÖ Precision: {lr_smote_metrics['precision']:.4f}\")\nprint(f\"‚úÖ Recall: {lr_smote_metrics['recall']:.4f}\")\nprint(f\"‚úÖ F1-Score: {lr_smote_metrics['f1']:.4f}\")\nprint(f\"‚úÖ ROC-AUC: {lr_smote_metrics['roc_auc']:.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "dl6dty4vfj7",
   "source": "## 7. Model Comparison\n\nLet's compare all our models to see which performs best:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "hozgj45coyt",
   "source": "# Create comparison DataFrame\ncomparison_df = pd.DataFrame(results).T\ncomparison_df = comparison_df.sort_values('f1', ascending=False)\n\nprint(\"üìä Model Performance Comparison\")\nprint(\"=\" * 60)\nprint(comparison_df.round(4))\n\n# Visualize model comparison\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# 1. F1-Score comparison\nax1 = axes[0, 0]\ncomparison_df['f1'].plot(kind='bar', ax=ax1, color='#3498db')\nax1.set_title('F1-Score by Model', fontsize=14, fontweight='bold')\nax1.set_ylabel('F1-Score')\nax1.set_xticklabels(comparison_df.index, rotation=45, ha='right')\nax1.axhline(y=0.5, color='r', linestyle='--', alpha=0.5)\n\n# 2. Precision vs Recall\nax2 = axes[0, 1]\nax2.scatter(comparison_df['recall'], comparison_df['precision'], s=100)\nfor idx, model in enumerate(comparison_df.index):\n    ax2.annotate(model, (comparison_df.iloc[idx]['recall'], comparison_df.iloc[idx]['precision']),\n                xytext=(5, 5), textcoords='offset points', fontsize=8)\nax2.set_xlabel('Recall')\nax2.set_ylabel('Precision')\nax2.set_title('Precision vs Recall Trade-off', fontsize=14, fontweight='bold')\nax2.grid(True, alpha=0.3)\n\n# 3. ROC-AUC comparison\nax3 = axes[1, 0]\ncomparison_df['roc_auc'].plot(kind='bar', ax=ax3, color='#e74c3c')\nax3.set_title('ROC-AUC by Model', fontsize=14, fontweight='bold')\nax3.set_ylabel('ROC-AUC')\nax3.set_xticklabels(comparison_df.index, rotation=45, ha='right')\nax3.axhline(y=0.9, color='g', linestyle='--', alpha=0.5)\n\n# 4. All metrics heatmap\nax4 = axes[1, 1]\nsns.heatmap(comparison_df.T, annot=True, fmt='.3f', cmap='YlOrRd', ax=ax4)\nax4.set_title('All Metrics Heatmap', fontsize=14, fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\n# Find best model\nbest_model_name = comparison_df['f1'].idxmax()\nprint(f\"\\nüèÜ Best Model (by F1-Score): {best_model_name}\")\nprint(f\"   - F1-Score: {comparison_df.loc[best_model_name, 'f1']:.4f}\")\nprint(f\"   - Precision: {comparison_df.loc[best_model_name, 'precision']:.4f}\")\nprint(f\"   - Recall: {comparison_df.loc[best_model_name, 'recall']:.4f}\")\nprint(f\"   - ROC-AUC: {comparison_df.loc[best_model_name, 'roc_auc']:.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "yxlpteedxg",
   "source": "## 8. Detailed Analysis of Best Model\n\nLet's dive deeper into our best performing model:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "yyup0n41uv",
   "source": "# Get best model\nbest_model = models[best_model_name]\n\n# Make predictions with best model\nif best_model_name == 'Isolation Forest':\n    y_pred_best = np.where(best_model.predict(X_test_scaled) == -1, 1, 0)\n    y_scores_best = -best_model.score_samples(X_test_scaled)\nelse:\n    y_pred_best = best_model.predict(X_test_scaled)\n    y_scores_best = best_model.predict_proba(X_test_scaled)[:, 1]\n\n# Create detailed visualizations\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\n\n# 1. Confusion Matrix\nax1 = axes[0, 0]\ncm = confusion_matrix(y_test, y_pred_best)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1,\n            xticklabels=['Normal', 'Fraud'], yticklabels=['Normal', 'Fraud'])\nax1.set_title(f'Confusion Matrix - {best_model_name}', fontsize=14, fontweight='bold')\nax1.set_ylabel('True Label')\nax1.set_xlabel('Predicted Label')\n\n# Add text annotations\ntn, fp, fn, tp = cm.ravel()\nax1.text(2.5, -0.5, f'True Negatives: {tn:,}', ha='left')\nax1.text(2.5, -0.3, f'False Positives: {fp:,}', ha='left')\nax1.text(2.5, -0.1, f'False Negatives: {fn:,}', ha='left')\nax1.text(2.5, 0.1, f'True Positives: {tp:,}', ha='left')\n\n# 2. ROC Curve\nax2 = axes[0, 1]\nfpr, tpr, _ = roc_curve(y_test, y_scores_best)\nauc = roc_auc_score(y_test, y_scores_best)\nax2.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {auc:.3f})')\nax2.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\nax2.set_xlim([0.0, 1.0])\nax2.set_ylim([0.0, 1.05])\nax2.set_xlabel('False Positive Rate')\nax2.set_ylabel('True Positive Rate')\nax2.set_title('ROC Curve', fontsize=14, fontweight='bold')\nax2.legend(loc=\"lower right\")\nax2.grid(True, alpha=0.3)\n\n# 3. Precision-Recall Curve\nax3 = axes[1, 0]\nprecision, recall, _ = precision_recall_curve(y_test, y_scores_best)\navg_precision = average_precision_score(y_test, y_scores_best)\nax3.plot(recall, precision, color='red', lw=2, label=f'PR curve (AP = {avg_precision:.3f})')\nax3.set_xlabel('Recall')\nax3.set_ylabel('Precision')\nax3.set_title('Precision-Recall Curve', fontsize=14, fontweight='bold')\nax3.legend()\nax3.grid(True, alpha=0.3)\n\n# 4. Feature Importance (if available)\nax4 = axes[1, 1]\nif hasattr(best_model, 'feature_importances_'):\n    importances = best_model.feature_importances_\n    indices = np.argsort(importances)[-10:]  # Top 10 features\n    \n    ax4.barh(range(len(indices)), importances[indices], color='#2ecc71')\n    ax4.set_yticks(range(len(indices)))\n    ax4.set_yticklabels([feature_cols[i] for i in indices])\n    ax4.set_title('Top 10 Feature Importances', fontsize=14, fontweight='bold')\n    ax4.set_xlabel('Importance')\nelse:\n    ax4.text(0.5, 0.5, 'Feature importance not available\\nfor this model type', \n             ha='center', va='center', transform=ax4.transAxes)\n    ax4.set_title('Feature Importance', fontsize=14, fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\n# Print classification report\nprint(f\"\\nüìã Classification Report for {best_model_name}:\")\nprint(\"=\" * 60)\nprint(classification_report(y_test, y_pred_best, target_names=['Normal', 'Fraud'], digits=4))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "v9rigv9yvs",
   "source": "## 9. Business Impact Analysis\n\nLet's analyze the business impact of our model:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "eed4k1vy6rb",
   "source": "# Business metrics calculation\nprint(\"üíº Business Impact Analysis\")\nprint(\"=\" * 50)\n\n# Confusion matrix values\ntn, fp, fn, tp = confusion_matrix(y_test, y_pred_best).ravel()\n\n# Calculate business metrics\ntotal_transactions = len(y_test)\navg_transaction_amount = df['Amount'].mean()\navg_fraud_amount = df[df['Class'] == 1]['Amount'].mean()\n\n# Assumptions for business impact\nfalse_positive_cost = 5  # Cost of investigating false positive\nfraud_loss_prevented = avg_fraud_amount  # Amount saved per caught fraud\n\n# Calculate costs and savings\ninvestigation_cost = fp * false_positive_cost\nfraud_prevented_value = tp * fraud_loss_prevented\nfraud_losses = fn * avg_fraud_amount\nnet_benefit = fraud_prevented_value - investigation_cost - fraud_losses\n\nprint(f\"\\nüìä Model Performance on Test Set:\")\nprint(f\"   - Total transactions: {total_transactions:,}\")\nprint(f\"   - Frauds caught: {tp}/{tp+fn} ({tp/(tp+fn)*100:.1f}%)\")\nprint(f\"   - False alarms: {fp:,} ({fp/total_transactions*100:.2f}% of all transactions)\")\n\nprint(f\"\\nüí∞ Financial Impact (Estimated):\")\nprint(f\"   - Average fraud amount: ${avg_fraud_amount:.2f}\")\nprint(f\"   - Fraud prevented value: ${fraud_prevented_value:,.2f}\")\nprint(f\"   - Investigation costs: ${investigation_cost:,.2f}\")\nprint(f\"   - Fraud losses (missed): ${fraud_losses:,.2f}\")\nprint(f\"   - Net benefit: ${net_benefit:,.2f}\")\n\nprint(f\"\\nüìà Efficiency Metrics:\")\nprint(f\"   - Precision (investigation efficiency): {tp/(tp+fp)*100:.1f}%\")\nprint(f\"   - For every 100 investigations, {tp/(tp+fp)*100:.0f} are actual frauds\")\nprint(f\"   - Workload increase: {(tp+fp)/total_transactions*100:.2f}% of transactions flagged\")\n\n# Visualize business impact\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\n# Cost breakdown\nax1.bar(['Fraud\\nPrevented', 'Investigation\\nCost', 'Fraud\\nLosses', 'Net\\nBenefit'],\n        [fraud_prevented_value, -investigation_cost, -fraud_losses, net_benefit],\n        color=['#2ecc71', '#e74c3c', '#e74c3c', '#3498db'])\nax1.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\nax1.set_title('Financial Impact Breakdown', fontsize=14, fontweight='bold')\nax1.set_ylabel('Amount ($)')\n\n# Detection rates\nax2.pie([tp, fn], labels=['Caught', 'Missed'], autopct='%1.1f%%',\n        colors=['#2ecc71', '#e74c3c'], startangle=90)\nax2.set_title('Fraud Detection Rate', fontsize=14, fontweight='bold')\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7e04hjvmm8v",
   "source": "## 10. Saving the Models\n\nLet's save our trained models for future use:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "rm19r3fl11n",
   "source": "# Save models and preprocessing objects\nprint(\"üíæ Saving models and preprocessing objects...\")\n\n# Save all models\njoblib.dump(models, '../fraud_models.joblib')\nprint(\"‚úÖ Saved all models to 'fraud_models.joblib'\")\n\n# Save the scaler\njoblib.dump(scaler, '../scaler.joblib')\nprint(\"‚úÖ Saved scaler to 'scaler.joblib'\")\n\n# Save results for comparison\njoblib.dump(results, '../model_results.joblib')\nprint(\"‚úÖ Saved results to 'model_results.joblib'\")\n\n# Save best model separately\njoblib.dump(best_model, f'../best_model_{best_model_name.replace(\" \", \"_\").lower()}.joblib')\nprint(f\"‚úÖ Saved best model to 'best_model_{best_model_name.replace(' ', '_').lower()}.joblib'\")\n\n# Create a model summary\nmodel_summary = {\n    'best_model_name': best_model_name,\n    'best_model_metrics': results[best_model_name],\n    'feature_columns': feature_cols,\n    'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n    'test_set_size': len(y_test),\n    'fraud_rate': y_test.mean()\n}\n\njoblib.dump(model_summary, '../model_summary.joblib')\nprint(\"‚úÖ Saved model summary to 'model_summary.joblib'\")\n\nprint(\"\\nüì¶ All models and artifacts saved successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9itbx7fqz5e",
   "source": "## 11. Key Takeaways and Conclusions\n\n### üéØ What We Learned:\n\n1. **Class Imbalance is Critical**: \n   - With only 0.172% fraud rate, accuracy is misleading\n   - Precision, Recall, and F1-score are better metrics\n   - ROC-AUC helps evaluate overall discrimination ability\n\n2. **Different Approaches Work**:\n   - **Class Weights**: Adjust algorithm focus on minority class\n   - **SMOTE**: Create synthetic fraud examples\n   - **Anomaly Detection**: Learn normal behavior patterns\n\n3. **Model Performance Insights**:\n   - Simple models (Logistic Regression) can be surprisingly effective\n   - Ensemble methods (Random Forest) often perform best\n   - SMOTE can improve recall but may reduce precision\n\n4. **Business Impact Matters**:\n   - High recall catches more fraud but increases false positives\n   - Balance depends on investigation costs vs fraud losses\n   - Model threshold can be adjusted based on business needs\n\n### üí° Best Practices:\n\n1. **Always use stratified splits** to maintain class distribution\n2. **Scale features** for distance-based algorithms\n3. **Try multiple approaches** - no single solution fits all\n4. **Evaluate with multiple metrics** - not just accuracy\n5. **Consider business constraints** when selecting models\n\n### üöÄ Next Steps:\n\nIn the next tutorials, you'll learn:\n- **Enhanced Models**: XGBoost, LightGBM, and advanced ensembles\n- **Deep Learning**: Autoencoders and neural networks for fraud\n- **Real-time Systems**: Building production-ready APIs\n- **Advanced Techniques**: Graph neural networks and active learning\n\n### üìù Practice Exercises:\n\n1. Try adjusting the `class_weight` parameter to see its effect\n2. Experiment with different SMOTE ratios\n3. Create a custom threshold for the best model to optimize for precision or recall\n4. Try combining predictions from multiple models (ensemble)\n\n## üéâ Congratulations!\n\nYou've successfully built your first fraud detection models! You now understand:\n- How to handle severely imbalanced datasets\n- Multiple approaches to fraud detection\n- How to evaluate models appropriately\n- The importance of business context in ML\n\nReady for more advanced techniques? Check out `enhanced_fraud_models.ipynb`!",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}