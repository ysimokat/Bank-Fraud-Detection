{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": "# Credit Card Fraud Detection: Data Exploration Tutorial\n\n## üéØ Learning Objectives\nBy the end of this tutorial, you will:\n- Understand how to explore a fraud detection dataset\n- Learn to identify class imbalance and its implications\n- Discover which features are most important for fraud detection\n- Create visualizations to understand data patterns\n- Handle missing values and outliers\n\n## üìä What is This File About?\nThe `data_exploration.py` file is our starting point for understanding the credit card fraud dataset. It performs **Exploratory Data Analysis (EDA)** - the crucial first step in any machine learning project.\n\n**Why is EDA important?**\n- Helps us understand our data before building models\n- Reveals patterns and anomalies\n- Identifies data quality issues\n- Guides feature engineering decisions\n\n## üìÅ Dataset Overview\n- **Source**: European cardholders (September 2013)\n- **Size**: 284,807 transactions over 2 days\n- **Fraud Rate**: ~0.172% (highly imbalanced!)\n- **Features**: \n  - V1-V28: PCA-transformed features (anonymized for privacy)\n  - Time: Seconds elapsed from first transaction\n  - Amount: Transaction amount\n  - Class: Target variable (0=Normal, 1=Fraud)"
  },
  {
   "cell_type": "markdown",
   "id": "7axkaxkc4sy",
   "source": "## 1. Setting Up Our Environment\n\nFirst, let's import all the necessary libraries for data exploration:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "9cl20fp6m3c",
   "source": "# Import essential libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport warnings\n\n# Configure visualization settings\nwarnings.filterwarnings('ignore')\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")\n\n# Set display options for better viewing\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 100)\npd.set_option('display.float_format', '{:.2f}'.format)\n\nprint(\"‚úÖ Libraries imported successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "wvhi1be5fgd",
   "source": "## 2. Loading the Dataset\n\nLet's load the credit card fraud dataset and examine its structure:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "g85rn7zrhb5",
   "source": "# Load the dataset\nfile_path = '../creditcard.csv'\ndf = pd.read_csv(file_path)\n\n# Display basic information\nprint(\"üîç Dataset Overview\")\nprint(\"=\" * 50)\nprint(f\"üìä Shape: {df.shape[0]:,} transactions √ó {df.shape[1]} features\")\nprint(f\"üíæ Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\nprint(f\"üìÖ Time span: {df['Time'].max() / 3600:.1f} hours\")\nprint(\"\\nüìã Column names:\")\nprint(df.columns.tolist())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "kibfcfk4svp",
   "source": "# Display first few rows\nprint(\"\\nüìä First 5 transactions:\")\ndf.head()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "flojdtod3xs",
   "source": "## 3. Understanding the Target Variable\n\nThe most important aspect of fraud detection is understanding the class distribution:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "fqc8hmbojqa",
   "source": "# Analyze class distribution\nprint(\"üéØ Target Variable Analysis (Class)\")\nprint(\"=\" * 50)\n\n# Count and percentage\nclass_counts = df['Class'].value_counts()\nclass_percentages = df['Class'].value_counts(normalize=True) * 100\n\n# Display results\nprint(f\"‚úÖ Normal transactions (0): {class_counts[0]:,} ({class_percentages[0]:.3f}%)\")\nprint(f\"üö® Fraudulent transactions (1): {class_counts[1]:,} ({class_percentages[1]:.3f}%)\")\nprint(f\"\\n‚ö†Ô∏è  Imbalance ratio: {class_counts[0]/class_counts[1]:.0f}:1\")\nprint(f\"üìä This means for every fraud transaction, there are ~{class_counts[0]/class_counts[1]:.0f} normal ones!\")\n\n# Visualize\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n# Bar plot\nax1.bar(['Normal', 'Fraud'], class_counts.values, color=['#2ecc71', '#e74c3c'])\nax1.set_title('Transaction Counts by Class', fontsize=14, fontweight='bold')\nax1.set_ylabel('Number of Transactions')\nfor i, v in enumerate(class_counts.values):\n    ax1.text(i, v + 1000, f'{v:,}', ha='center', fontweight='bold')\n\n# Pie chart\nax2.pie(class_counts.values, labels=['Normal', 'Fraud'], autopct='%1.3f%%', \n        colors=['#2ecc71', '#e74c3c'], startangle=90)\nax2.set_title('Class Distribution', fontsize=14, fontweight='bold')\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "fdess8qw9x6",
   "source": "### üí° Key Insight: Extreme Class Imbalance\n\nThe fraud rate is only ~0.172%! This extreme imbalance presents challenges:\n- **Accuracy Paradox**: A model that predicts all transactions as normal would achieve 99.83% accuracy!\n- **Need for Special Metrics**: We'll need precision, recall, and F1-score instead of just accuracy\n- **Sampling Strategies**: We may need to use SMOTE, undersampling, or class weights\n\n## 4. Missing Values and Data Quality",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "lnbj1f2c7oc",
   "source": "# Check for missing values\nprint(\"‚ùì Missing Values Check\")\nprint(\"=\" * 50)\n\nmissing_values = df.isnull().sum()\nif missing_values.sum() == 0:\n    print(\"‚úÖ Great news! No missing values found in the dataset.\")\nelse:\n    print(\"‚ö†Ô∏è  Missing values found:\")\n    print(missing_values[missing_values > 0])\n\n# Check data types\nprint(\"\\nüìä Data Types:\")\nprint(df.dtypes.value_counts())\n\n# Check for duplicates\nduplicates = df.duplicated().sum()\nprint(f\"\\nüîç Duplicate rows: {duplicates}\")\n\n# Statistical summary\nprint(\"\\nüìà Statistical Summary of Key Features:\")\ndf[['Time', 'Amount', 'V1', 'V2', 'V3', 'Class']].describe()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "tjrwduggvto",
   "source": "## 5. Exploring Transaction Amounts\n\nTransaction amounts can be a strong indicator of fraud. Let's analyze the distribution:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "cbj7kzfbr8l",
   "source": "# Amount analysis\nprint(\"üí∞ Transaction Amount Analysis\")\nprint(\"=\" * 50)\n\n# Overall statistics\nprint(f\"Range: ${df['Amount'].min():.2f} - ${df['Amount'].max():,.2f}\")\nprint(f\"Mean: ${df['Amount'].mean():.2f}\")\nprint(f\"Median: ${df['Amount'].median():.2f}\")\nprint(f\"Std Dev: ${df['Amount'].std():.2f}\")\n\n# Compare amounts by class\nprint(\"\\nüí∞ Amount Statistics by Class:\")\namount_by_class = df.groupby('Class')['Amount'].agg(['mean', 'median', 'std', 'min', 'max'])\namount_by_class.index = ['Normal', 'Fraud']\nprint(amount_by_class.round(2))\n\n# Visualize amount distributions\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# 1. Overall amount distribution\nax1 = axes[0, 0]\ndf['Amount'].hist(bins=100, ax=ax1, edgecolor='black', alpha=0.7)\nax1.set_title('Transaction Amount Distribution', fontsize=12, fontweight='bold')\nax1.set_xlabel('Amount ($)')\nax1.set_ylabel('Frequency')\nax1.set_yscale('log')\n\n# 2. Amount distribution by class (box plot)\nax2 = axes[0, 1]\ndf.boxplot(column='Amount', by='Class', ax=ax2)\nax2.set_title('Amount Distribution by Class', fontsize=12, fontweight='bold')\nax2.set_xlabel('Class (0=Normal, 1=Fraud)')\nax2.set_ylabel('Amount ($)')\nax2.set_yscale('log')\nplt.suptitle('')  # Remove default title\n\n# 3. Amount distribution (log scale) for better visibility\nax3 = axes[1, 0]\n# Add small value to handle zero amounts\namount_log = np.log10(df['Amount'] + 1)\nnormal_amount_log = np.log10(df[df['Class']==0]['Amount'] + 1)\nfraud_amount_log = np.log10(df[df['Class']==1]['Amount'] + 1)\n\nax3.hist([normal_amount_log, fraud_amount_log], bins=50, label=['Normal', 'Fraud'], \n         color=['#2ecc71', '#e74c3c'], alpha=0.7)\nax3.set_title('Log10(Amount+1) Distribution by Class', fontsize=12, fontweight='bold')\nax3.set_xlabel('Log10(Amount + 1)')\nax3.set_ylabel('Frequency')\nax3.legend()\n\n# 4. Kernel Density Estimation\nax4 = axes[1, 1]\ndf[df['Class']==0]['Amount'].plot.density(ax=ax4, label='Normal', color='#2ecc71')\ndf[df['Class']==1]['Amount'].plot.density(ax=ax4, label='Fraud', color='#e74c3c')\nax4.set_title('Amount Density Distribution', fontsize=12, fontweight='bold')\nax4.set_xlabel('Amount ($)')\nax4.set_xlim(0, 500)  # Focus on smaller amounts for clarity\nax4.legend()\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "6t7587hp0ua",
   "source": "### üí° Key Insights from Amount Analysis:\n- Fraud transactions tend to have different amount patterns than normal ones\n- Many small transactions (note the log scale needed for visualization)\n- Fraudulent transactions show different distribution characteristics\n\n## 6. Time Pattern Analysis\n\nUnderstanding when fraud occurs can help with real-time detection:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "q7hvha5kwf",
   "source": "# Time analysis\nprint(\"‚è∞ Time Pattern Analysis\")\nprint(\"=\" * 50)\n\n# Convert time from seconds to hours\ndf['Hour'] = df['Time'] / 3600\n\nprint(f\"Time range: {df['Time'].min():.0f} - {df['Time'].max():.0f} seconds\")\nprint(f\"Duration: {df['Hour'].max():.1f} hours ({df['Hour'].max()/24:.1f} days)\")\n\n# Create time-based visualizations\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# 1. Transaction volume over time\nax1 = axes[0, 0]\ndf['Hour'].hist(bins=48, ax=ax1, edgecolor='black', alpha=0.7)\nax1.set_title('Transaction Volume Over Time', fontsize=12, fontweight='bold')\nax1.set_xlabel('Hours from Start')\nax1.set_ylabel('Number of Transactions')\n\n# 2. Fraud rate over time\nax2 = axes[0, 1]\n# Create hourly bins\nhourly_bins = pd.cut(df['Hour'], bins=48)\nfraud_rate_by_hour = df.groupby(hourly_bins)['Class'].agg(['mean', 'count'])\nfraud_rate_by_hour['mean'].plot(ax=ax2, color='red', marker='o', markersize=4)\nax2.set_title('Fraud Rate Over Time', fontsize=12, fontweight='bold')\nax2.set_xlabel('Hours from Start')\nax2.set_ylabel('Fraud Rate')\nax2.grid(True, alpha=0.3)\n\n# 3. Transaction patterns by class\nax3 = axes[1, 0]\nnormal_time = df[df['Class']==0]['Hour']\nfraud_time = df[df['Class']==1]['Hour']\nax3.hist([normal_time, fraud_time], bins=48, label=['Normal', 'Fraud'], \n         color=['#2ecc71', '#e74c3c'], alpha=0.7, density=True)\nax3.set_title('Time Distribution by Class (Normalized)', fontsize=12, fontweight='bold')\nax3.set_xlabel('Hours from Start')\nax3.set_ylabel('Density')\nax3.legend()\n\n# 4. Scatter plot: Time vs Amount\nax4 = axes[1, 1]\n# Sample normal transactions for visibility\nnormal_sample = df[df['Class']==0].sample(1000, random_state=42)\nfraud_all = df[df['Class']==1]\n\nax4.scatter(normal_sample['Hour'], normal_sample['Amount'], \n           alpha=0.5, s=10, label='Normal (sample)', color='#2ecc71')\nax4.scatter(fraud_all['Hour'], fraud_all['Amount'], \n           alpha=0.7, s=20, label='Fraud (all)', color='#e74c3c')\nax4.set_title('Time vs Amount Pattern', fontsize=12, fontweight='bold')\nax4.set_xlabel('Hours from Start')\nax4.set_ylabel('Amount ($)')\nax4.set_yscale('log')\nax4.legend()\n\nplt.tight_layout()\nplt.show()\n\n# Calculate some interesting time-based statistics\nprint(\"\\nüìä Fraud Distribution by Time Period:\")\ndf['TimeOfDay'] = pd.cut(df['Hour'] % 24, bins=[0, 6, 12, 18, 24], \n                         labels=['Night', 'Morning', 'Afternoon', 'Evening'])\ntime_fraud_stats = pd.crosstab(df['TimeOfDay'], df['Class'], normalize='index') * 100\ntime_fraud_stats.columns = ['Normal %', 'Fraud %']\nprint(time_fraud_stats.round(3))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2xc7ltu4zuj",
   "source": "## 7. PCA Feature Analysis\n\nThe V1-V28 features are PCA-transformed. Let's analyze which ones are most important for fraud detection:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "yq1h2hoc6s",
   "source": "# Analyze PCA features\nprint(\"üî¨ PCA Feature Analysis\")\nprint(\"=\" * 50)\n\n# Get PCA feature columns\npca_features = [col for col in df.columns if col.startswith('V')]\nprint(f\"Number of PCA features: {len(pca_features)}\")\n\n# Calculate effect sizes (Cohen's d) for each feature\ndef calculate_effect_size(feature):\n    \"\"\"Calculate Cohen's d effect size for a feature\"\"\"\n    normal_data = df[df['Class'] == 0][feature]\n    fraud_data = df[df['Class'] == 1][feature]\n    \n    mean_diff = abs(normal_data.mean() - fraud_data.mean())\n    pooled_std = np.sqrt((normal_data.std()**2 + fraud_data.std()**2) / 2)\n    \n    return mean_diff / pooled_std if pooled_std > 0 else 0\n\n# Calculate effect sizes for all PCA features\neffect_sizes = []\nfor feature in pca_features:\n    effect_size = calculate_effect_size(feature)\n    effect_sizes.append({\n        'Feature': feature,\n        'Effect_Size': effect_size,\n        'Normal_Mean': df[df['Class'] == 0][feature].mean(),\n        'Fraud_Mean': df[df['Class'] == 1][feature].mean()\n    })\n\n# Create DataFrame and sort by effect size\neffect_df = pd.DataFrame(effect_sizes).sort_values('Effect_Size', ascending=False)\n\nprint(\"\\nüéØ Top 10 Most Discriminative Features:\")\nprint(\"(Effect Size: Cohen's d - larger values indicate better separation)\")\nprint(\"-\" * 60)\nprint(effect_df.head(10).to_string(index=False))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "49klx3qdyof",
   "source": "# Visualize top discriminative features\nfig, axes = plt.subplots(2, 5, figsize=(20, 8))\naxes = axes.ravel()\n\ntop_features = effect_df.head(10)['Feature'].tolist()\n\nfor i, feature in enumerate(top_features):\n    ax = axes[i]\n    \n    # Get data for each class\n    normal_data = df[df['Class'] == 0][feature]\n    fraud_data = df[df['Class'] == 1][feature]\n    \n    # Create distributions\n    ax.hist(normal_data, bins=50, alpha=0.6, density=True, label='Normal', color='#2ecc71')\n    ax.hist(fraud_data, bins=50, alpha=0.6, density=True, label='Fraud', color='#e74c3c')\n    \n    # Add title with effect size\n    effect_size = effect_df[effect_df['Feature'] == feature]['Effect_Size'].iloc[0]\n    ax.set_title(f'{feature}\\n(d = {effect_size:.3f})', fontsize=10, fontweight='bold')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Density')\n    ax.legend(fontsize=8)\n    \nplt.suptitle('Top 10 Most Discriminative PCA Features', fontsize=16, fontweight='bold')\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "614xgns37h8",
   "source": "## 8. Feature Correlations\n\nUnderstanding feature relationships can help with feature selection and engineering:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "9nbantjgcd",
   "source": "# Feature correlation analysis\nprint(\"üîó Feature Correlation Analysis\")\nprint(\"=\" * 50)\n\n# Calculate correlations with target\ntarget_corr = df.corr()['Class'].sort_values(ascending=False)\nprint(\"Top features correlated with fraud (Class):\")\nprint(target_corr.head(10).to_string())\nprint(\"\\nTop features negatively correlated with fraud:\")\nprint(target_corr.tail(10).to_string())\n\n# Create correlation heatmaps\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n\n# 1. Top correlated features heatmap\ntop_corr_features = target_corr.abs().sort_values(ascending=False).head(15).index.tolist()\ncorr_matrix_top = df[top_corr_features].corr()\n\nsns.heatmap(corr_matrix_top, annot=True, fmt='.2f', cmap='coolwarm', \n            center=0, square=True, ax=ax1, cbar_kws={'shrink': 0.8})\nax1.set_title('Correlation Matrix: Top 15 Features', fontsize=14, fontweight='bold')\n\n# 2. Feature correlation with Class (bar plot)\nax2.barh(range(len(target_corr.head(20))), target_corr.head(20).values)\nax2.set_yticks(range(len(target_corr.head(20))))\nax2.set_yticklabels(target_corr.head(20).index)\nax2.set_xlabel('Correlation with Fraud')\nax2.set_title('Top 20 Features Correlated with Fraud', fontsize=14, fontweight='bold')\nax2.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n\n# Color bars based on positive/negative correlation\ncolors = ['#e74c3c' if x > 0 else '#3498db' for x in target_corr.head(20).values]\nbars = ax2.patches\nfor bar, color in zip(bars, colors):\n    bar.set_color(color)\n\nplt.tight_layout()\nplt.show()\n\n# Check multicollinearity among PCA features\nprint(\"\\nüîç Multicollinearity Check:\")\nprint(\"PCA features should have low correlation with each other...\")\npca_corr = df[pca_features].corr()\nhigh_corr_pairs = []\nfor i in range(len(pca_features)):\n    for j in range(i+1, len(pca_features)):\n        if abs(pca_corr.iloc[i, j]) > 0.8:\n            high_corr_pairs.append((pca_features[i], pca_features[j], pca_corr.iloc[i, j]))\n\nif high_corr_pairs:\n    print(\"‚ö†Ô∏è  High correlation pairs found:\")\n    for pair in high_corr_pairs:\n        print(f\"{pair[0]} - {pair[1]}: {pair[2]:.3f}\")\nelse:\n    print(\"‚úÖ Good news! No high correlations (>0.8) found among PCA features.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4db75wh0tsm",
   "source": "## 9. Outlier Detection\n\nFraudulent transactions often appear as outliers. Let's identify them:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "idkkj0yoo8",
   "source": "# Outlier analysis\nprint(\"üîç Outlier Detection Analysis\")\nprint(\"=\" * 50)\n\n# Calculate IQR for Amount\nQ1 = df['Amount'].quantile(0.25)\nQ3 = df['Amount'].quantile(0.75)\nIQR = Q3 - Q1\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\n\n# Find outliers\namount_outliers = df[(df['Amount'] < lower_bound) | (df['Amount'] > upper_bound)]\nprint(f\"Amount outliers: {len(amount_outliers)} ({len(amount_outliers)/len(df)*100:.2f}%)\")\nprint(f\"Fraud rate in amount outliers: {amount_outliers['Class'].mean()*100:.2f}%\")\nprint(f\"Normal fraud rate: {df['Class'].mean()*100:.2f}%\")\n\n# Use Isolation Forest for multivariate outlier detection\nfrom sklearn.ensemble import IsolationForest\n\n# Prepare features for outlier detection\noutlier_features = pca_features + ['Amount']\nX_outlier = df[outlier_features].values\n\n# Fit Isolation Forest\niso_forest = IsolationForest(contamination=0.01, random_state=42, n_estimators=100)\noutlier_predictions = iso_forest.fit_predict(X_outlier)\n\n# Analyze results\ndf['Outlier'] = outlier_predictions\noutlier_fraud_rate = df[df['Outlier'] == -1]['Class'].mean()\nnormal_fraud_rate = df[df['Outlier'] == 1]['Class'].mean()\n\nprint(f\"\\nüå≤ Isolation Forest Results:\")\nprint(f\"Outliers detected: {(outlier_predictions == -1).sum()} ({(outlier_predictions == -1).sum()/len(df)*100:.2f}%)\")\nprint(f\"Fraud rate in outliers: {outlier_fraud_rate*100:.2f}%\")\nprint(f\"Fraud rate in normal points: {normal_fraud_rate*100:.2f}%\")\nprint(f\"Improvement factor: {outlier_fraud_rate/normal_fraud_rate:.1f}x\")\n\n# Visualize outliers\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\n# 1. Amount outliers visualization\nax1.scatter(df[df['Outlier'] == 1]['Hour'], df[df['Outlier'] == 1]['Amount'], \n           alpha=0.5, s=1, label='Normal', color='#3498db')\nax1.scatter(df[df['Outlier'] == -1]['Hour'], df[df['Outlier'] == -1]['Amount'], \n           alpha=0.8, s=10, label='Outlier', color='#e74c3c')\nax1.set_xlabel('Time (hours)')\nax1.set_ylabel('Amount ($)')\nax1.set_yscale('log')\nax1.set_title('Outliers in Time-Amount Space', fontsize=12, fontweight='bold')\nax1.legend()\n\n# 2. Outlier vs Fraud comparison\noutlier_fraud_crosstab = pd.crosstab(df['Outlier'], df['Class'], normalize='index') * 100\noutlier_fraud_crosstab.plot(kind='bar', ax=ax2, color=['#2ecc71', '#e74c3c'])\nax2.set_xlabel('Outlier Status (-1: Outlier, 1: Normal)')\nax2.set_ylabel('Percentage')\nax2.set_title('Fraud Rate by Outlier Status', fontsize=12, fontweight='bold')\nax2.legend(['Normal Transaction', 'Fraud'])\nax2.set_xticklabels(['Outlier', 'Normal'], rotation=0)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "mx51j5j09bf",
   "source": "## 10. Summary and Key Insights\n\nLet's summarize our findings from the data exploration:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "tmv6bncjhtb",
   "source": "# Summary of findings\nprint(\"üìä DATA EXPLORATION SUMMARY\")\nprint(\"=\" * 60)\n\nprint(\"\\n1Ô∏è‚É£ Dataset Characteristics:\")\nprint(f\"   ‚Ä¢ Total transactions: {len(df):,}\")\nprint(f\"   ‚Ä¢ Features: {df.shape[1]} (28 PCA + Time + Amount + Class)\")\nprint(f\"   ‚Ä¢ Time span: {df['Hour'].max():.1f} hours\")\nprint(f\"   ‚Ä¢ No missing values\")\n\nprint(\"\\n2Ô∏è‚É£ Class Imbalance:\")\nprint(f\"   ‚Ä¢ Normal transactions: {class_counts[0]:,} ({class_percentages[0]:.3f}%)\")\nprint(f\"   ‚Ä¢ Fraudulent transactions: {class_counts[1]:,} ({class_percentages[1]:.3f}%)\")\nprint(f\"   ‚Ä¢ Imbalance ratio: {class_counts[0]/class_counts[1]:.0f}:1\")\n\nprint(\"\\n3Ô∏è‚É£ Transaction Amounts:\")\nprint(f\"   ‚Ä¢ Range: ${df['Amount'].min():.2f} - ${df['Amount'].max():,.2f}\")\nprint(f\"   ‚Ä¢ Mean (Normal): ${df[df['Class']==0]['Amount'].mean():.2f}\")\nprint(f\"   ‚Ä¢ Mean (Fraud): ${df[df['Class']==1]['Amount'].mean():.2f}\")\nprint(f\"   ‚Ä¢ Fraud transactions tend to have different amount patterns\")\n\nprint(\"\\n4Ô∏è‚É£ Most Discriminative Features:\")\ntop_5_features = effect_df.head(5)\nfor idx, row in top_5_features.iterrows():\n    print(f\"   ‚Ä¢ {row['Feature']}: Effect size = {row['Effect_Size']:.3f}\")\n\nprint(\"\\n5Ô∏è‚É£ Outlier Analysis:\")\nprint(f\"   ‚Ä¢ Outliers detected: {(df['Outlier'] == -1).sum()} ({(df['Outlier'] == -1).sum()/len(df)*100:.2f}%)\")\nprint(f\"   ‚Ä¢ Fraud rate in outliers: {outlier_fraud_rate*100:.2f}%\")\nprint(f\"   ‚Ä¢ Outliers are {outlier_fraud_rate/df['Class'].mean():.1f}x more likely to be fraud\")\n\nprint(\"\\n6Ô∏è‚É£ Recommendations for Modeling:\")\nprint(\"   ‚Ä¢ Use stratified sampling for train/test split\")\nprint(\"   ‚Ä¢ Consider SMOTE or class weights for imbalance\")\nprint(\"   ‚Ä¢ Focus on Precision-Recall instead of accuracy\")\nprint(\"   ‚Ä¢ Use ensemble methods to combine different approaches\")\nprint(\"   ‚Ä¢ Consider time-based validation splits\")\n\n# Create a final comprehensive visualization\nfig = plt.figure(figsize=(20, 12))\n\n# 1. Class distribution\nax1 = plt.subplot(3, 3, 1)\ndf['Class'].value_counts().plot(kind='pie', ax=ax1, autopct='%1.3f%%', \n                                colors=['#2ecc71', '#e74c3c'])\nax1.set_title('Class Distribution', fontweight='bold')\nax1.set_ylabel('')\n\n# 2. Top 5 discriminative features\nax2 = plt.subplot(3, 3, 2)\ntop_5_features.plot(x='Feature', y='Effect_Size', kind='bar', ax=ax2, color='#3498db')\nax2.set_title('Top 5 Discriminative Features', fontweight='bold')\nax2.set_xlabel('Feature')\nax2.set_ylabel('Effect Size (Cohen\\'s d)')\n\n# 3. Amount distribution comparison\nax3 = plt.subplot(3, 3, 3)\ndf[df['Class']==0]['Amount'].plot.hist(bins=50, alpha=0.7, label='Normal', \n                                       ax=ax3, color='#2ecc71', density=True)\ndf[df['Class']==1]['Amount'].plot.hist(bins=50, alpha=0.7, label='Fraud', \n                                       ax=ax3, color='#e74c3c', density=True)\nax3.set_xlabel('Amount ($)')\nax3.set_ylabel('Density')\nax3.set_title('Amount Distribution by Class', fontweight='bold')\nax3.legend()\nax3.set_xlim(0, 500)\n\n# 4. Time patterns\nax4 = plt.subplot(3, 3, 4)\nhourly_fraud = df.groupby(df['Hour'].astype(int))['Class'].agg(['sum', 'count'])\nhourly_fraud['rate'] = (hourly_fraud['sum'] / hourly_fraud['count']) * 100\nhourly_fraud['rate'].plot(ax=ax4, color='red', marker='o')\nax4.set_xlabel('Hour')\nax4.set_ylabel('Fraud Rate (%)')\nax4.set_title('Fraud Rate Over Time', fontweight='bold')\nax4.grid(True, alpha=0.3)\n\n# 5. Feature correlation with fraud\nax5 = plt.subplot(3, 3, 5)\ntop_corr = target_corr.drop('Class').abs().sort_values(ascending=False).head(10)\ntop_corr.plot(kind='barh', ax=ax5, color='#9b59b6')\nax5.set_xlabel('|Correlation| with Fraud')\nax5.set_title('Top 10 Feature Correlations', fontweight='bold')\n\n# 6. Outlier analysis\nax6 = plt.subplot(3, 3, 6)\noutlier_stats = pd.DataFrame({\n    'Normal': [normal_fraud_rate*100, (1-normal_fraud_rate)*100],\n    'Outlier': [outlier_fraud_rate*100, (1-outlier_fraud_rate)*100]\n}, index=['Fraud', 'Normal'])\noutlier_stats.T.plot(kind='bar', stacked=True, ax=ax6, color=['#e74c3c', '#2ecc71'])\nax6.set_xlabel('Point Type')\nax6.set_ylabel('Percentage')\nax6.set_title('Fraud Rate: Normal vs Outliers', fontweight='bold')\nax6.set_xticklabels(['Normal Points', 'Outliers'], rotation=0)\n\n# 7. PCA feature 1 vs 2\nax7 = plt.subplot(3, 3, 7)\nsample_normal = df[df['Class']==0].sample(1000)\nax7.scatter(sample_normal['V1'], sample_normal['V2'], alpha=0.5, s=5, \n           label='Normal', color='#2ecc71')\nax7.scatter(df[df['Class']==1]['V1'], df[df['Class']==1]['V2'], alpha=0.7, s=10, \n           label='Fraud', color='#e74c3c')\nax7.set_xlabel('V1')\nax7.set_ylabel('V2')\nax7.set_title('V1 vs V2 Feature Space', fontweight='bold')\nax7.legend()\n\n# 8. Transaction amount percentiles\nax8 = plt.subplot(3, 3, 8)\npercentiles = [10, 25, 50, 75, 90, 95, 99]\nnormal_percentiles = [df[df['Class']==0]['Amount'].quantile(p/100) for p in percentiles]\nfraud_percentiles = [df[df['Class']==1]['Amount'].quantile(p/100) for p in percentiles]\nx = np.arange(len(percentiles))\nwidth = 0.35\nax8.bar(x - width/2, normal_percentiles, width, label='Normal', color='#2ecc71')\nax8.bar(x + width/2, fraud_percentiles, width, label='Fraud', color='#e74c3c')\nax8.set_xlabel('Percentile')\nax8.set_ylabel('Amount ($)')\nax8.set_title('Amount Percentiles by Class', fontweight='bold')\nax8.set_xticks(x)\nax8.set_xticklabels(percentiles)\nax8.legend()\nax8.set_yscale('log')\n\n# 9. Sample size recommendations\nax9 = plt.subplot(3, 3, 9)\nstrategies = ['Original', 'Undersample', 'SMOTE', 'Class Weights']\nfraud_counts_viz = [class_counts[1], class_counts[1], class_counts[0], class_counts[1]]\nnormal_counts_viz = [class_counts[0], class_counts[1], class_counts[0], class_counts[0]]\nx = np.arange(len(strategies))\nax9.bar(x, normal_counts_viz, label='Normal', color='#2ecc71', alpha=0.7)\nax9.bar(x, fraud_counts_viz, bottom=normal_counts_viz, label='Fraud', color='#e74c3c', alpha=0.7)\nax9.set_xlabel('Strategy')\nax9.set_ylabel('Sample Count')\nax9.set_title('Class Balancing Strategies', fontweight='bold')\nax9.set_xticks(x)\nax9.set_xticklabels(strategies, rotation=45)\nax9.legend()\nax9.set_yscale('log')\n\nplt.suptitle('Credit Card Fraud Detection - Data Exploration Summary', fontsize=16, fontweight='bold')\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "li2wz7drw7b",
   "source": "## 11. Next Steps\n\nNow that we've thoroughly explored the data, we're ready to build fraud detection models! Here's what comes next:\n\n### üöÄ In the Next Tutorial (fraud_detection_models.ipynb):\n1. **Model Building**: Implement various ML algorithms (Logistic Regression, Random Forest, XGBoost, etc.)\n2. **Handling Imbalance**: Apply SMOTE, undersampling, and class weights\n3. **Model Evaluation**: Use appropriate metrics (Precision, Recall, F1, AUC-ROC)\n4. **Model Comparison**: Compare performance across different algorithms\n\n### üìö Additional Resources:\n- **Feature Engineering**: Create new features based on our exploration insights\n- **Advanced Techniques**: Deep learning, ensemble methods, and graph neural networks\n- **Production Deployment**: API development and real-time scoring\n\n### üí° Key Takeaways from Data Exploration:\n1. **Extreme class imbalance** requires special handling techniques\n2. **PCA features** (especially V14, V4, V12, V10) are highly discriminative\n3. **Transaction amounts** show different patterns for fraud vs normal\n4. **Outliers** are much more likely to be fraudulent\n5. **Time patterns** exist but are subtle\n\n## üéâ Congratulations!\n\nYou've completed the data exploration tutorial! You now understand:\n- The characteristics of credit card fraud data\n- How to identify and visualize class imbalance\n- Which features are most important for fraud detection\n- How to prepare for the modeling phase\n\n### üìù Practice Exercise:\nTry exploring the data with different visualization techniques or statistical tests. Can you find any other interesting patterns?\n\n### üîó Continue to the next tutorial:\nOpen `fraud_detection_models.ipynb` to start building your first fraud detection models!",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}