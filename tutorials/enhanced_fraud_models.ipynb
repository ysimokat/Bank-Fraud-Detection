{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced Fraud Detection Models Tutorial üöÄ\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By the end of this tutorial, you will understand:\n",
    "\n",
    "1. **Cost-Sensitive Learning** - Incorporating business costs into model training\n",
    "2. **Advanced Models** - XGBoost and LightGBM with early stopping\n",
    "3. **Comprehensive Evaluation** - Beyond accuracy to business impact\n",
    "4. **Cross-Validation Strategies** - Robust model evaluation\n",
    "5. **Model Selection** - Choosing models based on business objectives\n",
    "\n",
    "## üìã What This File Does\n",
    "The `enhanced_fraud_models.py` file implements:\n",
    "\n",
    "**üí∞ Cost-Sensitive Approach:**\n",
    "- Different costs for false positives vs false negatives\n",
    "- Investigation costs consideration\n",
    "- ROI-based model selection\n",
    "\n",
    "**ü§ñ Advanced Models:**\n",
    "- XGBoost with early stopping\n",
    "- LightGBM with class balancing\n",
    "- Multiple baseline models for comparison\n",
    "\n",
    "**üìä Business Metrics:**\n",
    "- Total cost analysis\n",
    "- Fraud prevention value\n",
    "- Alert efficiency metrics\n",
    "- Return on Investment (ROI)\n",
    "\n",
    "**üîç Comprehensive Evaluation:**\n",
    "- Cross-validation with multiple metrics\n",
    "- Cost-sensitive scoring\n",
    "- Business impact analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, \n",
    "    roc_auc_score, f1_score, precision_score, recall_score,\n",
    "    make_scorer, fbeta_score, cohen_kappa_score\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Advanced models\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# For saving results\n",
    "import joblib\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Enhanced Fraud Detection Models Tutorial\")\n",
    "print(f\"üìÖ Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Cost-Sensitive Learning\n",
    "\n",
    "In fraud detection, not all errors are equal. Let's understand the business costs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define business costs\n",
    "class BusinessCosts:\n",
    "    \"\"\"Define the business costs for fraud detection\"\"\"\n",
    "    def __init__(self):\n",
    "        # Cost of missing a fraud (False Negative)\n",
    "        self.fraud_cost = 200  # Average fraud amount\n",
    "        \n",
    "        # Cost of false alarm (False Positive)\n",
    "        self.false_positive_cost = 10  # Customer dissatisfaction, support time\n",
    "        \n",
    "        # Cost to investigate each alert\n",
    "        self.investigation_cost = 30  # Manual review cost\n",
    "        \n",
    "    def calculate_total_cost(self, confusion_matrix):\n",
    "        \"\"\"Calculate total cost based on confusion matrix\"\"\"\n",
    "        tn, fp, fn, tp = confusion_matrix.ravel()\n",
    "        \n",
    "        fraud_loss = fn * self.fraud_cost  # Missed frauds\n",
    "        false_alarm_cost = fp * self.false_positive_cost  # False alarms\n",
    "        investigation_cost = (tp + fp) * self.investigation_cost  # All alerts\n",
    "        \n",
    "        total_cost = fraud_loss + false_alarm_cost + investigation_cost\n",
    "        \n",
    "        return {\n",
    "            'fraud_loss': fraud_loss,\n",
    "            'false_alarm_cost': false_alarm_cost,\n",
    "            'investigation_cost': investigation_cost,\n",
    "            'total_cost': total_cost,\n",
    "            'fraud_prevented': tp * self.fraud_cost,\n",
    "            'net_benefit': tp * self.fraud_cost - total_cost\n",
    "        }\n",
    "\n",
    "# Initialize business costs\n",
    "costs = BusinessCosts()\n",
    "\n",
    "print(\"üí∞ Business Cost Structure:\")\n",
    "print(f\"  ‚Ä¢ Missing a fraud (FN): ${costs.fraud_cost}\")\n",
    "print(f\"  ‚Ä¢ False alarm (FP): ${costs.false_positive_cost}\")\n",
    "print(f\"  ‚Ä¢ Investigation per alert: ${costs.investigation_cost}\")\n",
    "print(\"\\nüìä This means:\")\n",
    "print(\"  ‚Ä¢ It's 20x more expensive to miss a fraud than to have a false alarm\")\n",
    "print(\"  ‚Ä¢ Every alert costs money to investigate\")\n",
    "print(\"  ‚Ä¢ We need to balance detection rate with operational costs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cost implications\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Cost comparison\n",
    "errors = ['False Negative\\n(Miss Fraud)', 'False Positive\\n(False Alarm)']\n",
    "error_costs = [costs.fraud_cost, costs.false_positive_cost + costs.investigation_cost]\n",
    "colors = ['#e74c3c', '#f39c12']\n",
    "\n",
    "ax1.bar(errors, error_costs, color=colors)\n",
    "ax1.set_title('Cost Comparison: FN vs FP', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('Cost ($)')\n",
    "for i, cost in enumerate(error_costs):\n",
    "    ax1.text(i, cost + 5, f'${cost}', ha='center', fontweight='bold')\n",
    "\n",
    "# ROI visualization\n",
    "fraud_caught = np.arange(0, 101, 10)\n",
    "false_alarms = fraud_caught * 5  # Assume 5 FP for each TP\n",
    "fraud_prevented_value = fraud_caught * costs.fraud_cost\n",
    "total_costs = false_alarms * (costs.false_positive_cost + costs.investigation_cost) + \\\n",
    "              fraud_caught * costs.investigation_cost\n",
    "net_benefit = fraud_prevented_value - total_costs\n",
    "\n",
    "ax2.plot(fraud_caught, fraud_prevented_value, label='Fraud Prevented Value', color='#27ae60', linewidth=2)\n",
    "ax2.plot(fraud_caught, total_costs, label='Total Costs', color='#e74c3c', linewidth=2)\n",
    "ax2.plot(fraud_caught, net_benefit, label='Net Benefit', color='#3498db', linewidth=3)\n",
    "ax2.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "ax2.set_xlabel('Number of Frauds Caught')\n",
    "ax2.set_ylabel('Amount ($)')\n",
    "ax2.set_title('Cost-Benefit Analysis', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading and Preparing Data\n",
    "\n",
    "Let's load our credit card fraud dataset with a focus on cost-sensitive preparation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('../creditcard.csv')\n",
    "\n",
    "print(\"üìä Dataset Information:\")\n",
    "print(f\"Total transactions: {len(df):,}\")\n",
    "print(f\"Features: {df.shape[1]}\")\n",
    "print(f\"Fraud rate: {df['Class'].mean()*100:.3f}%\")\n",
    "print(f\"Average transaction amount: ${df['Amount'].mean():.2f}\")\n",
    "print(f\"Average fraud amount: ${df[df['Class']==1]['Amount'].mean():.2f}\")\n",
    "\n",
    "# Calculate class weights for cost-sensitive learning\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced', \n",
    "    classes=np.unique(df['Class']), \n",
    "    y=df['Class']\n",
    ")\n",
    "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "\n",
    "print(f\"\\n‚öñÔ∏è Class Weights (for balanced learning):\")\n",
    "print(f\"Normal transactions (0): {class_weights[0]:.2f}\")\n",
    "print(f\"Fraud transactions (1): {class_weights[1]:.2f}\")\n",
    "print(f\"Weight ratio: {class_weights[1]/class_weights[0]:.0f}:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and split data\n",
    "feature_columns = [col for col in df.columns if col not in ['Class']]\n",
    "X = df[feature_columns]\n",
    "y = df['Class']\n",
    "\n",
    "# Train-test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\nüìÇ Data Split:\")\n",
    "print(f\"Training set: {len(X_train):,} samples ({y_train.sum()} frauds)\")\n",
    "print(f\"Test set: {len(X_test):,} samples ({y_test.sum()} frauds)\")\n",
    "\n",
    "# Create validation set for early stopping\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X_train_scaled, y_train, test_size=0.2, stratify=y_train, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Validation Split (for early stopping):\")\n",
    "print(f\"Training: {len(X_train_split):,} samples\")\n",
    "print(f\"Validation: {len(X_val_split):,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating Cost-Sensitive Scorer\n",
    "\n",
    "Let's create a custom scorer that considers business costs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cost-sensitive scorer\n",
    "def cost_sensitive_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate cost-sensitive score based on business costs.\n",
    "    Returns negative cost (lower is better for sklearn scorers).\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    # Calculate total cost\n",
    "    fraud_loss = fn * costs.fraud_cost  # Missed frauds\n",
    "    false_alarm_cost = fp * costs.false_positive_cost  # False alarms\n",
    "    investigation_cost = (tp + fp) * costs.investigation_cost  # All alerts\n",
    "    \n",
    "    total_cost = fraud_loss + false_alarm_cost + investigation_cost\n",
    "    \n",
    "    return -total_cost  # Negative because sklearn maximizes scores\n",
    "\n",
    "# Create scorer for cross-validation\n",
    "cost_scorer = make_scorer(cost_sensitive_score, greater_is_better=False)\n",
    "\n",
    "# Define multiple scorers for comprehensive evaluation\n",
    "scoring_metrics = {\n",
    "    'f1': 'f1',\n",
    "    'precision': 'precision',\n",
    "    'recall': 'recall',\n",
    "    'roc_auc': 'roc_auc',\n",
    "    'cost': cost_scorer\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Cost-sensitive scorer created!\")\n",
    "print(\"üìä This scorer will help us find models that minimize business costs, not just maximize accuracy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Baseline Models with Cost-Sensitive Approach\n",
    "\n",
    "Let's train several baseline models with class weights to handle imbalance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize results storage\n",
    "models = {}\n",
    "results = {}\n",
    "cv_scores = {}\n",
    "\n",
    "# Define baseline models\n",
    "baseline_models = {\n",
    "    'Logistic Regression': LogisticRegression(\n",
    "        class_weight='balanced', \n",
    "        random_state=42, \n",
    "        max_iter=1000\n",
    "    ),\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=100, \n",
    "        class_weight='balanced', \n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'Decision Tree': DecisionTreeClassifier(\n",
    "        class_weight='balanced', \n",
    "        random_state=42,\n",
    "        max_depth=10\n",
    "    ),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Neural Network': MLPClassifier(\n",
    "        hidden_layer_sizes=(100, 50),\n",
    "        random_state=42,\n",
    "        max_iter=500,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1\n",
    "    )\n",
    "}\n",
    "\n",
    "print(\"ü§ñ Training Baseline Models with Cost-Sensitive Approach\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform cross-validation\n",
    "def perform_cross_validation(model, X, y, model_name, cv_folds=3):\n",
    "    \"\"\"Perform comprehensive cross-validation with multiple metrics.\"\"\"\n",
    "    print(f\"\\nüîÑ Performing {cv_folds}-fold cross-validation for {model_name}\")\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "    cv_results = {}\n",
    "    \n",
    "    for metric_name, scorer in scoring_metrics.items():\n",
    "        scores = cross_val_score(model, X, y, cv=cv, scoring=scorer, n_jobs=-1)\n",
    "        \n",
    "        cv_results[f'{metric_name}_mean'] = scores.mean()\n",
    "        cv_results[f'{metric_name}_std'] = scores.std()\n",
    "        \n",
    "        if metric_name == 'cost':\n",
    "            print(f\"  {metric_name}: ${-scores.mean():.0f} (+/- ${scores.std() * 2:.0f})\")\n",
    "        else:\n",
    "            print(f\"  {metric_name}: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})\")\n",
    "    \n",
    "    return cv_results\n",
    "\n",
    "# Function to calculate business metrics\n",
    "def calculate_business_metrics(y_true, y_pred, y_pred_proba=None):\n",
    "    \"\"\"Calculate comprehensive business and performance metrics.\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    # Standard metrics\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    # Business cost metrics\n",
    "    cost_details = costs.calculate_total_cost(cm)\n",
    "    \n",
    "    # ROI calculation\n",
    "    roi = (cost_details['net_benefit'] / cost_details['total_cost']) * 100 if cost_details['total_cost'] > 0 else 0\n",
    "    \n",
    "    metrics = {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'tp': tp,\n",
    "        'fp': fp,\n",
    "        'tn': tn,\n",
    "        'fn': fn,\n",
    "        'total_cost': cost_details['total_cost'],\n",
    "        'fraud_prevented': cost_details['fraud_prevented'],\n",
    "        'net_benefit': cost_details['net_benefit'],\n",
    "        'roi': roi\n",
    "    }\n",
    "    \n",
    "    if y_pred_proba is not None:\n",
    "        metrics['roc_auc'] = roc_auc_score(y_true, y_pred_proba)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline models\n",
    "for name, model in baseline_models.items():\n",
    "    print(f\"\\nüöÄ Training {name}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_results = perform_cross_validation(model, X_train_scaled, y_train, name)\n",
    "    cv_scores[name] = cv_results\n",
    "    \n",
    "    # Train final model\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = calculate_business_metrics(y_test, y_pred, y_pred_proba)\n",
    "    \n",
    "    # Store results\n",
    "    models[name] = model\n",
    "    results[name] = metrics\n",
    "    \n",
    "    print(f\"\\nüìä Test Set Performance:\")\n",
    "    print(f\"  F1-Score: {metrics['f1_score']:.4f}\")\n",
    "    print(f\"  Total Cost: ${metrics['total_cost']:,.0f}\")\n",
    "    print(f\"  ROI: {metrics['roi']:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Advanced Models with Early Stopping\n",
    "\n",
    "Now let's train XGBoost and LightGBM with early stopping to prevent overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate scale_pos_weight for imbalanced data\n",
    "scale_pos_weight = len(y_train[y_train == 0]) / len(y_train[y_train == 1])\n",
    "\n",
    "print(\"üöÄ Training Advanced Models with Early Stopping\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Scale positive weight: {scale_pos_weight:.2f}\")\n",
    "\n",
    "# XGBoost with early stopping\n",
    "print(\"\\nüìä Training XGBoost with early stopping...\")\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=42,\n",
    "    early_stopping_rounds=20,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "# Fit with evaluation set\n",
    "xgb_model.fit(\n",
    "    X_train_split, y_train_split,\n",
    "    eval_set=[(X_val_split, y_val_split)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ XGBoost stopped at iteration: {xgb_model.best_iteration}\")\n",
    "\n",
    "# Predictions\n",
    "y_pred_xgb = xgb_model.predict(X_test_scaled)\n",
    "y_proba_xgb = xgb_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "xgb_metrics = calculate_business_metrics(y_test, y_pred_xgb, y_proba_xgb)\n",
    "models['XGBoost'] = xgb_model\n",
    "results['XGBoost'] = xgb_metrics\n",
    "\n",
    "print(f\"\\nüìä XGBoost Test Performance:\")\n",
    "print(f\"  F1-Score: {xgb_metrics['f1_score']:.4f}\")\n",
    "print(f\"  ROC-AUC: {xgb_metrics['roc_auc']:.4f}\")\n",
    "print(f\"  Total Cost: ${xgb_metrics['total_cost']:,.0f}\")\n",
    "print(f\"  ROI: {xgb_metrics['roi']:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM with early stopping\n",
    "print(\"\\nüìä Training LightGBM with early stopping...\")\n",
    "\n",
    "lgb_model = lgb.LGBMClassifier(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "# Fit with evaluation set and early stopping\n",
    "lgb_model.fit(\n",
    "    X_train_split, y_train_split,\n",
    "    eval_set=[(X_val_split, y_val_split)],\n",
    "    callbacks=[lgb.early_stopping(20), lgb.log_evaluation(0)]\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ LightGBM stopped at iteration: {lgb_model.best_iteration_}\")\n",
    "\n",
    "# Predictions\n",
    "y_pred_lgb = lgb_model.predict(X_test_scaled)\n",
    "y_proba_lgb = lgb_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "lgb_metrics = calculate_business_metrics(y_test, y_pred_lgb, y_proba_lgb)\n",
    "models['LightGBM'] = lgb_model\n",
    "results['LightGBM'] = lgb_metrics\n",
    "\n",
    "print(f\"\\nüìä LightGBM Test Performance:\")\n",
    "print(f\"  F1-Score: {lgb_metrics['f1_score']:.4f}\")\n",
    "print(f\"  ROC-AUC: {lgb_metrics['roc_auc']:.4f}\")\n",
    "print(f\"  Total Cost: ${lgb_metrics['total_cost']:,.0f}\")\n",
    "print(f\"  ROI: {lgb_metrics['roi']:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comprehensive Model Comparison\n",
    "\n",
    "Let's compare all models from both ML performance and business perspectives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "comparison_data = []\n",
    "\n",
    "for model_name, metrics in results.items():\n",
    "    row = {\n",
    "        'Model': model_name,\n",
    "        'Precision': metrics['precision'],\n",
    "        'Recall': metrics['recall'],\n",
    "        'F1-Score': metrics['f1_score'],\n",
    "        'ROC-AUC': metrics.get('roc_auc', 0),\n",
    "        'Total Cost': metrics['total_cost'],\n",
    "        'Net Benefit': metrics['net_benefit'],\n",
    "        'ROI (%)': metrics['roi'],\n",
    "        'Alerts': metrics['tp'] + metrics['fp'],\n",
    "        'Frauds Caught': metrics['tp'],\n",
    "        'False Alarms': metrics['fp']\n",
    "    }\n",
    "    comparison_data.append(row)\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('Total Cost')\n",
    "\n",
    "print(\"üìä Comprehensive Model Comparison\")\n",
    "print(\"=\" * 100)\n",
    "print(comparison_df.round(4).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# 1. F1-Score comparison\n",
    "ax1 = axes[0, 0]\n",
    "comparison_df.sort_values('F1-Score', ascending=False).plot.bar(\n",
    "    x='Model', y='F1-Score', ax=ax1, color='#3498db', legend=False\n",
    ")\n",
    "ax1.set_title('F1-Score by Model', fontsize=14, fontweight='bold')\n",
    "ax1.set_xticklabels(ax1.get_xticklabels(), rotation=45, ha='right')\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# 2. Total Cost comparison\n",
    "ax2 = axes[0, 1]\n",
    "comparison_df.plot.bar(\n",
    "    x='Model', y='Total Cost', ax=ax2, color='#e74c3c', legend=False\n",
    ")\n",
    "ax2.set_title('Total Cost by Model (Lower is Better)', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticklabels(ax2.get_xticklabels(), rotation=45, ha='right')\n",
    "ax2.set_ylabel('Total Cost ($)')\n",
    "\n",
    "# 3. ROI comparison\n",
    "ax3 = axes[0, 2]\n",
    "comparison_df.sort_values('ROI (%)', ascending=False).plot.bar(\n",
    "    x='Model', y='ROI (%)', ax=ax3, color='#27ae60', legend=False\n",
    ")\n",
    "ax3.set_title('Return on Investment by Model', fontsize=14, fontweight='bold')\n",
    "ax3.set_xticklabels(ax3.get_xticklabels(), rotation=45, ha='right')\n",
    "ax3.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "\n",
    "# 4. Precision vs Recall\n",
    "ax4 = axes[1, 0]\n",
    "ax4.scatter(comparison_df['Recall'], comparison_df['Precision'], s=200, alpha=0.6)\n",
    "for idx, row in comparison_df.iterrows():\n",
    "    ax4.annotate(row['Model'], (row['Recall'], row['Precision']), \n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "ax4.set_xlabel('Recall')\n",
    "ax4.set_ylabel('Precision')\n",
    "ax4.set_title('Precision vs Recall Trade-off', fontsize=14, fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Alert Volume Analysis\n",
    "ax5 = axes[1, 1]\n",
    "alert_data = comparison_df[['Model', 'Frauds Caught', 'False Alarms']]\n",
    "alert_data.set_index('Model').plot.bar(stacked=True, ax=ax5, \n",
    "                                       color=['#27ae60', '#e74c3c'])\n",
    "ax5.set_title('Alert Volume Analysis', fontsize=14, fontweight='bold')\n",
    "ax5.set_ylabel('Number of Alerts')\n",
    "ax5.set_xticklabels(ax5.get_xticklabels(), rotation=45, ha='right')\n",
    "ax5.legend(['Frauds Caught', 'False Alarms'])\n",
    "\n",
    "# 6. Cost Breakdown\n",
    "ax6 = axes[1, 2]\n",
    "best_model = comparison_df.iloc[0]['Model']\n",
    "best_metrics = results[best_model]\n",
    "cost_breakdown = [\n",
    "    best_metrics['fn'] * costs.fraud_cost,\n",
    "    best_metrics['fp'] * costs.false_positive_cost,\n",
    "    (best_metrics['tp'] + best_metrics['fp']) * costs.investigation_cost\n",
    "]\n",
    "labels = ['Fraud Losses', 'False Alarm Cost', 'Investigation Cost']\n",
    "ax6.pie(cost_breakdown, labels=labels, autopct='%1.1f%%', \n",
    "        colors=['#e74c3c', '#f39c12', '#3498db'])\n",
    "ax6.set_title(f'Cost Breakdown - {best_model}', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance Analysis\n",
    "\n",
    "Let's analyze which features are most important for fraud detection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from tree-based models\n",
    "importance_models = ['Random Forest', 'XGBoost', 'LightGBM']\n",
    "feature_importance_dict = {}\n",
    "\n",
    "for model_name in importance_models:\n",
    "    if model_name in models:\n",
    "        model = models[model_name]\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            feature_importance_dict[model_name] = model.feature_importances_\n",
    "\n",
    "# Create feature importance DataFrame\n",
    "feature_names = feature_columns\n",
    "importance_df = pd.DataFrame(feature_importance_dict, index=feature_names)\n",
    "\n",
    "# Calculate average importance\n",
    "importance_df['Average'] = importance_df.mean(axis=1)\n",
    "importance_df = importance_df.sort_values('Average', ascending=False)\n",
    "\n",
    "# Plot top 15 features\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = importance_df.head(15)\n",
    "\n",
    "# Create horizontal bar plot\n",
    "y_pos = np.arange(len(top_features))\n",
    "plt.barh(y_pos, top_features['Average'], color='#3498db')\n",
    "plt.yticks(y_pos, top_features.index)\n",
    "plt.xlabel('Average Feature Importance')\n",
    "plt.title('Top 15 Most Important Features for Fraud Detection', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Add importance values\n",
    "for i, v in enumerate(top_features['Average']):\n",
    "    plt.text(v + 0.001, i, f'{v:.4f}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üîù Top 10 Most Important Features:\")\n",
    "print(top_features['Average'].head(10).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Business Recommendations\n",
    "\n",
    "Based on our comprehensive analysis, let's provide actionable recommendations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best model based on different criteria\n",
    "best_f1_model = comparison_df.loc[comparison_df['F1-Score'].idxmax(), 'Model']\n",
    "best_cost_model = comparison_df.loc[comparison_df['Total Cost'].idxmin(), 'Model']\n",
    "best_roi_model = comparison_df.loc[comparison_df['ROI (%)'].idxmax(), 'Model']\n",
    "\n",
    "print(\"üíº Business Recommendations\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüèÜ Best Models by Criteria:\")\n",
    "print(f\"  ‚Ä¢ Best F1-Score: {best_f1_model} ({comparison_df[comparison_df['Model']==best_f1_model]['F1-Score'].values[0]:.4f})\")\n",
    "print(f\"  ‚Ä¢ Lowest Cost: {best_cost_model} (${comparison_df[comparison_df['Model']==best_cost_model]['Total Cost'].values[0]:,.0f})\")\n",
    "print(f\"  ‚Ä¢ Best ROI: {best_roi_model} ({comparison_df[comparison_df['Model']==best_roi_model]['ROI (%)'].values[0]:.1f}%)\")\n",
    "\n",
    "# Calculate potential savings\n",
    "baseline_cost = comparison_df['Total Cost'].max()\n",
    "best_cost = comparison_df['Total Cost'].min()\n",
    "savings = baseline_cost - best_cost\n",
    "savings_percent = (savings / baseline_cost) * 100\n",
    "\n",
    "print(f\"\\nüí∞ Potential Cost Savings:\")\n",
    "print(f\"  ‚Ä¢ Maximum savings: ${savings:,.0f} ({savings_percent:.1f}% reduction)\")\n",
    "print(f\"  ‚Ä¢ Annual projection (assuming similar volume): ${savings * 365 / 2:,.0f}\")\n",
    "\n",
    "# Alert workload analysis\n",
    "best_model_alerts = comparison_df[comparison_df['Model']==best_cost_model]['Alerts'].values[0]\n",
    "total_transactions = len(y_test)\n",
    "alert_rate = (best_model_alerts / total_transactions) * 100\n",
    "\n",
    "print(f\"\\nüìä Operational Impact:\")\n",
    "print(f\"  ‚Ä¢ Alert rate: {alert_rate:.2f}% of transactions\")\n",
    "print(f\"  ‚Ä¢ Daily alerts (estimated): {best_model_alerts * 365 / 2:.0f}\")\n",
    "print(f\"  ‚Ä¢ Precision: {comparison_df[comparison_df['Model']==best_cost_model]['Precision'].values[0]:.1%} of alerts are actual fraud\")\n",
    "\n",
    "print(\"\\nüéØ Recommendations:\")\n",
    "print(f\"  1. Deploy {best_cost_model} for production use\")\n",
    "print(\"  2. Set up monitoring for model drift and performance degradation\")\n",
    "print(\"  3. Consider A/B testing with current system\")\n",
    "print(\"  4. Implement feedback loop for continuous improvement\")\n",
    "print(\"  5. Review and adjust cost parameters quarterly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Saving Models and Results\n",
    "\n",
    "Let's save our trained models and results for future use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all models and results\n",
    "save_data = {\n",
    "    'models': models,\n",
    "    'scaler': scaler,\n",
    "    'results': results,\n",
    "    'comparison': comparison_df,\n",
    "    'cost_config': {\n",
    "        'fraud_cost': costs.fraud_cost,\n",
    "        'false_positive_cost': costs.false_positive_cost,\n",
    "        'investigation_cost': costs.investigation_cost\n",
    "    },\n",
    "    'feature_columns': feature_columns\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "joblib.dump(save_data, '../enhanced_fraud_models_tutorial.joblib')\n",
    "print(\"‚úÖ Models and results saved to 'enhanced_fraud_models_tutorial.joblib'\")\n",
    "\n",
    "# Save comparison report\n",
    "comparison_df.to_csv('../model_comparison_report.csv', index=False)\n",
    "print(\"‚úÖ Comparison report saved to 'model_comparison_report.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Key Takeaways and Conclusions\n",
    "\n",
    "### üéØ What We Learned:\n",
    "\n",
    "1. **Cost-Sensitive Learning is Crucial**:\n",
    "   - Traditional accuracy metrics are misleading for imbalanced data\n",
    "   - Business costs should drive model selection\n",
    "   - ROI provides a holistic view of model value\n",
    "\n",
    "2. **Advanced Models Excel**:\n",
    "   - XGBoost and LightGBM often outperform traditional models\n",
    "   - Early stopping prevents overfitting\n",
    "   - Proper class balancing is essential\n",
    "\n",
    "3. **Business Impact Matters**:\n",
    "   - Lower F1-score might be acceptable if costs are lower\n",
    "   - Alert volume affects operational capacity\n",
    "   - False positive costs add up quickly\n",
    "\n",
    "4. **Feature Importance Insights**:\n",
    "   - PCA features (V-series) are highly predictive\n",
    "   - Transaction amount plays a role\n",
    "   - Time patterns exist but are subtle\n",
    "\n",
    "### üí° Best Practices:\n",
    "\n",
    "1. **Always consider business costs** when selecting models\n",
    "2. **Use multiple evaluation metrics** for comprehensive assessment\n",
    "3. **Implement early stopping** for gradient boosting models\n",
    "4. **Monitor feature importance** for model interpretability\n",
    "5. **Plan for operational impact** of model deployment\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "\n",
    "In the upcoming tutorials, you'll learn:\n",
    "- **Model Calibration**: Adjusting prediction thresholds for optimal performance\n",
    "- **Deep Learning**: Using autoencoders and neural networks\n",
    "- **Real-time Systems**: Building production-ready APIs\n",
    "- **Advanced Techniques**: Graph neural networks and ensemble methods\n",
    "\n",
    "### üìù Practice Exercises:\n",
    "\n",
    "1. Try adjusting the business costs and see how model selection changes\n",
    "2. Experiment with different XGBoost hyperparameters\n",
    "3. Implement a custom threshold based on cost optimization\n",
    "4. Create an ensemble of the top 3 models\n",
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "You've successfully implemented cost-sensitive fraud detection models! You now understand:\n",
    "- How to incorporate business costs into ML workflows\n",
    "- Advanced model training with early stopping\n",
    "- Comprehensive evaluation beyond accuracy\n",
    "- Making data-driven business recommendations\n",
    "\n",
    "Ready to calibrate your models for production? Check out `advanced_model_calibration.ipynb`!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}