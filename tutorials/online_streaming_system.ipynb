{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online Streaming Fraud Detection System\n",
    "\n",
    "## Tutorial 7: Real-Time Fraud Detection with Online Learning\n",
    "\n",
    "In this tutorial, you'll learn how to build production-ready streaming fraud detection systems:\n",
    "- **Online Learning**: Incremental learning without retraining\n",
    "- **Concept Drift**: Detecting and adapting to changing patterns\n",
    "- **Real-Time Processing**: High-throughput streaming architecture\n",
    "- **Adaptive Ensembles**: Dynamic model weighting and adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you'll understand:\n",
    "\n",
    "1. **Online vs Batch Learning**: Why streaming systems need different approaches\n",
    "2. **Concept Drift**: How fraud patterns evolve and how to detect changes\n",
    "3. **Incremental Learning**: Update models with new data without full retraining\n",
    "4. **Streaming Architecture**: Build scalable real-time processing systems\n",
    "5. **Adaptive Ensembles**: Combine multiple models with dynamic weighting\n",
    "6. **Performance Monitoring**: Track system health in real-time\n",
    "7. **Production Deployment**: Handle threading, queues, and error recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier, PassiveAggressiveClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "from collections import deque, defaultdict\n",
    "import threading\n",
    "import queue\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Online Streaming Fraud Detection System\")\n",
    "print(\"Real-time learning and adaptation tutorial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Online Learning vs Batch Learning\n",
    "\n",
    "### The Challenge of Real-Time Fraud Detection\n",
    "\n",
    "Traditional ML approaches (batch learning) have limitations in fraud detection:\n",
    "- **Static Models**: Trained once, performance degrades over time\n",
    "- **Concept Drift**: Fraud patterns constantly evolve\n",
    "- **Latency**: Retraining takes hours or days\n",
    "- **Scalability**: Cannot handle continuous high-volume streams\n",
    "\n",
    "### Online Learning Advantages\n",
    "\n",
    "Online learning addresses these challenges:\n",
    "- **Incremental Updates**: Learn from each new transaction\n",
    "- **Adaptation**: Automatically adjust to changing patterns\n",
    "- **Low Latency**: Real-time predictions and updates\n",
    "- **Memory Efficiency**: No need to store entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "df = pd.read_csv('creditcard.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Fraud rate: {df['Class'].mean()*100:.3f}%\")\n",
    "\n",
    "# Sort by time for streaming simulation\n",
    "df_sorted = df.sort_values('Time').reset_index(drop=True)\n",
    "\n",
    "# Prepare features\n",
    "feature_columns = [col for col in df.columns if col not in ['Class', 'Time']]\n",
    "X = df_sorted[feature_columns]\n",
    "y = df_sorted['Class']\n",
    "times = df_sorted['Time']\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"\\nFeatures: {len(feature_columns)}\")\n",
    "print(f\"Transactions: {len(X):,}\")\n",
    "print(f\"Time range: {times.min():.0f} to {times.max():.0f} seconds\")\n",
    "\n",
    "# Visualize the concept of streaming vs batch learning\n",
    "def visualize_learning_paradigms():\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Batch Learning\n",
    "    ax1.text(0.5, 0.9, 'Batch Learning', ha='center', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Training phase\n",
    "    ax1.add_patch(plt.Rectangle((0.1, 0.7), 0.8, 0.1, facecolor='lightblue', alpha=0.7))\n",
    "    ax1.text(0.5, 0.75, 'Training Phase\\n(Hours/Days)', ha='center', fontsize=12)\n",
    "    \n",
    "    # Deployment phase\n",
    "    ax1.add_patch(plt.Rectangle((0.1, 0.5), 0.8, 0.1, facecolor='lightgreen', alpha=0.7))\n",
    "    ax1.text(0.5, 0.55, 'Deployment Phase\\n(Static Model)', ha='center', fontsize=12)\n",
    "    \n",
    "    # Performance degradation\n",
    "    ax1.add_patch(plt.Rectangle((0.1, 0.3), 0.8, 0.1, facecolor='lightcoral', alpha=0.7))\n",
    "    ax1.text(0.5, 0.35, 'Performance Degradation\\n(Concept Drift)', ha='center', fontsize=12)\n",
    "    \n",
    "    # Retraining\n",
    "    ax1.add_patch(plt.Rectangle((0.1, 0.1), 0.8, 0.1, facecolor='lightyellow', alpha=0.7))\n",
    "    ax1.text(0.5, 0.15, 'Retraining Required\\n(Manual Process)', ha='center', fontsize=12)\n",
    "    \n",
    "    ax1.set_xlim(0, 1)\n",
    "    ax1.set_ylim(0, 1)\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # Online Learning\n",
    "    ax2.text(0.5, 0.9, 'Online Learning', ha='center', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Continuous learning\n",
    "    ax2.add_patch(plt.Rectangle((0.1, 0.6), 0.8, 0.2, facecolor='lightgreen', alpha=0.7))\n",
    "    ax2.text(0.5, 0.7, 'Continuous Learning\\n(Real-time Updates)', ha='center', fontsize=12)\n",
    "    \n",
    "    # Adaptive performance\n",
    "    ax2.add_patch(plt.Rectangle((0.1, 0.3), 0.8, 0.2, facecolor='lightblue', alpha=0.7))\n",
    "    ax2.text(0.5, 0.4, 'Adaptive Performance\\n(Concept Drift Handling)', ha='center', fontsize=12)\n",
    "    \n",
    "    # Real-time processing\n",
    "    ax2.add_patch(plt.Rectangle((0.1, 0.1), 0.8, 0.1, facecolor='lightyellow', alpha=0.7))\n",
    "    ax2.text(0.5, 0.15, 'Real-time Processing\\n(Low Latency)', ha='center', fontsize=12)\n",
    "    \n",
    "    ax2.set_xlim(0, 1)\n",
    "    ax2.set_ylim(0, 1)\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Key Differences:\")\n",
    "    print(\"1. Batch: Train once, deploy static model\")\n",
    "    print(\"2. Online: Continuous learning from each transaction\")\n",
    "    print(\"3. Batch: Performance degrades over time\")\n",
    "    print(\"4. Online: Adapts to changing patterns automatically\")\n",
    "\n",
    "visualize_learning_paradigms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Concept Drift in Fraud Detection\n",
    "\n",
    "### Understanding Concept Drift\n",
    "\n",
    "Concept drift occurs when the underlying data distribution changes over time:\n",
    "- **Fraud Evolution**: Attackers develop new techniques\n",
    "- **Seasonal Patterns**: Shopping behaviors change during holidays\n",
    "- **Economic Changes**: Financial crises affect transaction patterns\n",
    "- **Technology Adoption**: New payment methods emerge\n",
    "\n",
    "### Types of Concept Drift\n",
    "\n",
    "1. **Sudden Drift**: Abrupt change in patterns\n",
    "2. **Gradual Drift**: Slow evolution over time\n",
    "3. **Incremental Drift**: Small continuous changes\n",
    "4. **Recurring Drift**: Cyclical patterns that repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate concept drift in fraud patterns\n",
    "def simulate_concept_drift(X, y, times, drift_points=[0.3, 0.7]):\n",
    "    \"\"\"\n",
    "    Simulate concept drift by modifying fraud patterns at specific time points.\n",
    "    \n",
    "    Args:\n",
    "        X: Features\n",
    "        y: Labels\n",
    "        times: Timestamps\n",
    "        drift_points: Relative time points where drift occurs\n",
    "    \n",
    "    Returns:\n",
    "        Modified features with simulated drift\n",
    "    \"\"\"\n",
    "    X_drift = X.copy()\n",
    "    time_max = times.max()\n",
    "    \n",
    "    for i, drift_point in enumerate(drift_points):\n",
    "        drift_time = time_max * drift_point\n",
    "        \n",
    "        # Find transactions after drift point\n",
    "        drift_mask = (times > drift_time) & (y == 1)  # Only fraud transactions\n",
    "        \n",
    "        if drift_mask.sum() > 0:\n",
    "            # Simulate drift by shifting fraud patterns\n",
    "            shift_amount = (i + 1) * 0.5  # Increasing shift for later drifts\n",
    "            \n",
    "            # Shift specific features that might change in fraud patterns\n",
    "            feature_indices = [0, 1, 2, 3, 4]  # V1-V5 features\n",
    "            for idx in feature_indices:\n",
    "                X_drift[drift_mask, idx] += shift_amount\n",
    "    \n",
    "    return X_drift\n",
    "\n",
    "# Create dataset with concept drift\n",
    "X_with_drift = simulate_concept_drift(X_scaled, y, times)\n",
    "\n",
    "# Visualize concept drift\n",
    "def visualize_concept_drift(X_original, X_drift, y, times):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Create time windows\n",
    "    time_windows = np.linspace(times.min(), times.max(), 4)\n",
    "    window_labels = ['Early', 'Mid-Early', 'Mid-Late', 'Late']\n",
    "    \n",
    "    for i, (start_time, end_time, label) in enumerate(zip(time_windows[:-1], time_windows[1:], window_labels)):\n",
    "        ax = axes[i//2, i%2]\n",
    "        \n",
    "        # Get transactions in this time window\n",
    "        window_mask = (times >= start_time) & (times < end_time)\n",
    "        fraud_mask = window_mask & (y == 1)\n",
    "        normal_mask = window_mask & (y == 0)\n",
    "        \n",
    "        # Plot original vs drifted patterns (using first two features)\n",
    "        if fraud_mask.sum() > 0:\n",
    "            ax.scatter(X_original[normal_mask, 0], X_original[normal_mask, 1], \n",
    "                      alpha=0.3, s=10, label='Normal', color='blue')\n",
    "            ax.scatter(X_original[fraud_mask, 0], X_original[fraud_mask, 1], \n",
    "                      alpha=0.7, s=30, label='Fraud (Original)', color='red', marker='o')\n",
    "            ax.scatter(X_drift[fraud_mask, 0], X_drift[fraud_mask, 1], \n",
    "                      alpha=0.7, s=30, label='Fraud (Drifted)', color='orange', marker='s')\n",
    "        \n",
    "        ax.set_xlabel('Feature V1')\n",
    "        ax.set_ylabel('Feature V2')\n",
    "        ax.set_title(f'{label} Period')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Concept Drift Simulation:\")\n",
    "    print(\"- Orange squares show how fraud patterns change over time\")\n",
    "    print(\"- Red circles show original fraud patterns\")\n",
    "    print(\"- Blue dots show normal transactions (unchanged)\")\n",
    "\n",
    "# Visualize the drift\n",
    "visualize_concept_drift(X_scaled, X_with_drift, y, times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Adaptive Online Classifier\n",
    "\n",
    "### Key Components of Online Learning\n",
    "\n",
    "1. **Incremental Updates**: `partial_fit()` method for streaming updates\n",
    "2. **Forgetting Factor**: Give higher weight to recent examples\n",
    "3. **Drift Detection**: Monitor performance to detect changes\n",
    "4. **Adaptive Learning**: Adjust learning rate based on performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveOnlineClassifier:\n",
    "    \"\"\"\n",
    "    Adaptive online classifier with concept drift detection and handling.\n",
    "    \n",
    "    Key features:\n",
    "    - Incremental learning with forgetting factor\n",
    "    - Automatic drift detection\n",
    "    - Adaptive learning rate adjustment\n",
    "    - Performance monitoring\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_model_type='sgd', learning_rate=0.01, \n",
    "                 forgetting_factor=0.95, drift_threshold=0.05):\n",
    "        \"\"\"\n",
    "        Initialize adaptive online classifier.\n",
    "        \n",
    "        Args:\n",
    "            base_model_type: Type of base model ('sgd' or 'passive_aggressive')\n",
    "            learning_rate: Initial learning rate\n",
    "            forgetting_factor: Weight decay for older samples (0.9-0.99)\n",
    "            drift_threshold: Performance drop threshold for drift detection\n",
    "        \"\"\"\n",
    "        self.base_model_type = base_model_type\n",
    "        self.learning_rate = learning_rate\n",
    "        self.initial_learning_rate = learning_rate\n",
    "        self.forgetting_factor = forgetting_factor\n",
    "        self.drift_threshold = drift_threshold\n",
    "        \n",
    "        # Initialize base model\n",
    "        if base_model_type == 'sgd':\n",
    "            self.model = SGDClassifier(\n",
    "                loss='log_loss',\n",
    "                learning_rate='constant',\n",
    "                eta0=learning_rate,\n",
    "                random_state=42\n",
    "            )\n",
    "        elif base_model_type == 'passive_aggressive':\n",
    "            self.model = PassiveAggressiveClassifier(\n",
    "                C=1.0,\n",
    "                random_state=42\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model type: {base_model_type}\")\n",
    "        \n",
    "        # Performance tracking\n",
    "        self.performance_history = deque(maxlen=100)\n",
    "        self.recent_performance = deque(maxlen=20)\n",
    "        self.is_fitted = False\n",
    "        self.drift_detected = False\n",
    "        \n",
    "        # Sample weighting\n",
    "        self.sample_weights = deque(maxlen=1000)\n",
    "        \n",
    "    def partial_fit(self, X, y, sample_weight=None):\n",
    "        \"\"\"\n",
    "        Incremental learning with forgetting factor.\n",
    "        \n",
    "        Args:\n",
    "            X: Features (n_samples, n_features)\n",
    "            y: Labels (n_samples,)\n",
    "            sample_weight: Optional sample weights\n",
    "        \"\"\"\n",
    "        # Initialize classes if first fit\n",
    "        if not self.is_fitted:\n",
    "            self.model.partial_fit(X, y, classes=[0, 1])\n",
    "            self.is_fitted = True\n",
    "        else:\n",
    "            # Apply forgetting factor to sample weights\n",
    "            if sample_weight is None:\n",
    "                # Create exponential decay weights\n",
    "                n_samples = len(X)\n",
    "                weights = np.array([self.forgetting_factor ** (n_samples - i - 1) \n",
    "                                  for i in range(n_samples)])\n",
    "            else:\n",
    "                weights = sample_weight\n",
    "            \n",
    "            # Update model\n",
    "            self.model.partial_fit(X, y, sample_weight=weights)\n",
    "        \n",
    "        # Store sample weights for analysis\n",
    "        if len(X) > 0:\n",
    "            self.sample_weights.extend(weights if 'weights' in locals() else [1.0] * len(X))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions.\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            return np.zeros(len(X))\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict probabilities.\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            # Return uniform probabilities\n",
    "            return np.full((len(X), 2), 0.5)\n",
    "        \n",
    "        if hasattr(self.model, 'predict_proba'):\n",
    "            return self.model.predict_proba(X)\n",
    "        else:\n",
    "            # For models without predict_proba, use decision function\n",
    "            decisions = self.model.decision_function(X)\n",
    "            # Convert to probabilities using sigmoid\n",
    "            prob_pos = 1 / (1 + np.exp(-decisions))\n",
    "            return np.column_stack([1 - prob_pos, prob_pos])\n",
    "    \n",
    "    def update_performance(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Update performance metrics and detect drift.\n",
    "        \n",
    "        Args:\n",
    "            y_true: True labels\n",
    "            y_pred: Predicted labels\n",
    "        \"\"\"\n",
    "        # Calculate current performance\n",
    "        current_accuracy = accuracy_score(y_true, y_pred)\n",
    "        current_f1 = f1_score(y_true, y_pred) if len(np.unique(y_true)) > 1 else 0.0\n",
    "        \n",
    "        # Update performance history\n",
    "        self.performance_history.append(current_accuracy)\n",
    "        self.recent_performance.append(current_accuracy)\n",
    "        \n",
    "        # Detect drift\n",
    "        self.drift_detected = self._detect_drift()\n",
    "        \n",
    "        # Handle drift if detected\n",
    "        if self.drift_detected:\n",
    "            self._handle_drift()\n",
    "        \n",
    "        # Adapt learning rate\n",
    "        self._adapt_learning_rate()\n",
    "    \n",
    "    def _detect_drift(self):\n",
    "        \"\"\"\n",
    "        Detect concept drift based on performance degradation.\n",
    "        \n",
    "        Returns:\n",
    "            bool: True if drift detected, False otherwise\n",
    "        \"\"\"\n",
    "        if len(self.performance_history) < 50:  # Need sufficient history\n",
    "            return False\n",
    "        \n",
    "        # Compare recent performance with historical average\n",
    "        historical_avg = np.mean(list(self.performance_history)[:-20])\n",
    "        recent_avg = np.mean(list(self.recent_performance))\n",
    "        \n",
    "        # Drift detected if recent performance significantly drops\n",
    "        performance_drop = historical_avg - recent_avg\n",
    "        return performance_drop > self.drift_threshold\n",
    "    \n",
    "    def _handle_drift(self):\n",
    "        \"\"\"\n",
    "        Handle detected concept drift.\n",
    "        \n",
    "        Strategies:\n",
    "        1. Increase learning rate for faster adaptation\n",
    "        2. Reset performance history\n",
    "        3. Reduce forgetting factor (focus more on recent data)\n",
    "        \"\"\"\n",
    "        # Increase learning rate temporarily\n",
    "        self.learning_rate = min(self.learning_rate * 2, 0.1)\n",
    "        \n",
    "        # Update model learning rate if possible\n",
    "        if hasattr(self.model, 'eta0'):\n",
    "            self.model.eta0 = self.learning_rate\n",
    "        \n",
    "        # Reset performance history\n",
    "        self.performance_history.clear()\n",
    "        self.recent_performance.clear()\n",
    "        \n",
    "        # Reduce forgetting factor to focus on recent data\n",
    "        self.forgetting_factor = max(self.forgetting_factor * 0.9, 0.8)\n",
    "        \n",
    "        print(f\"Drift detected! Adapted learning rate to {self.learning_rate:.4f}\")\n",
    "    \n",
    "    def _adapt_learning_rate(self):\n",
    "        \"\"\"\n",
    "        Adapt learning rate based on performance trends.\n",
    "        \n",
    "        - Increase if performance is declining\n",
    "        - Decrease if performance is stable\n",
    "        \"\"\"\n",
    "        if len(self.recent_performance) < 10:\n",
    "            return\n",
    "        \n",
    "        # Calculate performance trend\n",
    "        recent_scores = list(self.recent_performance)\n",
    "        early_avg = np.mean(recent_scores[:5])\n",
    "        late_avg = np.mean(recent_scores[-5:])\n",
    "        \n",
    "        # Adapt learning rate\n",
    "        if late_avg < early_avg - 0.02:  # Performance declining\n",
    "            self.learning_rate = min(self.learning_rate * 1.1, 0.1)\n",
    "        elif late_avg > early_avg + 0.01:  # Performance improving\n",
    "            self.learning_rate = max(self.learning_rate * 0.95, self.initial_learning_rate)\n",
    "        \n",
    "        # Update model learning rate\n",
    "        if hasattr(self.model, 'eta0'):\n",
    "            self.model.eta0 = self.learning_rate\n",
    "    \n",
    "    def get_status(self):\n",
    "        \"\"\"\n",
    "        Get current status of the classifier.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Status information\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'is_fitted': self.is_fitted,\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'forgetting_factor': self.forgetting_factor,\n",
    "            'drift_detected': self.drift_detected,\n",
    "            'performance_history_length': len(self.performance_history),\n",
    "            'recent_performance_avg': np.mean(self.recent_performance) if self.recent_performance else 0,\n",
    "            'sample_weights_avg': np.mean(self.sample_weights) if self.sample_weights else 0\n",
    "        }\n",
    "\n",
    "# Test the adaptive classifier\n",
    "print(\"Testing Adaptive Online Classifier...\")\n",
    "\n",
    "# Create classifier\n",
    "classifier = AdaptiveOnlineClassifier(base_model_type='sgd', learning_rate=0.01)\n",
    "\n",
    "# Simulate streaming learning\n",
    "batch_size = 100\n",
    "performance_track = []\n",
    "drift_points = []\n",
    "\n",
    "for i in range(0, min(5000, len(X_with_drift)), batch_size):\n",
    "    end_idx = min(i + batch_size, len(X_with_drift))\n",
    "    \n",
    "    X_batch = X_with_drift[i:end_idx]\n",
    "    y_batch = y.iloc[i:end_idx]\n",
    "    \n",
    "    # Train on batch\n",
    "    classifier.partial_fit(X_batch, y_batch)\n",
    "    \n",
    "    # Evaluate on batch\n",
    "    y_pred = classifier.predict(X_batch)\n",
    "    accuracy = accuracy_score(y_batch, y_pred)\n",
    "    \n",
    "    # Update performance\n",
    "    classifier.update_performance(y_batch, y_pred)\n",
    "    \n",
    "    # Track performance\n",
    "    performance_track.append(accuracy)\n",
    "    \n",
    "    # Track drift detection\n",
    "    if classifier.drift_detected:\n",
    "        drift_points.append(i // batch_size)\n",
    "\n",
    "# Visualize performance\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(performance_track, 'b-', linewidth=2, label='Accuracy')\n",
    "plt.axhline(y=np.mean(performance_track), color='g', linestyle='--', label='Average')\n",
    "\n",
    "# Mark drift points\n",
    "for drift_point in drift_points:\n",
    "    plt.axvline(x=drift_point, color='r', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.xlabel('Batch Number')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Adaptive Online Classifier Performance')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Print final status\n",
    "print(\"\\nFinal Classifier Status:\")\n",
    "status = classifier.get_status()\n",
    "for key, value in status.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nDrift detected {len(drift_points)} times at batches: {drift_points}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Online Ensemble Methods\n",
    "\n",
    "### Why Ensembles for Online Learning?\n",
    "\n",
    "Online ensembles provide several advantages:\n",
    "- **Robustness**: Multiple models reduce individual model failures\n",
    "- **Diversity**: Different models capture different patterns\n",
    "- **Adaptation**: Poor-performing models get lower weights\n",
    "- **Stability**: Ensemble predictions are more stable than individual models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnlineEnsemble:\n",
    "    \"\"\"\n",
    "    Online ensemble of adaptive classifiers with dynamic weighting.\n",
    "    \n",
    "    Features:\n",
    "    - Multiple diverse base models\n",
    "    - Performance-based weighting\n",
    "    - Automatic model selection\n",
    "    - Robust error handling\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_models=3, learning_rate=0.01, forgetting_factor=0.95):\n",
    "        \"\"\"\n",
    "        Initialize online ensemble.\n",
    "        \n",
    "        Args:\n",
    "            n_models: Number of base models\n",
    "            learning_rate: Learning rate for base models\n",
    "            forgetting_factor: Forgetting factor for base models\n",
    "        \"\"\"\n",
    "        self.n_models = n_models\n",
    "        self.learning_rate = learning_rate\n",
    "        self.forgetting_factor = forgetting_factor\n",
    "        \n",
    "        # Create diverse base models\n",
    "        self.models = []\n",
    "        model_configs = [\n",
    "            {'base_model_type': 'sgd', 'learning_rate': learning_rate},\n",
    "            {'base_model_type': 'sgd', 'learning_rate': learning_rate * 0.5},\n",
    "            {'base_model_type': 'passive_aggressive', 'learning_rate': learning_rate}\n",
    "        ]\n",
    "        \n",
    "        for i in range(n_models):\n",
    "            config = model_configs[i % len(model_configs)]\n",
    "            model = AdaptiveOnlineClassifier(\n",
    "                base_model_type=config['base_model_type'],\n",
    "                learning_rate=config['learning_rate'],\n",
    "                forgetting_factor=forgetting_factor\n",
    "            )\n",
    "            self.models.append(model)\n",
    "        \n",
    "        # Model weights (performance-based)\n",
    "        self.model_weights = np.ones(n_models) / n_models\n",
    "        self.model_performances = [deque(maxlen=50) for _ in range(n_models)]\n",
    "        \n",
    "        # Ensemble performance tracking\n",
    "        self.ensemble_performance = deque(maxlen=100)\n",
    "        \n",
    "    def partial_fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train all models in the ensemble.\n",
    "        \n",
    "        Args:\n",
    "            X: Features\n",
    "            y: Labels\n",
    "        \"\"\"\n",
    "        # Train each model\n",
    "        for model in self.models:\n",
    "            try:\n",
    "                model.partial_fit(X, y)\n",
    "            except Exception as e:\n",
    "                print(f\"Error training model: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Update model weights based on performance\n",
    "        self._update_weights(X, y)\n",
    "    \n",
    "    def _update_weights(self, X, y):\n",
    "        \"\"\"\n",
    "        Update model weights based on recent performance.\n",
    "        \n",
    "        Args:\n",
    "            X: Features\n",
    "            y: Labels\n",
    "        \"\"\"\n",
    "        # Evaluate each model\n",
    "        for i, model in enumerate(self.models):\n",
    "            try:\n",
    "                if model.is_fitted:\n",
    "                    y_pred = model.predict(X)\n",
    "                    accuracy = accuracy_score(y, y_pred)\n",
    "                    self.model_performances[i].append(accuracy)\n",
    "                    \n",
    "                    # Update model's performance tracking\n",
    "                    model.update_performance(y, y_pred)\n",
    "                else:\n",
    "                    self.model_performances[i].append(0.5)  # Default performance\n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluating model {i}: {e}\")\n",
    "                self.model_performances[i].append(0.0)  # Penalize failed models\n",
    "        \n",
    "        # Calculate new weights based on recent performance\n",
    "        recent_performances = []\n",
    "        for i in range(self.n_models):\n",
    "            if len(self.model_performances[i]) > 0:\n",
    "                recent_perf = np.mean(list(self.model_performances[i])[-10:])  # Last 10 evaluations\n",
    "            else:\n",
    "                recent_perf = 0.5\n",
    "            recent_performances.append(recent_perf)\n",
    "        \n",
    "        # Convert to weights (softmax with temperature)\n",
    "        temperature = 2.0  # Controls weight distribution sharpness\n",
    "        exp_perfs = np.exp(np.array(recent_performances) / temperature)\n",
    "        self.model_weights = exp_perfs / np.sum(exp_perfs)\n",
    "        \n",
    "        # Ensure minimum weight to prevent models from being completely ignored\n",
    "        min_weight = 0.05\n",
    "        self.model_weights = np.maximum(self.model_weights, min_weight)\n",
    "        self.model_weights = self.model_weights / np.sum(self.model_weights)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make ensemble predictions.\n",
    "        \n",
    "        Args:\n",
    "            X: Features\n",
    "        \n",
    "        Returns:\n",
    "            Ensemble predictions\n",
    "        \"\"\"\n",
    "        if not any(model.is_fitted for model in self.models):\n",
    "            return np.zeros(len(X))\n",
    "        \n",
    "        # Get predictions from all models\n",
    "        predictions = []\n",
    "        for model in self.models:\n",
    "            try:\n",
    "                if model.is_fitted:\n",
    "                    pred = model.predict(X)\n",
    "                else:\n",
    "                    pred = np.zeros(len(X))\n",
    "                predictions.append(pred)\n",
    "            except Exception as e:\n",
    "                print(f\"Error getting predictions: {e}\")\n",
    "                predictions.append(np.zeros(len(X)))\n",
    "        \n",
    "        # Weighted ensemble prediction\n",
    "        predictions = np.array(predictions)\n",
    "        ensemble_pred = np.average(predictions, axis=0, weights=self.model_weights)\n",
    "        \n",
    "        return (ensemble_pred > 0.5).astype(int)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Make ensemble probability predictions.\n",
    "        \n",
    "        Args:\n",
    "            X: Features\n",
    "        \n",
    "        Returns:\n",
    "            Ensemble prediction probabilities\n",
    "        \"\"\"\n",
    "        if not any(model.is_fitted for model in self.models):\n",
    "            return np.full((len(X), 2), 0.5)\n",
    "        \n",
    "        # Get probabilities from all models\n",
    "        probabilities = []\n",
    "        for model in self.models:\n",
    "            try:\n",
    "                if model.is_fitted:\n",
    "                    prob = model.predict_proba(X)\n",
    "                else:\n",
    "                    prob = np.full((len(X), 2), 0.5)\n",
    "                probabilities.append(prob)\n",
    "            except Exception as e:\n",
    "                print(f\"Error getting probabilities: {e}\")\n",
    "                probabilities.append(np.full((len(X), 2), 0.5))\n",
    "        \n",
    "        # Weighted ensemble probabilities\n",
    "        probabilities = np.array(probabilities)\n",
    "        ensemble_prob = np.average(probabilities, axis=0, weights=self.model_weights)\n",
    "        \n",
    "        return ensemble_prob\n",
    "    \n",
    "    def get_ensemble_status(self):\n",
    "        \"\"\"\n",
    "        Get ensemble status and model information.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Ensemble status\n",
    "        \"\"\"\n",
    "        model_statuses = []\n",
    "        for i, model in enumerate(self.models):\n",
    "            status = model.get_status()\n",
    "            status['weight'] = self.model_weights[i]\n",
    "            status['recent_performance'] = (np.mean(list(self.model_performances[i])[-10:]) \n",
    "                                          if len(self.model_performances[i]) > 0 else 0.5)\n",
    "            model_statuses.append(status)\n",
    "        \n",
    "        return {\n",
    "            'n_models': self.n_models,\n",
    "            'model_weights': self.model_weights,\n",
    "            'model_statuses': model_statuses,\n",
    "            'ensemble_performance_length': len(self.ensemble_performance)\n",
    "        }\n",
    "\n",
    "# Test the online ensemble\n",
    "print(\"Testing Online Ensemble...\")\n",
    "\n",
    "# Create ensemble\n",
    "ensemble = OnlineEnsemble(n_models=3, learning_rate=0.01)\n",
    "\n",
    "# Simulate streaming learning\n",
    "batch_size = 100\n",
    "ensemble_performance = []\n",
    "weight_history = []\n",
    "\n",
    "for i in range(0, min(5000, len(X_with_drift)), batch_size):\n",
    "    end_idx = min(i + batch_size, len(X_with_drift))\n",
    "    \n",
    "    X_batch = X_with_drift[i:end_idx]\n",
    "    y_batch = y.iloc[i:end_idx]\n",
    "    \n",
    "    # Train ensemble\n",
    "    ensemble.partial_fit(X_batch, y_batch)\n",
    "    \n",
    "    # Evaluate ensemble\n",
    "    y_pred = ensemble.predict(X_batch)\n",
    "    accuracy = accuracy_score(y_batch, y_pred)\n",
    "    \n",
    "    # Track performance and weights\n",
    "    ensemble_performance.append(accuracy)\n",
    "    weight_history.append(ensemble.model_weights.copy())\n",
    "\n",
    "# Visualize ensemble performance and weights\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "# Performance plot\n",
    "ax1.plot(ensemble_performance, 'b-', linewidth=2, label='Ensemble Accuracy')\n",
    "ax1.axhline(y=np.mean(ensemble_performance), color='g', linestyle='--', label='Average')\n",
    "ax1.set_xlabel('Batch Number')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_title('Online Ensemble Performance')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Weight evolution plot\n",
    "weight_history = np.array(weight_history)\n",
    "for i in range(ensemble.n_models):\n",
    "    ax2.plot(weight_history[:, i], label=f'Model {i+1}', linewidth=2)\n",
    "\n",
    "ax2.set_xlabel('Batch Number')\n",
    "ax2.set_ylabel('Model Weight')\n",
    "ax2.set_title('Model Weight Evolution')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print ensemble status\n",
    "print(\"\\nEnsemble Status:\")\n",
    "status = ensemble.get_ensemble_status()\n",
    "print(f\"Model weights: {status['model_weights']}\")\n",
    "print(f\"\\nIndividual model performance:\")\n",
    "for i, model_status in enumerate(status['model_statuses']):\n",
    "    print(f\"  Model {i+1}: Weight={model_status['weight']:.3f}, \"\n",
    "          f\"Performance={model_status['recent_performance']:.3f}, \"\n",
    "          f\"Drift={model_status['drift_detected']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Complete Streaming System\n",
    "\n",
    "### Production-Ready Streaming Architecture\n",
    "\n",
    "A complete streaming fraud detection system needs:\n",
    "- **Threaded Processing**: Handle high-throughput streams\n",
    "- **Queue Management**: Buffer transactions and handle backpressure\n",
    "- **Real-time Monitoring**: Track performance and system health\n",
    "- **Error Handling**: Graceful degradation and recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamingFraudDetector:\n",
    "    \"\"\"\n",
    "    Complete streaming fraud detection system.\n",
    "    \n",
    "    Features:\n",
    "    - Real-time transaction processing\n",
    "    - Threaded architecture\n",
    "    - Performance monitoring\n",
    "    - Adaptive learning\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_type='ensemble', batch_size=50, update_frequency=10):\n",
    "        \"\"\"\n",
    "        Initialize streaming fraud detector.\n",
    "        \n",
    "        Args:\n",
    "            model_type: Type of model ('ensemble' or 'single')\n",
    "            batch_size: Batch size for model updates\n",
    "            update_frequency: How often to update model (in batches)\n",
    "        \"\"\"\n",
    "        self.model_type = model_type\n",
    "        self.batch_size = batch_size\n",
    "        self.update_frequency = update_frequency\n",
    "        \n",
    "        # Initialize model\n",
    "        if model_type == 'ensemble':\n",
    "            self.model = OnlineEnsemble(n_models=3)\n",
    "        else:\n",
    "            self.model = AdaptiveOnlineClassifier()\n",
    "        \n",
    "        # Streaming infrastructure\n",
    "        self.transaction_queue = queue.Queue(maxsize=10000)\n",
    "        self.processing_thread = None\n",
    "        self.is_running = False\n",
    "        \n",
    "        # Buffers for batch processing\n",
    "        self.feature_buffer = deque(maxlen=self.batch_size * 2)\n",
    "        self.label_buffer = deque(maxlen=self.batch_size * 2)\n",
    "        self.prediction_buffer = deque(maxlen=1000)\n",
    "        \n",
    "        # Performance monitoring\n",
    "        self.performance_metrics = {\n",
    "            'transactions_processed': 0,\n",
    "            'predictions_made': 0,\n",
    "            'model_updates': 0,\n",
    "            'processing_times': deque(maxlen=1000),\n",
    "            'accuracies': deque(maxlen=100),\n",
    "            'f1_scores': deque(maxlen=100),\n",
    "            'precisions': deque(maxlen=100),\n",
    "            'recalls': deque(maxlen=100)\n",
    "        }\n",
    "        \n",
    "        # Thread synchronization\n",
    "        self.lock = threading.Lock()\n",
    "        \n",
    "    def start_streaming(self):\n",
    "        \"\"\"\n",
    "        Start the streaming processing thread.\n",
    "        \"\"\"\n",
    "        if self.is_running:\n",
    "            print(\"Streaming already running\")\n",
    "            return\n",
    "        \n",
    "        self.is_running = True\n",
    "        self.processing_thread = threading.Thread(target=self._process_stream)\n",
    "        self.processing_thread.daemon = True\n",
    "        self.processing_thread.start()\n",
    "        \n",
    "        print(\"Streaming fraud detection started\")\n",
    "    \n",
    "    def stop_streaming(self):\n",
    "        \"\"\"\n",
    "        Stop the streaming processing thread.\n",
    "        \"\"\"\n",
    "        if not self.is_running:\n",
    "            print(\"Streaming not running\")\n",
    "            return\n",
    "        \n",
    "        self.is_running = False\n",
    "        \n",
    "        # Wait for thread to finish\n",
    "        if self.processing_thread:\n",
    "            self.processing_thread.join(timeout=5)\n",
    "        \n",
    "        print(\"Streaming fraud detection stopped\")\n",
    "    \n",
    "    def submit_transaction(self, features, label=None, transaction_id=None):\n",
    "        \"\"\"\n",
    "        Submit a transaction for processing.\n",
    "        \n",
    "        Args:\n",
    "            features: Transaction features\n",
    "            label: True label (if available)\n",
    "            transaction_id: Optional transaction ID\n",
    "        \n",
    "        Returns:\n",
    "            bool: True if successfully queued, False if queue full\n",
    "        \"\"\"\n",
    "        try:\n",
    "            transaction = {\n",
    "                'features': features,\n",
    "                'label': label,\n",
    "                'transaction_id': transaction_id,\n",
    "                'timestamp': time.time()\n",
    "            }\n",
    "            \n",
    "            self.transaction_queue.put(transaction, block=False)\n",
    "            return True\n",
    "            \n",
    "        except queue.Full:\n",
    "            print(\"Warning: Transaction queue full, dropping transaction\")\n",
    "            return False\n",
    "    \n",
    "    def _process_stream(self):\n",
    "        \"\"\"\n",
    "        Main processing loop (runs in separate thread).\n",
    "        \"\"\"\n",
    "        print(\"Processing thread started\")\n",
    "        \n",
    "        while self.is_running:\n",
    "            try:\n",
    "                # Get transaction from queue\n",
    "                transaction = self.transaction_queue.get(timeout=1)\n",
    "                \n",
    "                # Process transaction\n",
    "                self._process_single_transaction(transaction)\n",
    "                \n",
    "                # Update model if enough data accumulated\n",
    "                if len(self.feature_buffer) >= self.batch_size:\n",
    "                    self._update_model()\n",
    "                \n",
    "            except queue.Empty:\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing transaction: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(\"Processing thread stopped\")\n",
    "    \n",
    "    def _process_single_transaction(self, transaction):\n",
    "        \"\"\"\n",
    "        Process a single transaction.\n",
    "        \n",
    "        Args:\n",
    "            transaction: Transaction dictionary\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        features = transaction['features']\n",
    "        label = transaction['label']\n",
    "        \n",
    "        # Make prediction\n",
    "        if hasattr(self.model, 'predict_proba'):\n",
    "            prob = self.model.predict_proba([features])[0]\n",
    "            prediction = 1 if prob[1] > 0.5 else 0\n",
    "            confidence = prob[1]\n",
    "        else:\n",
    "            prediction = self.model.predict([features])[0]\n",
    "            confidence = 0.5\n",
    "        \n",
    "        # Store prediction\n",
    "        self.prediction_buffer.append({\n",
    "            'transaction_id': transaction['transaction_id'],\n",
    "            'prediction': prediction,\n",
    "            'confidence': confidence,\n",
    "            'timestamp': transaction['timestamp']\n",
    "        })\n",
    "        \n",
    "        # Buffer for training (if label available)\n",
    "        if label is not None:\n",
    "            self.feature_buffer.append(features)\n",
    "            self.label_buffer.append(label)\n",
    "        \n",
    "        # Update performance metrics\n",
    "        with self.lock:\n",
    "            self.performance_metrics['transactions_processed'] += 1\n",
    "            self.performance_metrics['predictions_made'] += 1\n",
    "            \n",
    "            processing_time = time.time() - start_time\n",
    "            self.performance_metrics['processing_times'].append(processing_time)\n",
    "    \n",
    "    def _update_model(self):\n",
    "        \"\"\"\n",
    "        Update the model with buffered data.\n",
    "        \"\"\"\n",
    "        if len(self.feature_buffer) == 0 or len(self.label_buffer) == 0:\n",
    "            return\n",
    "        \n",
    "        # Convert buffers to arrays\n",
    "        X_batch = np.array(list(self.feature_buffer))\n",
    "        y_batch = np.array(list(self.label_buffer))\n",
    "        \n",
    "        # Update model\n",
    "        self.model.partial_fit(X_batch, y_batch)\n",
    "        \n",
    "        # Update performance metrics\n",
    "        self._update_performance_metrics(X_batch, y_batch)\n",
    "        \n",
    "        # Clear buffers\n",
    "        self.feature_buffer.clear()\n",
    "        self.label_buffer.clear()\n",
    "        \n",
    "        with self.lock:\n",
    "            self.performance_metrics['model_updates'] += 1\n",
    "    \n",
    "    def _update_performance_metrics(self, X_batch, y_batch):\n",
    "        \"\"\"\n",
    "        Update performance metrics.\n",
    "        \n",
    "        Args:\n",
    "            X_batch: Features batch\n",
    "            y_batch: Labels batch\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Make predictions\n",
    "            y_pred = self.model.predict(X_batch)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            accuracy = accuracy_score(y_batch, y_pred)\n",
    "            precision = precision_score(y_batch, y_pred, zero_division=0)\n",
    "            recall = recall_score(y_batch, y_pred, zero_division=0)\n",
    "            f1 = f1_score(y_batch, y_pred, zero_division=0)\n",
    "            \n",
    "            # Update metrics\n",
    "            with self.lock:\n",
    "                self.performance_metrics['accuracies'].append(accuracy)\n",
    "                self.performance_metrics['precisions'].append(precision)\n",
    "                self.performance_metrics['recalls'].append(recall)\n",
    "                self.performance_metrics['f1_scores'].append(f1)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error updating performance metrics: {e}\")\n",
    "    \n",
    "    def get_performance_report(self):\n",
    "        \"\"\"\n",
    "        Get comprehensive performance report.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Performance metrics\n",
    "        \"\"\"\n",
    "        with self.lock:\n",
    "            metrics = self.performance_metrics.copy()\n",
    "        \n",
    "        # Calculate averages\n",
    "        report = {\n",
    "            'transactions_processed': metrics['transactions_processed'],\n",
    "            'predictions_made': metrics['predictions_made'],\n",
    "            'model_updates': metrics['model_updates'],\n",
    "            'avg_processing_time': np.mean(metrics['processing_times']) if metrics['processing_times'] else 0,\n",
    "            'avg_accuracy': np.mean(metrics['accuracies']) if metrics['accuracies'] else 0,\n",
    "            'avg_precision': np.mean(metrics['precisions']) if metrics['precisions'] else 0,\n",
    "            'avg_recall': np.mean(metrics['recalls']) if metrics['recalls'] else 0,\n",
    "            'avg_f1_score': np.mean(metrics['f1_scores']) if metrics['f1_scores'] else 0,\n",
    "            'processing_rate': metrics['transactions_processed'] / (time.time() - (metrics['processing_times'][0] if metrics['processing_times'] else time.time())) if metrics['processing_times'] else 0\n",
    "        }\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def get_recent_predictions(self, n=10):\n",
    "        \"\"\"\n",
    "        Get recent predictions.\n",
    "        \n",
    "        Args:\n",
    "            n: Number of recent predictions to return\n",
    "        \n",
    "        Returns:\n",
    "            list: Recent predictions\n",
    "        \"\"\"\n",
    "        return list(self.prediction_buffer)[-n:]\n",
    "\n",
    "# Test the complete streaming system\n",
    "print(\"Testing Complete Streaming System...\")\n",
    "\n",
    "# Initialize streaming detector\n",
    "detector = StreamingFraudDetector(model_type='ensemble', batch_size=50)\n",
    "\n",
    "# Start streaming\n",
    "detector.start_streaming()\n",
    "\n",
    "# Simulate streaming transactions\n",
    "print(\"\\nSimulating streaming transactions...\")\n",
    "n_transactions = 2000\n",
    "transactions_per_second = 100\n",
    "\n",
    "for i in range(n_transactions):\n",
    "    # Get transaction\n",
    "    features = X_with_drift[i]\n",
    "    label = y.iloc[i]\n",
    "    \n",
    "    # Submit transaction\n",
    "    success = detector.submit_transaction(features, label, transaction_id=f\"txn_{i}\")\n",
    "    \n",
    "    if not success:\n",
    "        print(f\"Failed to submit transaction {i}\")\n",
    "    \n",
    "    # Control rate\n",
    "    if i % transactions_per_second == 0:\n",
    "        time.sleep(0.1)  # Brief pause\n",
    "        \n",
    "        # Print progress\n",
    "        if i % 500 == 0:\n",
    "            report = detector.get_performance_report()\n",
    "            print(f\"Processed {report['transactions_processed']} transactions, \"\n",
    "                  f\"Accuracy: {report['avg_accuracy']:.3f}, \"\n",
    "                  f\"F1: {report['avg_f1_score']:.3f}\")\n",
    "\n",
    "# Wait for processing to complete\n",
    "time.sleep(2)\n",
    "\n",
    "# Get final report\n",
    "final_report = detector.get_performance_report()\n",
    "print(\"\\nFinal Performance Report:\")\n",
    "print(\"=\"*50)\n",
    "for key, value in final_report.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "# Stop streaming\n",
    "detector.stop_streaming()\n",
    "\n",
    "print(\"\\nStreaming simulation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Performance Monitoring and Visualization\n",
    "\n",
    "### Real-time Performance Tracking\n",
    "\n",
    "Production streaming systems need comprehensive monitoring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_streaming_fraud_detection():\n",
    "    \"\"\"\n",
    "    Comprehensive streaming fraud detection simulation with visualization.\n",
    "    \"\"\"\n",
    "    print(\"Starting comprehensive streaming simulation...\")\n",
    "    \n",
    "    # Initialize detector\n",
    "    detector = StreamingFraudDetector(model_type='ensemble', batch_size=50)\n",
    "    detector.start_streaming()\n",
    "    \n",
    "    # Tracking variables\n",
    "    performance_timeline = []\n",
    "    timestamp_start = time.time()\n",
    "    \n",
    "    # Simulate realistic streaming\n",
    "    n_transactions = 3000\n",
    "    report_interval = 200\n",
    "    \n",
    "    for i in range(n_transactions):\n",
    "        # Get transaction\n",
    "        features = X_with_drift[i]\n",
    "        \n",
    "        # Simulate delayed labeling (common in real systems)\n",
    "        # Only 30% of transactions have immediate labels\n",
    "        label = y.iloc[i] if np.random.random() < 0.3 else None\n",
    "        \n",
    "        # Submit transaction\n",
    "        detector.submit_transaction(features, label, transaction_id=f\"txn_{i}\")\n",
    "        \n",
    "        # Collect performance data\n",
    "        if i % report_interval == 0 and i > 0:\n",
    "            report = detector.get_performance_report()\n",
    "            report['timestamp'] = time.time() - timestamp_start\n",
    "            report['transaction_index'] = i\n",
    "            performance_timeline.append(report)\n",
    "            \n",
    "            print(f\"Batch {i//report_interval}: \"\n",
    "                  f\"Processed {report['transactions_processed']}, \"\n",
    "                  f\"Accuracy: {report['avg_accuracy']:.3f}, \"\n",
    "                  f\"Rate: {report['processing_rate']:.1f} tx/s\")\n",
    "        \n",
    "        # Small delay to simulate realistic timing\n",
    "        if i % 50 == 0:\n",
    "            time.sleep(0.01)\n",
    "    \n",
    "    # Wait for processing to complete\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # Final report\n",
    "    final_report = detector.get_performance_report()\n",
    "    final_report['timestamp'] = time.time() - timestamp_start\n",
    "    final_report['transaction_index'] = n_transactions\n",
    "    performance_timeline.append(final_report)\n",
    "    \n",
    "    # Stop streaming\n",
    "    detector.stop_streaming()\n",
    "    \n",
    "    return performance_timeline, detector\n",
    "\n",
    "# Run simulation\n",
    "performance_timeline, detector = simulate_streaming_fraud_detection()\n",
    "\n",
    "# Visualize performance timeline\n",
    "def plot_streaming_performance(timeline):\n",
    "    \"\"\"\n",
    "    Plot comprehensive streaming performance metrics.\n",
    "    \"\"\"\n",
    "    if not timeline:\n",
    "        print(\"No performance data to plot\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Extract data\n",
    "    timestamps = [t['timestamp'] for t in timeline]\n",
    "    accuracies = [t['avg_accuracy'] for t in timeline]\n",
    "    f1_scores = [t['avg_f1_score'] for t in timeline]\n",
    "    processing_times = [t['avg_processing_time'] * 1000 for t in timeline]  # Convert to ms\n",
    "    processing_rates = [t['processing_rate'] for t in timeline]\n",
    "    \n",
    "    # Accuracy over time\n",
    "    axes[0, 0].plot(timestamps, accuracies, 'b-', linewidth=2, label='Accuracy')\n",
    "    axes[0, 0].plot(timestamps, f1_scores, 'r-', linewidth=2, label='F1-Score')\n",
    "    axes[0, 0].set_xlabel('Time (seconds)')\n",
    "    axes[0, 0].set_ylabel('Score')\n",
    "    axes[0, 0].set_title('Model Performance Over Time')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Processing time\n",
    "    axes[0, 1].plot(timestamps, processing_times, 'g-', linewidth=2)\n",
    "    axes[0, 1].set_xlabel('Time (seconds)')\n",
    "    axes[0, 1].set_ylabel('Processing Time (ms)')\n",
    "    axes[0, 1].set_title('Processing Latency')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Processing rate\n",
    "    axes[1, 0].plot(timestamps, processing_rates, 'orange', linewidth=2)\n",
    "    axes[1, 0].set_xlabel('Time (seconds)')\n",
    "    axes[1, 0].set_ylabel('Transactions/second')\n",
    "    axes[1, 0].set_title('Processing Rate')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Cumulative transactions\n",
    "    transactions = [t['transactions_processed'] for t in timeline]\n",
    "    axes[1, 1].plot(timestamps, transactions, 'purple', linewidth=2)\n",
    "    axes[1, 1].set_xlabel('Time (seconds)')\n",
    "    axes[1, 1].set_ylabel('Transactions Processed')\n",
    "    axes[1, 1].set_title('Cumulative Transactions')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot performance\n",
    "plot_streaming_performance(performance_timeline)\n",
    "\n",
    "# Show recent predictions\n",
    "recent_predictions = detector.get_recent_predictions(10)\n",
    "print(\"\\nRecent Predictions:\")\n",
    "print(\"=\"*70)\n",
    "for pred in recent_predictions:\n",
    "    print(f\"ID: {pred['transaction_id']}, \"\n",
    "          f\"Prediction: {pred['prediction']}, \"\n",
    "          f\"Confidence: {pred['confidence']:.3f}\")\n",
    "\n",
    "# Final summary\n",
    "if performance_timeline:\n",
    "    final_perf = performance_timeline[-1]\n",
    "    print(\"\\nFinal Performance Summary:\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Total transactions processed: {final_perf['transactions_processed']:,}\")\n",
    "    print(f\"Average accuracy: {final_perf['avg_accuracy']:.4f}\")\n",
    "    print(f\"Average F1-score: {final_perf['avg_f1_score']:.4f}\")\n",
    "    print(f\"Average processing time: {final_perf['avg_processing_time']*1000:.2f} ms\")\n",
    "    print(f\"Processing rate: {final_perf['processing_rate']:.1f} transactions/second\")\n",
    "    print(f\"Model updates: {final_perf['model_updates']}\")\n",
    "    print(f\"Total simulation time: {final_perf['timestamp']:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises\n",
    "\n",
    "Now it's your turn! Try these exercises to deepen your understanding:\n",
    "\n",
    "### Exercise 1: Drift Detection Sensitivity\n",
    "Experiment with different drift detection parameters:\n",
    "- Modify the `drift_threshold` in `AdaptiveOnlineClassifier`\n",
    "- Try different window sizes for performance monitoring\n",
    "- Implement alternative drift detection methods (e.g., statistical tests)\n",
    "\n",
    "How do these changes affect adaptation speed and stability?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Create classifiers with different drift thresholds\n",
    "# Compare their performance on the same data stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Ensemble Composition\n",
    "Experiment with different ensemble compositions:\n",
    "- Try different numbers of base models (2, 5, 10)\n",
    "- Mix different model types (SGD, Passive-Aggressive, etc.)\n",
    "- Implement different weighting strategies\n",
    "\n",
    "Which ensemble composition works best for your data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Modify the OnlineEnsemble class to accept different model configurations\n",
    "# Compare performance with different ensemble sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Streaming Performance Optimization\n",
    "Optimize the streaming system for higher throughput:\n",
    "- Implement batch processing for predictions\n",
    "- Add connection pooling for database updates\n",
    "- Implement backpressure handling\n",
    "\n",
    "How much can you improve the processing rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Modify the StreamingFraudDetector to process transactions in batches\n",
    "# Measure the impact on processing rate and latency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### 1. Online Learning Fundamentals\n",
    "- **Incremental Updates**: Learn from each new transaction without retraining\n",
    "- **Memory Efficiency**: No need to store entire training dataset\n",
    "- **Real-time Adaptation**: Adjust to changing patterns immediately\n",
    "- **Forgetting Factor**: Give higher weight to recent examples\n",
    "\n",
    "### 2. Concept Drift Handling\n",
    "- **Detection**: Monitor performance degradation to detect drift\n",
    "- **Adaptation**: Increase learning rate and focus on recent data\n",
    "- **Types**: Understand sudden, gradual, and recurring drift patterns\n",
    "- **Recovery**: Implement strategies to quickly adapt to new patterns\n",
    "\n",
    "### 3. Ensemble Methods\n",
    "- **Diversity**: Use different model types and configurations\n",
    "- **Weighting**: Dynamically adjust model weights based on performance\n",
    "- **Robustness**: Ensemble predictions are more stable than individual models\n",
    "- **Error Handling**: Graceful degradation when individual models fail\n",
    "\n",
    "### 4. Streaming Architecture\n",
    "- **Threading**: Separate processing from transaction ingestion\n",
    "- **Queues**: Buffer transactions to handle variable rates\n",
    "- **Monitoring**: Track performance and system health in real-time\n",
    "- **Scalability**: Design for high-throughput processing\n",
    "\n",
    "### 5. Production Considerations\n",
    "- **Error Recovery**: Handle failures gracefully without stopping\n",
    "- **Monitoring**: Comprehensive metrics for system health\n",
    "- **Latency**: Optimize for low-latency predictions\n",
    "- **Backpressure**: Handle queue overflow and high load\n",
    "\n",
    "### 6. Performance Optimization\n",
    "- **Batch Processing**: Process multiple transactions together\n",
    "- **Memory Management**: Use circular buffers and efficient data structures\n",
    "- **Threading**: Proper synchronization and resource management\n",
    "- **Caching**: Cache frequently used computations\n",
    "\n",
    "### 7. Real-world Applications\n",
    "- **Financial Services**: Credit card fraud, loan defaults, market anomalies\n",
    "- **E-commerce**: Fake reviews, account takeovers, payment fraud\n",
    "- **Cybersecurity**: Intrusion detection, malware analysis, network monitoring\n",
    "- **IoT**: Sensor anomalies, device failures, security breaches\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next tutorial, we'll explore:\n",
    "- Hybrid ensemble systems combining multiple approaches\n",
    "- Advanced model selection and meta-learning\n",
    "- Distributed streaming architectures\n",
    "- Production deployment strategies\n",
    "\n",
    "Remember: Online learning is essential for production fraud detection systems. The key is balancing adaptation speed with stability, and ensuring your system can handle real-world complexities like delayed labels, variable data rates, and concept drift!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}