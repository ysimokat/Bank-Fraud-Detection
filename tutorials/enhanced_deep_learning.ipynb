{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced Deep Learning for Fraud Detection\n",
    "\n",
    "## Tutorial 5: Advanced Neural Networks for Imbalanced Classification\n",
    "\n",
    "In this tutorial, you'll master advanced deep learning techniques specifically designed for fraud detection:\n",
    "- **Focal Loss**: Focus learning on hard examples\n",
    "- **Weighted Binary Cross-Entropy**: Traditional imbalance handling\n",
    "- **Autoencoder Anomaly Detection**: Unsupervised fraud detection\n",
    "- **Modern Architecture Design**: BatchNorm, Dropout, and more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you'll understand:\n",
    "\n",
    "1. **Deep Learning for Imbalanced Data**: Why standard approaches fail\n",
    "2. **Focal Loss**: Mathematical foundation and implementation\n",
    "3. **Autoencoder Anomaly Detection**: Unsupervised fraud detection\n",
    "4. **Modern Neural Architecture**: Best practices for stable training\n",
    "5. **Training Strategies**: Learning rate scheduling, early stopping\n",
    "6. **Evaluation Techniques**: Comprehensive model assessment\n",
    "7. **Production Considerations**: GPU optimization and deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report, roc_curve, auc,\n",
    "    precision_recall_curve, average_precision_score\n",
    ")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding the Challenge - Class Imbalance in Fraud Detection\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Fraud detection presents unique challenges for deep learning:\n",
    "- **Extreme imbalance**: Only 0.1-2% of transactions are fraudulent\n",
    "- **Cost asymmetry**: Missing fraud is much more expensive than false alarms\n",
    "- **Evolving patterns**: Fraudsters constantly adapt their methods\n",
    "\n",
    "Standard loss functions like Cross-Entropy fail because they:\n",
    "- Are dominated by the majority class\n",
    "- Don't focus on hard examples\n",
    "- Don't account for business costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and examine the data\n",
    "df = pd.read_csv('creditcard.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Fraud rate: {df['Class'].mean()*100:.3f}%\")\n",
    "print(f\"Number of fraud cases: {df['Class'].sum():,}\")\n",
    "print(f\"Number of normal cases: {len(df) - df['Class'].sum():,}\")\n",
    "\n",
    "# Visualize class distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Class distribution\n",
    "class_counts = df['Class'].value_counts()\n",
    "ax1.bar(['Normal', 'Fraud'], class_counts.values, color=['lightblue', 'salmon'])\n",
    "ax1.set_ylabel('Number of Transactions')\n",
    "ax1.set_title('Class Distribution (Linear Scale)')\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "# Fraud percentage\n",
    "fraud_percentage = df['Class'].mean() * 100\n",
    "ax2.pie([100 - fraud_percentage, fraud_percentage], \n",
    "        labels=[f'Normal ({100-fraud_percentage:.1f}%)', f'Fraud ({fraud_percentage:.1f}%)'],\n",
    "        colors=['lightblue', 'salmon'],\n",
    "        autopct='%1.1f%%')\n",
    "ax2.set_title('Class Distribution (Percentage)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Challenge: How can we train a model to detect the 0.17% of transactions that are fraudulent?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Focal Loss - Focusing on Hard Examples\n",
    "\n",
    "### Theory Behind Focal Loss\n",
    "\n",
    "Focal Loss was introduced by Facebook AI Research to address class imbalance in object detection. The key insight:\n",
    "- **Focus on hard examples**: Well-classified examples contribute less to loss\n",
    "- **Down-weight easy examples**: Prevents overwhelming by majority class\n",
    "\n",
    "The formula is:\n",
    "$$FL(p_t) = -\\alpha(1-p_t)^\\gamma \\log(p_t)$$\n",
    "\n",
    "Where:\n",
    "- $p_t$ = model's predicted probability for the true class\n",
    "- $\\alpha$ = weighting factor for rare class (0.25-1.0)\n",
    "- $\\gamma$ = focusing parameter (1-5)\n",
    "\n",
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of Focal Loss for addressing class imbalance.\n",
    "    \n",
    "    Key benefits:\n",
    "    - Reduces loss contribution from easy examples\n",
    "    - Focuses learning on hard examples\n",
    "    - Naturally handles class imbalance\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=0.75, gamma=2.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        # Apply sigmoid to get probabilities\n",
    "        probs = torch.sigmoid(inputs)\n",
    "        \n",
    "        # Calculate binary cross entropy\n",
    "        bce_loss = nn.functional.binary_cross_entropy_with_logits(\n",
    "            inputs, targets, reduction='none'\n",
    "        )\n",
    "        \n",
    "        # Calculate p_t\n",
    "        p_t = probs * targets + (1 - probs) * (1 - targets)\n",
    "        \n",
    "        # Calculate alpha_t\n",
    "        alpha_t = self.alpha * targets + (1 - self.alpha) * (1 - targets)\n",
    "        \n",
    "        # Calculate focal loss\n",
    "        focal_loss = alpha_t * (1 - p_t) ** self.gamma * bce_loss\n",
    "        \n",
    "        return focal_loss.mean()\n",
    "\n",
    "# Let's visualize how focal loss works\n",
    "def visualize_focal_loss():\n",
    "    # Create probability range\n",
    "    p = np.linspace(0.01, 0.99, 100)\n",
    "    \n",
    "    # Standard cross-entropy\n",
    "    ce_loss = -np.log(p)\n",
    "    \n",
    "    # Focal loss with different gamma values\n",
    "    focal_gamma_1 = -(1 - p) ** 1 * np.log(p)\n",
    "    focal_gamma_2 = -(1 - p) ** 2 * np.log(p)\n",
    "    focal_gamma_5 = -(1 - p) ** 5 * np.log(p)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(p, ce_loss, label='Cross-Entropy', linewidth=2)\n",
    "    plt.plot(p, focal_gamma_1, label='Focal Loss (γ=1)', linewidth=2)\n",
    "    plt.plot(p, focal_gamma_2, label='Focal Loss (γ=2)', linewidth=2)\n",
    "    plt.plot(p, focal_gamma_5, label='Focal Loss (γ=5)', linewidth=2)\n",
    "    \n",
    "    plt.xlabel('Model Confidence (p)')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Focal Loss vs Cross-Entropy: Focus on Hard Examples')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Key Insight: Focal Loss reduces loss for high-confidence predictions\")\n",
    "    print(\"This prevents easy examples from dominating the training process\")\n",
    "\n",
    "visualize_focal_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Modern Neural Network Architecture\n",
    "\n",
    "### Best Practices for Stable Training\n",
    "\n",
    "Modern deep learning requires careful architecture design:\n",
    "- **Batch Normalization**: Stabilizes training and allows higher learning rates\n",
    "- **Dropout**: Prevents overfitting, especially important for fraud detection\n",
    "- **Proper Weight Initialization**: Prevents vanishing/exploding gradients\n",
    "- **ReLU Activation**: Helps with gradient flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FraudDetectionNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Modern neural network architecture for fraud detection.\n",
    "    \n",
    "    Features:\n",
    "    - Batch normalization for stable training\n",
    "    - Dropout for regularization\n",
    "    - Progressive dimension reduction\n",
    "    - Skip connections for better gradient flow\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_dims=[128, 64, 32], dropout_rate=0.3):\n",
    "        super(FraudDetectionNN, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_size\n",
    "        \n",
    "        # Build hidden layers\n",
    "        for i, hidden_dim in enumerate(hidden_dims):\n",
    "            # Linear layer\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            \n",
    "            # Batch normalization\n",
    "            layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            \n",
    "            # Activation\n",
    "            layers.append(nn.ReLU())\n",
    "            \n",
    "            # Dropout (not on last layer)\n",
    "            if i < len(hidden_dims) - 1:\n",
    "                layers.append(nn.Dropout(dropout_rate))\n",
    "            \n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights using Xavier initialization\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x).squeeze()\n",
    "\n",
    "# Let's also create a weighted BCE loss for comparison\n",
    "class WeightedBCELoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Weighted Binary Cross-Entropy Loss for handling class imbalance.\n",
    "    Traditional approach using position weights.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pos_weight):\n",
    "        super(WeightedBCELoss, self).__init__()\n",
    "        self.pos_weight = pos_weight\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        return nn.functional.binary_cross_entropy_with_logits(\n",
    "            inputs, targets, pos_weight=self.pos_weight\n",
    "        )\n",
    "\n",
    "print(\"Modern neural network architecture implemented!\")\n",
    "print(\"Key features:\")\n",
    "print(\"- Batch normalization for stable training\")\n",
    "print(\"- Dropout for regularization\")\n",
    "print(\"- Xavier weight initialization\")\n",
    "print(\"- Progressive dimension reduction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Autoencoder for Anomaly Detection\n",
    "\n",
    "### Unsupervised Fraud Detection\n",
    "\n",
    "Autoencoders offer a different approach to fraud detection:\n",
    "- **Train on normal transactions only**: Learn patterns of legitimate behavior\n",
    "- **Reconstruction error**: Fraud transactions should have high reconstruction error\n",
    "- **No labeled fraud data needed**: Can work with unlabeled data\n",
    "\n",
    "### Architecture Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FraudAutoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoencoder for fraud detection using anomaly detection.\n",
    "    \n",
    "    Key principle:\n",
    "    - Train only on normal transactions\n",
    "    - Fraud transactions should have high reconstruction error\n",
    "    - Use reconstruction error as anomaly score\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, encoding_dims=[64, 32, 16, 8]):\n",
    "        super(FraudAutoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_size\n",
    "        \n",
    "        for dim in encoding_dims:\n",
    "            encoder_layers.extend([\n",
    "                nn.Linear(prev_dim, dim),\n",
    "                nn.BatchNorm1d(dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.1)\n",
    "            ])\n",
    "            prev_dim = dim\n",
    "        \n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        \n",
    "        # Decoder (reverse of encoder)\n",
    "        decoder_layers = []\n",
    "        decoding_dims = list(reversed(encoding_dims[:-1])) + [input_size]\n",
    "        \n",
    "        for dim in decoding_dims:\n",
    "            decoder_layers.extend([\n",
    "                nn.Linear(prev_dim, dim),\n",
    "                nn.BatchNorm1d(dim),\n",
    "                nn.ReLU() if dim != input_size else nn.Identity(),\n",
    "                nn.Dropout(0.1) if dim != input_size else nn.Identity()\n",
    "            ])\n",
    "            prev_dim = dim\n",
    "        \n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "    def get_reconstruction_error(self, x):\n",
    "        \"\"\"Calculate reconstruction error for anomaly detection\"\"\"\n",
    "        with torch.no_grad():\n",
    "            reconstructed = self.forward(x)\n",
    "            error = torch.mean((x - reconstructed) ** 2, dim=1)\n",
    "        return error\n",
    "\n",
    "# Visualize autoencoder architecture\n",
    "def visualize_autoencoder_architecture():\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Define architecture\n",
    "    layers = [30, 64, 32, 16, 8, 16, 32, 64, 30]\n",
    "    layer_names = ['Input', 'Hidden 1', 'Hidden 2', 'Hidden 3', 'Bottleneck', \n",
    "                   'Hidden 4', 'Hidden 5', 'Hidden 6', 'Output']\n",
    "    \n",
    "    # Plot architecture\n",
    "    x_pos = np.arange(len(layers))\n",
    "    bars = ax.bar(x_pos, layers, color=['lightblue' if i < 4 else 'red' if i == 4 else 'lightgreen' for i in range(len(layers))])\n",
    "    \n",
    "    ax.set_xlabel('Layer')\n",
    "    ax.set_ylabel('Number of Neurons')\n",
    "    ax.set_title('Autoencoder Architecture for Fraud Detection')\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(layer_names, rotation=45)\n",
    "    \n",
    "    # Add annotations\n",
    "    ax.annotate('Encoder\\n(Compression)', xy=(1.5, 40), xytext=(1.5, 60),\n",
    "                arrowprops=dict(arrowstyle='->', color='blue'), ha='center')\n",
    "    ax.annotate('Bottleneck\\n(Compressed Rep.)', xy=(4, 20), xytext=(4, 40),\n",
    "                arrowprops=dict(arrowstyle='->', color='red'), ha='center')\n",
    "    ax.annotate('Decoder\\n(Reconstruction)', xy=(6.5, 40), xytext=(6.5, 60),\n",
    "                arrowprops=dict(arrowstyle='->', color='green'), ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Autoencoder Principle:\")\n",
    "    print(\"1. Train only on normal transactions\")\n",
    "    print(\"2. Normal transactions should reconstruct well (low error)\")\n",
    "    print(\"3. Fraud transactions should reconstruct poorly (high error)\")\n",
    "    print(\"4. Use reconstruction error as anomaly score\")\n",
    "\n",
    "visualize_autoencoder_architecture()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Data Preparation and Training Pipeline\n",
    "\n",
    "### Preparing Data for Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for deep learning\n",
    "def prepare_data(df, test_size=0.2, batch_size=512):\n",
    "    \"\"\"\n",
    "    Prepare data for deep learning training.\n",
    "    \n",
    "    Steps:\n",
    "    1. Feature scaling\n",
    "    2. Train-test split\n",
    "    3. Convert to PyTorch tensors\n",
    "    4. Create data loaders\n",
    "    \"\"\"\n",
    "    # Separate features and labels\n",
    "    X = df.drop('Class', axis=1).values\n",
    "    y = df['Class'].values\n",
    "    \n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, stratify=y, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Feature scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "    X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "    y_train_tensor = torch.FloatTensor(y_train).to(device)\n",
    "    y_test_tensor = torch.FloatTensor(y_test).to(device)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Calculate class weights for weighted loss\n",
    "    fraud_count = np.sum(y_train)\n",
    "    normal_count = len(y_train) - fraud_count\n",
    "    pos_weight = torch.FloatTensor([normal_count / fraud_count]).to(device)\n",
    "    \n",
    "    return {\n",
    "        'train_loader': train_loader,\n",
    "        'test_loader': test_loader,\n",
    "        'X_train': X_train_tensor,\n",
    "        'X_test': X_test_tensor,\n",
    "        'y_train': y_train_tensor,\n",
    "        'y_test': y_test_tensor,\n",
    "        'pos_weight': pos_weight,\n",
    "        'scaler': scaler,\n",
    "        'input_size': X_train_scaled.shape[1]\n",
    "    }\n",
    "\n",
    "# Prepare the data\n",
    "data = prepare_data(df)\n",
    "\n",
    "print(f\"Data preparation complete:\")\n",
    "print(f\"- Input features: {data['input_size']}\")\n",
    "print(f\"- Training samples: {len(data['train_loader'].dataset):,}\")\n",
    "print(f\"- Test samples: {len(data['test_loader'].dataset):,}\")\n",
    "print(f\"- Positive class weight: {data['pos_weight'].item():.1f}\")\n",
    "print(f\"- Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Training with Focal Loss\n",
    "\n",
    "### Advanced Training Techniques\n",
    "\n",
    "Modern deep learning training requires:\n",
    "- **Learning Rate Scheduling**: Adaptive learning rates\n",
    "- **Early Stopping**: Prevent overfitting\n",
    "- **Gradient Clipping**: Prevent exploding gradients\n",
    "- **Mixed Precision**: Faster training on modern GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_focal_loss(data, epochs=50, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Train fraud detection model using Focal Loss.\n",
    "    \n",
    "    Features:\n",
    "    - Focal Loss for handling class imbalance\n",
    "    - Learning rate scheduling\n",
    "    - Early stopping\n",
    "    - Comprehensive logging\n",
    "    \"\"\"\n",
    "    # Initialize model\n",
    "    model = FraudDetectionNN(data['input_size']).to(device)\n",
    "    \n",
    "    # Initialize loss function and optimizer\n",
    "    criterion = FocalLoss(alpha=0.75, gamma=2.0)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.5)\n",
    "    \n",
    "    # Training history\n",
    "    history = {'train_loss': [], 'val_loss': [], 'learning_rate': []}\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(\"Training with Focal Loss...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch_X, batch_y in data['train_loader']:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in data['test_loader']:\n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        # Calculate average losses\n",
    "        avg_train_loss = train_loss / len(data['train_loader'])\n",
    "        avg_val_loss = val_loss / len(data['test_loader'])\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        \n",
    "        # Save history\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['learning_rate'].append(current_lr)\n",
    "        \n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            # Save best model\n",
    "            torch.save(model.state_dict(), 'best_focal_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Print progress\n",
    "        if epoch % 5 == 0 or epoch == epochs - 1:\n",
    "            print(f\"Epoch {epoch+1:2d}: Train Loss={avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss={avg_val_loss:.4f}, LR={current_lr:.2e}\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load('best_focal_model.pth'))\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Train the model\n",
    "focal_model, focal_history = train_model_with_focal_loss(data)\n",
    "\n",
    "# Visualize training history\n",
    "def plot_training_history(history, title=\"Training History\"):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Loss curves\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    ax1.plot(epochs, history['train_loss'], 'b-', label='Training Loss')\n",
    "    ax1.plot(epochs, history['val_loss'], 'r-', label='Validation Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title(f'{title} - Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Learning rate schedule\n",
    "    ax2.plot(epochs, history['learning_rate'], 'g-', linewidth=2)\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Learning Rate')\n",
    "    ax2.set_title(f'{title} - Learning Rate Schedule')\n",
    "    ax2.set_yscale('log')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(focal_history, \"Focal Loss Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Training with Weighted Binary Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_weighted_bce(data, epochs=50, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Train fraud detection model using Weighted Binary Cross-Entropy.\n",
    "    This is the traditional approach for handling class imbalance.\n",
    "    \"\"\"\n",
    "    # Initialize model\n",
    "    model = FraudDetectionNN(data['input_size']).to(device)\n",
    "    \n",
    "    # Initialize loss function and optimizer\n",
    "    criterion = WeightedBCELoss(data['pos_weight'])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.5)\n",
    "    \n",
    "    # Training history\n",
    "    history = {'train_loss': [], 'val_loss': [], 'learning_rate': []}\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(\"Training with Weighted BCE...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch_X, batch_y in data['train_loader']:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in data['test_loader']:\n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        # Calculate average losses\n",
    "        avg_train_loss = train_loss / len(data['train_loader'])\n",
    "        avg_val_loss = val_loss / len(data['test_loader'])\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        \n",
    "        # Save history\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['learning_rate'].append(current_lr)\n",
    "        \n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            # Save best model\n",
    "            torch.save(model.state_dict(), 'best_weighted_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Print progress\n",
    "        if epoch % 5 == 0 or epoch == epochs - 1:\n",
    "            print(f\"Epoch {epoch+1:2d}: Train Loss={avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss={avg_val_loss:.4f}, LR={current_lr:.2e}\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load('best_weighted_model.pth'))\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Train the model\n",
    "weighted_model, weighted_history = train_model_with_weighted_bce(data)\n",
    "\n",
    "plot_training_history(weighted_history, \"Weighted BCE Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Training the Autoencoder for Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder(data, epochs=50, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Train autoencoder for anomaly detection.\n",
    "    \n",
    "    Key principle: Train only on normal transactions!\n",
    "    \"\"\"\n",
    "    # Initialize model\n",
    "    model = FraudAutoencoder(data['input_size']).to(device)\n",
    "    \n",
    "    # Create dataset with only normal transactions\n",
    "    normal_mask = data['y_train'] == 0\n",
    "    X_normal = data['X_train'][normal_mask]\n",
    "    \n",
    "    # Create data loader for normal transactions only\n",
    "    normal_dataset = TensorDataset(X_normal, X_normal)  # Input = Output for autoencoder\n",
    "    normal_loader = DataLoader(normal_dataset, batch_size=512, shuffle=True)\n",
    "    \n",
    "    # Initialize loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.5)\n",
    "    \n",
    "    # Training history\n",
    "    history = {'train_loss': [], 'learning_rate': []}\n",
    "    \n",
    "    print(f\"Training Autoencoder on {len(X_normal):,} normal transactions...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch_X, _ in normal_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            reconstructed = model(batch_X)\n",
    "            loss = criterion(reconstructed, batch_X)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Calculate average loss\n",
    "        avg_train_loss = train_loss / len(normal_loader)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        \n",
    "        # Save history\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['learning_rate'].append(current_lr)\n",
    "        \n",
    "        # Print progress\n",
    "        if epoch % 5 == 0 or epoch == epochs - 1:\n",
    "            print(f\"Epoch {epoch+1:2d}: Reconstruction Loss={avg_train_loss:.4f}, \"\n",
    "                  f\"LR={current_lr:.2e}\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder, autoencoder_history = train_autoencoder(data)\n",
    "\n",
    "# Visualize autoencoder training\n",
    "def plot_autoencoder_history(history):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    # Reconstruction loss\n",
    "    ax1.plot(epochs, history['train_loss'], 'b-', linewidth=2)\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Reconstruction Loss (MSE)')\n",
    "    ax1.set_title('Autoencoder Training - Reconstruction Loss')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Learning rate schedule\n",
    "    ax2.plot(epochs, history['learning_rate'], 'g-', linewidth=2)\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Learning Rate')\n",
    "    ax2.set_title('Autoencoder Training - Learning Rate Schedule')\n",
    "    ax2.set_yscale('log')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_autoencoder_history(autoencoder_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Model Evaluation and Comparison\n",
    "\n",
    "### Comprehensive Evaluation Metrics\n",
    "\n",
    "For fraud detection, we need to evaluate multiple aspects:\n",
    "- **Classification Performance**: Precision, Recall, F1-Score, AUC\n",
    "- **Business Impact**: Cost analysis, false positive rates\n",
    "- **Calibration**: How well do probabilities match actual frequencies?\n",
    "- **Threshold Analysis**: Finding optimal decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification_model(model, data, model_name):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation of classification model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get predictions\n",
    "        outputs = model(data['X_test'])\n",
    "        probabilities = torch.sigmoid(outputs).cpu().numpy()\n",
    "        predictions = (probabilities > 0.5).astype(int)\n",
    "        \n",
    "        # True labels\n",
    "        y_true = data['y_test'].cpu().numpy()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        from sklearn.metrics import (\n",
    "            accuracy_score, precision_score, recall_score, f1_score,\n",
    "            roc_auc_score, confusion_matrix, classification_report\n",
    "        )\n",
    "        \n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_true, predictions),\n",
    "            'precision': precision_score(y_true, predictions),\n",
    "            'recall': recall_score(y_true, predictions),\n",
    "            'f1_score': f1_score(y_true, predictions),\n",
    "            'roc_auc': roc_auc_score(y_true, probabilities)\n",
    "        }\n",
    "        \n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(y_true, predictions)\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        \n",
    "        metrics.update({\n",
    "            'true_negatives': tn,\n",
    "            'false_positives': fp,\n",
    "            'false_negatives': fn,\n",
    "            'true_positives': tp\n",
    "        })\n",
    "        \n",
    "        return metrics, probabilities, predictions\n",
    "\n",
    "def evaluate_autoencoder(model, data, model_name):\n",
    "    \"\"\"\n",
    "    Evaluate autoencoder using reconstruction error for anomaly detection.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get reconstruction errors\n",
    "        reconstruction_errors = model.get_reconstruction_error(data['X_test']).cpu().numpy()\n",
    "        y_true = data['y_test'].cpu().numpy()\n",
    "        \n",
    "        # Find optimal threshold using ROC curve\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, reconstruction_errors)\n",
    "        \n",
    "        # Optimal threshold (Youden's J statistic)\n",
    "        optimal_idx = np.argmax(tpr - fpr)\n",
    "        optimal_threshold = thresholds[optimal_idx]\n",
    "        \n",
    "        # Make predictions using optimal threshold\n",
    "        predictions = (reconstruction_errors > optimal_threshold).astype(int)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        from sklearn.metrics import (\n",
    "            accuracy_score, precision_score, recall_score, f1_score,\n",
    "            roc_auc_score, confusion_matrix\n",
    "        )\n",
    "        \n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_true, predictions),\n",
    "            'precision': precision_score(y_true, predictions),\n",
    "            'recall': recall_score(y_true, predictions),\n",
    "            'f1_score': f1_score(y_true, predictions),\n",
    "            'roc_auc': roc_auc_score(y_true, reconstruction_errors),\n",
    "            'optimal_threshold': optimal_threshold\n",
    "        }\n",
    "        \n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(y_true, predictions)\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        \n",
    "        metrics.update({\n",
    "            'true_negatives': tn,\n",
    "            'false_positives': fp,\n",
    "            'false_negatives': fn,\n",
    "            'true_positives': tp\n",
    "        })\n",
    "        \n",
    "        return metrics, reconstruction_errors, predictions\n",
    "\n",
    "# Evaluate all models\n",
    "print(\"Evaluating all models...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Classification models\n",
    "focal_metrics, focal_probs, focal_preds = evaluate_classification_model(focal_model, data, \"Focal Loss\")\n",
    "weighted_metrics, weighted_probs, weighted_preds = evaluate_classification_model(weighted_model, data, \"Weighted BCE\")\n",
    "\n",
    "# Autoencoder\n",
    "autoencoder_metrics, ae_errors, ae_preds = evaluate_autoencoder(autoencoder, data, \"Autoencoder\")\n",
    "\n",
    "# Create comparison table\n",
    "comparison_data = {\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC', 'True Positives', 'False Positives', 'False Negatives'],\n",
    "    'Focal Loss': [\n",
    "        f\"{focal_metrics['accuracy']:.4f}\",\n",
    "        f\"{focal_metrics['precision']:.4f}\",\n",
    "        f\"{focal_metrics['recall']:.4f}\",\n",
    "        f\"{focal_metrics['f1_score']:.4f}\",\n",
    "        f\"{focal_metrics['roc_auc']:.4f}\",\n",
    "        f\"{focal_metrics['true_positives']}\",\n",
    "        f\"{focal_metrics['false_positives']}\",\n",
    "        f\"{focal_metrics['false_negatives']}\"\n",
    "    ],\n",
    "    'Weighted BCE': [\n",
    "        f\"{weighted_metrics['accuracy']:.4f}\",\n",
    "        f\"{weighted_metrics['precision']:.4f}\",\n",
    "        f\"{weighted_metrics['recall']:.4f}\",\n",
    "        f\"{weighted_metrics['f1_score']:.4f}\",\n",
    "        f\"{weighted_metrics['roc_auc']:.4f}\",\n",
    "        f\"{weighted_metrics['true_positives']}\",\n",
    "        f\"{weighted_metrics['false_positives']}\",\n",
    "        f\"{weighted_metrics['false_negatives']}\"\n",
    "    ],\n",
    "    'Autoencoder': [\n",
    "        f\"{autoencoder_metrics['accuracy']:.4f}\",\n",
    "        f\"{autoencoder_metrics['precision']:.4f}\",\n",
    "        f\"{autoencoder_metrics['recall']:.4f}\",\n",
    "        f\"{autoencoder_metrics['f1_score']:.4f}\",\n",
    "        f\"{autoencoder_metrics['roc_auc']:.4f}\",\n",
    "        f\"{autoencoder_metrics['true_positives']}\",\n",
    "        f\"{autoencoder_metrics['false_positives']}\",\n",
    "        f\"{autoencoder_metrics['false_negatives']}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Determine best model\n",
    "f1_scores = {\n",
    "    'Focal Loss': focal_metrics['f1_score'],\n",
    "    'Weighted BCE': weighted_metrics['f1_score'],\n",
    "    'Autoencoder': autoencoder_metrics['f1_score']\n",
    "}\n",
    "\n",
    "best_model_name = max(f1_scores, key=f1_scores.get)\n",
    "print(f\"\\nBest Model: {best_model_name} (F1-Score: {f1_scores[best_model_name]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Advanced Visualizations and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "def create_comprehensive_evaluation_plots(data, focal_probs, weighted_probs, ae_errors):\n",
    "    \"\"\"\n",
    "    Create comprehensive evaluation plots for all models.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    y_true = data['y_test'].cpu().numpy()\n",
    "    \n",
    "    # ROC Curves\n",
    "    fpr_focal, tpr_focal, _ = roc_curve(y_true, focal_probs)\n",
    "    fpr_weighted, tpr_weighted, _ = roc_curve(y_true, weighted_probs)\n",
    "    fpr_ae, tpr_ae, _ = roc_curve(y_true, ae_errors)\n",
    "    \n",
    "    axes[0, 0].plot(fpr_focal, tpr_focal, label=f'Focal Loss (AUC={auc(fpr_focal, tpr_focal):.3f})', linewidth=2)\n",
    "    axes[0, 0].plot(fpr_weighted, tpr_weighted, label=f'Weighted BCE (AUC={auc(fpr_weighted, tpr_weighted):.3f})', linewidth=2)\n",
    "    axes[0, 0].plot(fpr_ae, tpr_ae, label=f'Autoencoder (AUC={auc(fpr_ae, tpr_ae):.3f})', linewidth=2)\n",
    "    axes[0, 0].plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "    axes[0, 0].set_xlabel('False Positive Rate')\n",
    "    axes[0, 0].set_ylabel('True Positive Rate')\n",
    "    axes[0, 0].set_title('ROC Curves')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Precision-Recall Curves\n",
    "    precision_focal, recall_focal, _ = precision_recall_curve(y_true, focal_probs)\n",
    "    precision_weighted, recall_weighted, _ = precision_recall_curve(y_true, weighted_probs)\n",
    "    precision_ae, recall_ae, _ = precision_recall_curve(y_true, ae_errors)\n",
    "    \n",
    "    axes[0, 1].plot(recall_focal, precision_focal, label=f'Focal Loss (AP={average_precision_score(y_true, focal_probs):.3f})', linewidth=2)\n",
    "    axes[0, 1].plot(recall_weighted, precision_weighted, label=f'Weighted BCE (AP={average_precision_score(y_true, weighted_probs):.3f})', linewidth=2)\n",
    "    axes[0, 1].plot(recall_ae, precision_ae, label=f'Autoencoder (AP={average_precision_score(y_true, ae_errors):.3f})', linewidth=2)\n",
    "    axes[0, 1].set_xlabel('Recall')\n",
    "    axes[0, 1].set_ylabel('Precision')\n",
    "    axes[0, 1].set_title('Precision-Recall Curves')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Score distributions\n",
    "    normal_mask = y_true == 0\n",
    "    fraud_mask = y_true == 1\n",
    "    \n",
    "    axes[0, 2].hist(focal_probs[normal_mask], bins=50, alpha=0.7, label='Normal', density=True)\n",
    "    axes[0, 2].hist(focal_probs[fraud_mask], bins=50, alpha=0.7, label='Fraud', density=True)\n",
    "    axes[0, 2].set_xlabel('Predicted Probability')\n",
    "    axes[0, 2].set_ylabel('Density')\n",
    "    axes[0, 2].set_title('Focal Loss - Score Distribution')\n",
    "    axes[0, 2].legend()\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Confusion matrices\n",
    "    cm_focal = confusion_matrix(y_true, (focal_probs > 0.5).astype(int))\n",
    "    cm_weighted = confusion_matrix(y_true, (weighted_probs > 0.5).astype(int))\n",
    "    cm_ae = confusion_matrix(y_true, (ae_errors > autoencoder_metrics['optimal_threshold']).astype(int))\n",
    "    \n",
    "    # Plot confusion matrices\n",
    "    im1 = axes[1, 0].imshow(cm_focal, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    axes[1, 0].set_title('Focal Loss - Confusion Matrix')\n",
    "    axes[1, 0].set_ylabel('True Label')\n",
    "    axes[1, 0].set_xlabel('Predicted Label')\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            axes[1, 0].text(j, i, format(cm_focal[i, j], 'd'),\n",
    "                          ha=\"center\", va=\"center\", color=\"white\" if cm_focal[i, j] > cm_focal.max() / 2 else \"black\")\n",
    "    \n",
    "    im2 = axes[1, 1].imshow(cm_weighted, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    axes[1, 1].set_title('Weighted BCE - Confusion Matrix')\n",
    "    axes[1, 1].set_ylabel('True Label')\n",
    "    axes[1, 1].set_xlabel('Predicted Label')\n",
    "    \n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            axes[1, 1].text(j, i, format(cm_weighted[i, j], 'd'),\n",
    "                          ha=\"center\", va=\"center\", color=\"white\" if cm_weighted[i, j] > cm_weighted.max() / 2 else \"black\")\n",
    "    \n",
    "    im3 = axes[1, 2].imshow(cm_ae, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    axes[1, 2].set_title('Autoencoder - Confusion Matrix')\n",
    "    axes[1, 2].set_ylabel('True Label')\n",
    "    axes[1, 2].set_xlabel('Predicted Label')\n",
    "    \n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            axes[1, 2].text(j, i, format(cm_ae[i, j], 'd'),\n",
    "                          ha=\"center\", va=\"center\", color=\"white\" if cm_ae[i, j] > cm_ae.max() / 2 else \"black\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create the comprehensive evaluation plots\n",
    "create_comprehensive_evaluation_plots(data, focal_probs, weighted_probs, ae_errors)\n",
    "\n",
    "# Analyze autoencoder reconstruction errors\n",
    "def analyze_autoencoder_performance(data, autoencoder, ae_errors):\n",
    "    \"\"\"\n",
    "    Analyze autoencoder performance in detail.\n",
    "    \"\"\"\n",
    "    y_true = data['y_test'].cpu().numpy()\n",
    "    normal_mask = y_true == 0\n",
    "    fraud_mask = y_true == 1\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Reconstruction error distribution\n",
    "    axes[0].hist(ae_errors[normal_mask], bins=50, alpha=0.7, label='Normal', density=True, color='blue')\n",
    "    axes[0].hist(ae_errors[fraud_mask], bins=50, alpha=0.7, label='Fraud', density=True, color='red')\n",
    "    axes[0].axvline(autoencoder_metrics['optimal_threshold'], color='green', linestyle='--', \n",
    "                   label=f'Optimal Threshold: {autoencoder_metrics[\"optimal_threshold\"]:.4f}')\n",
    "    axes[0].set_xlabel('Reconstruction Error')\n",
    "    axes[0].set_ylabel('Density')\n",
    "    axes[0].set_title('Reconstruction Error Distribution')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Box plot of reconstruction errors\n",
    "    box_data = [ae_errors[normal_mask], ae_errors[fraud_mask]]\n",
    "    axes[1].boxplot(box_data, labels=['Normal', 'Fraud'])\n",
    "    axes[1].set_ylabel('Reconstruction Error')\n",
    "    axes[1].set_title('Reconstruction Error by Class')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Threshold analysis\n",
    "    thresholds = np.linspace(ae_errors.min(), ae_errors.max(), 100)\n",
    "    f1_scores = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        predictions = (ae_errors > threshold).astype(int)\n",
    "        f1 = f1_score(y_true, predictions)\n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "    axes[2].plot(thresholds, f1_scores, 'b-', linewidth=2)\n",
    "    axes[2].axvline(autoencoder_metrics['optimal_threshold'], color='red', linestyle='--', \n",
    "                   label=f'Optimal: {autoencoder_metrics[\"optimal_threshold\"]:.4f}')\n",
    "    axes[2].set_xlabel('Threshold')\n",
    "    axes[2].set_ylabel('F1-Score')\n",
    "    axes[2].set_title('F1-Score vs Threshold')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistics\n",
    "    print(\"\\nAutoencoder Analysis:\")\n",
    "    print(f\"Normal transactions - Mean error: {ae_errors[normal_mask].mean():.4f}, Std: {ae_errors[normal_mask].std():.4f}\")\n",
    "    print(f\"Fraud transactions - Mean error: {ae_errors[fraud_mask].mean():.4f}, Std: {ae_errors[fraud_mask].std():.4f}\")\n",
    "    print(f\"Separation ratio: {ae_errors[fraud_mask].mean() / ae_errors[normal_mask].mean():.2f}x\")\n",
    "\n",
    "analyze_autoencoder_performance(data, autoencoder, ae_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises\n",
    "\n",
    "Now it's your turn! Try these exercises to deepen your understanding:\n",
    "\n",
    "### Exercise 1: Hyperparameter Tuning\n",
    "Experiment with different Focal Loss parameters (α, γ) and see how they affect performance. Try:\n",
    "- α = [0.25, 0.5, 0.75, 1.0]\n",
    "- γ = [0.5, 1.0, 2.0, 5.0]\n",
    "\n",
    "Which combination works best for your dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Create a parameter grid and train models with different α and γ values\n",
    "# Compare their F1-scores to find the best combination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Architecture Experimentation\n",
    "Modify the neural network architecture:\n",
    "- Try different numbers of layers\n",
    "- Experiment with different layer sizes\n",
    "- Add residual connections\n",
    "- Try different activation functions\n",
    "\n",
    "How does architecture affect performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Create a new class inheriting from nn.Module\n",
    "# Experiment with different architectures and compare results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Ensemble Methods\n",
    "Create an ensemble of your three models:\n",
    "- Use voting (majority rule)\n",
    "- Use weighted averaging based on model confidence\n",
    "- Use stacking with a meta-learner\n",
    "\n",
    "Does the ensemble perform better than individual models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Combine predictions from all three models using different strategies\n",
    "# Compare ensemble performance with individual models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### 1. Class Imbalance Solutions\n",
    "- **Focal Loss**: Focuses on hard examples, reduces impact of easy negatives\n",
    "- **Weighted BCE**: Traditional approach using class weights\n",
    "- **Autoencoder**: Unsupervised approach learning normal patterns\n",
    "\n",
    "### 2. Modern Architecture Design\n",
    "- **Batch Normalization**: Stabilizes training, allows higher learning rates\n",
    "- **Dropout**: Prevents overfitting, crucial for small datasets\n",
    "- **Proper Initialization**: Xavier/He initialization prevents gradient problems\n",
    "- **Progressive Reduction**: Gradual dimension reduction works better\n",
    "\n",
    "### 3. Training Best Practices\n",
    "- **Learning Rate Scheduling**: Adaptive learning rates improve convergence\n",
    "- **Early Stopping**: Prevents overfitting, saves computational resources\n",
    "- **Gradient Clipping**: Prevents exploding gradients\n",
    "- **Mixed Precision**: Faster training on modern GPUs\n",
    "\n",
    "### 4. Evaluation Strategies\n",
    "- **Multiple Metrics**: Don't rely on accuracy alone\n",
    "- **Threshold Analysis**: Find optimal decision boundaries\n",
    "- **Business Context**: Consider false positive/negative costs\n",
    "- **Visualization**: ROC curves, PR curves, score distributions\n",
    "\n",
    "### 5. Model Selection\n",
    "- **Focal Loss**: Best for extreme imbalance (like fraud detection)\n",
    "- **Weighted BCE**: Good baseline, easy to implement\n",
    "- **Autoencoder**: Useful when labeled fraud data is limited\n",
    "- **Ensemble**: Often provides best overall performance\n",
    "\n",
    "### 6. Production Considerations\n",
    "- **GPU Optimization**: Use appropriate batch sizes and memory management\n",
    "- **Model Serving**: Consider inference speed and resource requirements\n",
    "- **Monitoring**: Track model performance and drift over time\n",
    "- **Retraining**: Update models as fraud patterns evolve\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next tutorial, we'll explore:\n",
    "- Graph Neural Networks for fraud detection\n",
    "- Capturing relationships between transactions\n",
    "- Advanced feature engineering with graph structures\n",
    "- Heterogeneous graph neural networks\n",
    "\n",
    "Remember: The best model is the one that best serves your specific business needs and constraints!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}