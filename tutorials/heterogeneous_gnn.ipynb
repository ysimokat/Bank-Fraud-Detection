{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heterogeneous Graph Neural Networks for Fraud Detection\n",
    "\n",
    "## Tutorial 6: Graph-Based Fraud Detection with Multi-Type Relationships\n",
    "\n",
    "In this tutorial, you'll learn how to leverage the power of Graph Neural Networks (GNNs) for fraud detection:\n",
    "- **Heterogeneous Graphs**: Multiple node types (users, merchants, transactions)\n",
    "- **Graph Attention Networks**: Focus on important relationships\n",
    "- **Message Passing**: Information flow between connected entities\n",
    "- **Advanced Graph Construction**: Creating realistic financial networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you'll understand:\n",
    "\n",
    "1. **Graph-Based Fraud Detection**: Why graphs are powerful for fraud detection\n",
    "2. **Heterogeneous Graphs**: Multiple node types and relationship types\n",
    "3. **Graph Attention Networks**: Attention mechanisms in graph neural networks\n",
    "4. **Message Passing**: How information flows through graph structures\n",
    "5. **Graph Construction**: Building realistic financial networks from transaction data\n",
    "6. **Advanced Embeddings**: Learning representations that capture graph structure\n",
    "7. **Scalability**: Handling large-scale financial networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, HeteroConv, global_mean_pool\n",
    "from torch_geometric.data import HeteroData\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Graph-Based Fraud Detection\n",
    "\n",
    "### Why Graphs for Fraud Detection?\n",
    "\n",
    "Traditional ML approaches treat each transaction independently. But fraud detection is inherently about relationships:\n",
    "- **Fraudsters often target similar merchants**\n",
    "- **Compromised cards show unusual behavioral patterns**\n",
    "- **Fraudulent transactions cluster in time and space**\n",
    "- **Legitimate users have consistent behavioral patterns**\n",
    "\n",
    "### Heterogeneous vs Homogeneous Graphs\n",
    "\n",
    "**Homogeneous Graph**: One type of node, one type of edge\n",
    "- Simple but limited representation\n",
    "- Example: User-User connections only\n",
    "\n",
    "**Heterogeneous Graph**: Multiple node types, multiple edge types\n",
    "- Rich representation of real-world complexity\n",
    "- Example: Users, Merchants, Transactions with various relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and examine the data\n",
    "df = pd.read_csv('creditcard.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Fraud rate: {df['Class'].mean()*100:.3f}%\")\n",
    "\n",
    "# Visualize the challenge\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Transaction timeline\n",
    "df_sample = df.sample(1000, random_state=42)\n",
    "normal_transactions = df_sample[df_sample['Class'] == 0]\n",
    "fraud_transactions = df_sample[df_sample['Class'] == 1]\n",
    "\n",
    "axes[0].scatter(normal_transactions['Time'], normal_transactions['Amount'], \n",
    "               alpha=0.6, s=20, label='Normal', color='blue')\n",
    "axes[0].scatter(fraud_transactions['Time'], fraud_transactions['Amount'], \n",
    "               alpha=0.8, s=50, label='Fraud', color='red')\n",
    "axes[0].set_xlabel('Time')\n",
    "axes[0].set_ylabel('Amount')\n",
    "axes[0].set_title('Transactions in Time-Amount Space')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Feature space visualization (PCA)\n",
    "features = ['V1', 'V2', 'V3', 'V4', 'V5']\n",
    "X_sample = df_sample[features]\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_sample)\n",
    "\n",
    "normal_mask = df_sample['Class'] == 0\n",
    "fraud_mask = df_sample['Class'] == 1\n",
    "\n",
    "axes[1].scatter(X_pca[normal_mask, 0], X_pca[normal_mask, 1], \n",
    "               alpha=0.6, s=20, label='Normal', color='blue')\n",
    "axes[1].scatter(X_pca[fraud_mask, 0], X_pca[fraud_mask, 1], \n",
    "               alpha=0.8, s=50, label='Fraud', color='red')\n",
    "axes[1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2f})')\n",
    "axes[1].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2f})')\n",
    "axes[1].set_title('Feature Space (PCA)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Graph concept illustration\n",
    "axes[2].text(0.5, 0.8, 'Traditional ML', ha='center', va='center', \n",
    "            fontsize=14, fontweight='bold', transform=axes[2].transAxes)\n",
    "axes[2].text(0.5, 0.7, 'Each transaction\\nprocessed independently', ha='center', va='center', \n",
    "            fontsize=12, transform=axes[2].transAxes)\n",
    "axes[2].text(0.5, 0.5, 'vs', ha='center', va='center', \n",
    "            fontsize=16, fontweight='bold', transform=axes[2].transAxes)\n",
    "axes[2].text(0.5, 0.3, 'Graph-Based ML', ha='center', va='center', \n",
    "            fontsize=14, fontweight='bold', transform=axes[2].transAxes)\n",
    "axes[2].text(0.5, 0.2, 'Transactions connected\\nthrough relationships', ha='center', va='center', \n",
    "            fontsize=12, transform=axes[2].transAxes)\n",
    "axes[2].set_xlim(0, 1)\n",
    "axes[2].set_ylim(0, 1)\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insight: Fraud detection benefits from understanding relationships between entities!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Building a Heterogeneous Graph\n",
    "\n",
    "### Graph Design for Fraud Detection\n",
    "\n",
    "Our heterogeneous graph will have:\n",
    "- **3 Node Types**: Users, Merchants, Transactions\n",
    "- **7 Edge Types**: \n",
    "  - User → Transaction (makes)\n",
    "  - Transaction → User (made_by)\n",
    "  - Merchant → Transaction (processes)\n",
    "  - Transaction → Merchant (processed_by)\n",
    "  - User ↔ User (similar_to)\n",
    "  - Merchant ↔ Merchant (similar_to)\n",
    "  - Transaction ↔ Transaction (temporal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeterogeneousGraphBuilder:\n",
    "    \"\"\"\n",
    "    Build heterogeneous graphs from transaction data.\n",
    "    \n",
    "    Key challenge: Transaction data doesn't have user/merchant IDs.\n",
    "    Solution: Use clustering to create synthetic entities based on behavioral patterns.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, random_state=42):\n",
    "        self.random_state = random_state\n",
    "        self.user_clusters = None\n",
    "        self.merchant_clusters = None\n",
    "        self.scaler = StandardScaler()\n",
    "    \n",
    "    def build_heterogeneous_graph(self, df, max_users=1000, max_merchants=500):\n",
    "        \"\"\"\n",
    "        Build complete heterogeneous graph from transaction data.\n",
    "        \n",
    "        Args:\n",
    "            df: Transaction dataframe\n",
    "            max_users: Maximum number of synthetic users\n",
    "            max_merchants: Maximum number of synthetic merchants\n",
    "        \n",
    "        Returns:\n",
    "            HeteroData object with all nodes and edges\n",
    "        \"\"\"\n",
    "        print(\"Building heterogeneous graph...\")\n",
    "        \n",
    "        # Create synthetic entities\n",
    "        df_with_entities = self._create_entities(df, max_users, max_merchants)\n",
    "        \n",
    "        # Initialize graph data structure\n",
    "        data = HeteroData()\n",
    "        \n",
    "        # Add node features\n",
    "        data = self._add_node_features(data, df_with_entities)\n",
    "        \n",
    "        # Add edges\n",
    "        data = self._add_edges(data, df_with_entities)\n",
    "        \n",
    "        print(f\"Graph constructed with:\")\n",
    "        print(f\"  - {data['transaction'].num_nodes:,} transaction nodes\")\n",
    "        print(f\"  - {data['user'].num_nodes:,} user nodes\")\n",
    "        print(f\"  - {data['merchant'].num_nodes:,} merchant nodes\")\n",
    "        \n",
    "        return data, df_with_entities\n",
    "    \n",
    "    def _create_entities(self, df, max_users, max_merchants):\n",
    "        \"\"\"\n",
    "        Create synthetic user and merchant entities using clustering.\n",
    "        \n",
    "        Strategy:\n",
    "        - Users: Cluster by behavioral patterns (V1-V5)\n",
    "        - Merchants: Cluster by business patterns (time, amount, V6-V8)\n",
    "        \"\"\"\n",
    "        print(\"Creating synthetic entities...\")\n",
    "        \n",
    "        df_entities = df.copy()\n",
    "        \n",
    "        # Create user clusters based on behavioral patterns\n",
    "        user_features = ['V1', 'V2', 'V3', 'V4', 'V5']\n",
    "        X_user = self.scaler.fit_transform(df[user_features])\n",
    "        \n",
    "        n_user_clusters = min(max_users, len(df) // 10)  # At least 10 transactions per user\n",
    "        self.user_clusters = KMeans(n_clusters=n_user_clusters, random_state=self.random_state)\n",
    "        df_entities['user_id'] = self.user_clusters.fit_predict(X_user)\n",
    "        \n",
    "        # Create merchant clusters based on business patterns\n",
    "        # Extract hour from time\n",
    "        df_entities['hour'] = (df['Time'] % (24 * 3600)) // 3600\n",
    "        \n",
    "        merchant_features = ['hour', 'Amount', 'V6', 'V7', 'V8']\n",
    "        X_merchant = self.scaler.fit_transform(df_entities[merchant_features])\n",
    "        \n",
    "        n_merchant_clusters = min(max_merchants, len(df) // 20)  # At least 20 transactions per merchant\n",
    "        self.merchant_clusters = KMeans(n_clusters=n_merchant_clusters, random_state=self.random_state)\n",
    "        df_entities['merchant_id'] = self.merchant_clusters.fit_predict(X_merchant)\n",
    "        \n",
    "        print(f\"  - Created {n_user_clusters} synthetic users\")\n",
    "        print(f\"  - Created {n_merchant_clusters} synthetic merchants\")\n",
    "        \n",
    "        return df_entities\n",
    "    \n",
    "    def _add_node_features(self, data, df):\n",
    "        \"\"\"\n",
    "        Add features for each node type.\n",
    "        \n",
    "        Node features:\n",
    "        - Transaction: Original features (V1-V28, Amount, Time)\n",
    "        - User: Aggregated behavioral statistics\n",
    "        - Merchant: Aggregated business statistics\n",
    "        \"\"\"\n",
    "        print(\"Adding node features...\")\n",
    "        \n",
    "        # Transaction features (original features)\n",
    "        transaction_features = [f'V{i}' for i in range(1, 29)] + ['Amount', 'Time']\n",
    "        data['transaction'].x = torch.FloatTensor(df[transaction_features].values)\n",
    "        data['transaction'].y = torch.LongTensor(df['Class'].values)\n",
    "        \n",
    "        # User features (aggregated statistics)\n",
    "        user_stats = []\n",
    "        for user_id in range(df['user_id'].nunique()):\n",
    "            user_transactions = df[df['user_id'] == user_id]\n",
    "            \n",
    "            stats = [\n",
    "                len(user_transactions),  # transaction count\n",
    "                user_transactions['Amount'].mean(),  # avg amount\n",
    "                user_transactions['Amount'].std(),  # amount std\n",
    "                user_transactions['Class'].mean(),  # fraud rate\n",
    "                user_transactions['Time'].max() - user_transactions['Time'].min(),  # time range\n",
    "                user_transactions[[f'V{i}' for i in range(1, 6)]].mean().mean(),  # avg V features\n",
    "                user_transactions['Amount'].quantile(0.95),  # 95th percentile amount\n",
    "                user_transactions['Time'].std()  # time std\n",
    "            ]\n",
    "            \n",
    "            # Handle NaN values\n",
    "            stats = [0 if pd.isna(x) else x for x in stats]\n",
    "            user_stats.append(stats)\n",
    "        \n",
    "        data['user'].x = torch.FloatTensor(user_stats)\n",
    "        \n",
    "        # Merchant features (aggregated statistics)\n",
    "        merchant_stats = []\n",
    "        for merchant_id in range(df['merchant_id'].nunique()):\n",
    "            merchant_transactions = df[df['merchant_id'] == merchant_id]\n",
    "            \n",
    "            stats = [\n",
    "                len(merchant_transactions),  # transaction count\n",
    "                merchant_transactions['Amount'].mean(),  # avg amount\n",
    "                merchant_transactions['Amount'].std(),  # amount std\n",
    "                merchant_transactions['Class'].mean(),  # fraud rate\n",
    "                merchant_transactions['user_id'].nunique(),  # unique customers\n",
    "                merchant_transactions['Time'].max() - merchant_transactions['Time'].min(),  # time span\n",
    "                merchant_transactions[[f'V{i}' for i in range(6, 11)]].mean().mean(),  # avg V features\n",
    "                merchant_transactions['hour'].mode().iloc[0] if len(merchant_transactions) > 0 else 12  # peak hour\n",
    "            ]\n",
    "            \n",
    "            # Handle NaN values\n",
    "            stats = [0 if pd.isna(x) else x for x in stats]\n",
    "            merchant_stats.append(stats)\n",
    "        \n",
    "        data['merchant'].x = torch.FloatTensor(merchant_stats)\n",
    "        \n",
    "        print(f\"  - Transaction features: {data['transaction'].x.shape}\")\n",
    "        print(f\"  - User features: {data['user'].x.shape}\")\n",
    "        print(f\"  - Merchant features: {data['merchant'].x.shape}\")\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def _add_edges(self, data, df):\n",
    "        \"\"\"\n",
    "        Add all edge types to the graph.\n",
    "        \n",
    "        Edge types:\n",
    "        1. User-Transaction relationships\n",
    "        2. Merchant-Transaction relationships\n",
    "        3. User-User similarity\n",
    "        4. Merchant-Merchant similarity\n",
    "        5. Transaction-Transaction temporal\n",
    "        \"\"\"\n",
    "        print(\"Adding edges...\")\n",
    "        \n",
    "        # 1. User-Transaction edges\n",
    "        user_transaction_edges = []\n",
    "        transaction_user_edges = []\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            user_transaction_edges.append([row['user_id'], idx])\n",
    "            transaction_user_edges.append([idx, row['user_id']])\n",
    "        \n",
    "        data['user', 'makes', 'transaction'].edge_index = torch.LongTensor(user_transaction_edges).t().contiguous()\n",
    "        data['transaction', 'made_by', 'user'].edge_index = torch.LongTensor(transaction_user_edges).t().contiguous()\n",
    "        \n",
    "        # 2. Merchant-Transaction edges\n",
    "        merchant_transaction_edges = []\n",
    "        transaction_merchant_edges = []\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            merchant_transaction_edges.append([row['merchant_id'], idx])\n",
    "            transaction_merchant_edges.append([idx, row['merchant_id']])\n",
    "        \n",
    "        data['merchant', 'processes', 'transaction'].edge_index = torch.LongTensor(merchant_transaction_edges).t().contiguous()\n",
    "        data['transaction', 'processed_by', 'merchant'].edge_index = torch.LongTensor(transaction_merchant_edges).t().contiguous()\n",
    "        \n",
    "        # 3. User-User similarity edges\n",
    "        user_similarity_edges = self._create_similarity_edges(data['user'].x, threshold=0.8)\n",
    "        if len(user_similarity_edges) > 0:\n",
    "            data['user', 'similar_to', 'user'].edge_index = torch.LongTensor(user_similarity_edges).t().contiguous()\n",
    "        \n",
    "        # 4. Merchant-Merchant similarity edges\n",
    "        merchant_similarity_edges = self._create_similarity_edges(data['merchant'].x, threshold=0.8)\n",
    "        if len(merchant_similarity_edges) > 0:\n",
    "            data['merchant', 'similar_to', 'merchant'].edge_index = torch.LongTensor(merchant_similarity_edges).t().contiguous()\n",
    "        \n",
    "        # 5. Transaction-Transaction temporal edges\n",
    "        temporal_edges = self._create_temporal_edges(df, max_time_diff=3600)  # 1 hour\n",
    "        if len(temporal_edges) > 0:\n",
    "            data['transaction', 'temporal', 'transaction'].edge_index = torch.LongTensor(temporal_edges).t().contiguous()\n",
    "        \n",
    "        # Print edge statistics\n",
    "        print(f\"  - User-Transaction edges: {data['user', 'makes', 'transaction'].edge_index.shape[1]:,}\")\n",
    "        print(f\"  - Merchant-Transaction edges: {data['merchant', 'processes', 'transaction'].edge_index.shape[1]:,}\")\n",
    "        print(f\"  - User-User similarity edges: {len(user_similarity_edges):,}\")\n",
    "        print(f\"  - Merchant-Merchant similarity edges: {len(merchant_similarity_edges):,}\")\n",
    "        print(f\"  - Transaction-Transaction temporal edges: {len(temporal_edges):,}\")\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def _create_similarity_edges(self, features, threshold=0.8):\n",
    "        \"\"\"\n",
    "        Create similarity edges based on cosine similarity.\n",
    "        \n",
    "        Args:\n",
    "            features: Node features tensor\n",
    "            threshold: Similarity threshold\n",
    "        \n",
    "        Returns:\n",
    "            List of edge pairs\n",
    "        \"\"\"\n",
    "        if len(features) > 1000:  # Limit for computational efficiency\n",
    "            return []\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        similarity_matrix = cosine_similarity(features.numpy())\n",
    "        \n",
    "        # Find pairs above threshold\n",
    "        edges = []\n",
    "        for i in range(len(similarity_matrix)):\n",
    "            for j in range(i + 1, len(similarity_matrix)):\n",
    "                if similarity_matrix[i, j] > threshold:\n",
    "                    edges.append([i, j])\n",
    "                    edges.append([j, i])  # Undirected edge\n",
    "        \n",
    "        return edges\n",
    "    \n",
    "    def _create_temporal_edges(self, df, max_time_diff=3600):\n",
    "        \"\"\"\n",
    "        Create temporal edges between consecutive transactions.\n",
    "        \n",
    "        Args:\n",
    "            df: Transaction dataframe\n",
    "            max_time_diff: Maximum time difference (seconds)\n",
    "        \n",
    "        Returns:\n",
    "            List of temporal edge pairs\n",
    "        \"\"\"\n",
    "        df_sorted = df.sort_values('Time').reset_index()\n",
    "        \n",
    "        edges = []\n",
    "        for i in range(len(df_sorted) - 1):\n",
    "            current_idx = df_sorted.iloc[i]['index']\n",
    "            next_idx = df_sorted.iloc[i + 1]['index']\n",
    "            \n",
    "            time_diff = df_sorted.iloc[i + 1]['Time'] - df_sorted.iloc[i]['Time']\n",
    "            \n",
    "            if time_diff <= max_time_diff:\n",
    "                edges.append([current_idx, next_idx])\n",
    "        \n",
    "        return edges\n",
    "\n",
    "# Build the graph\n",
    "graph_builder = HeterogeneousGraphBuilder()\n",
    "graph_data, df_with_entities = graph_builder.build_heterogeneous_graph(df, max_users=500, max_merchants=250)\n",
    "\n",
    "print(\"\\nGraph construction complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Visualizing the Heterogeneous Graph\n",
    "\n",
    "Let's visualize our constructed graph to understand its structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize graph structure\n",
    "def visualize_graph_structure(graph_data, df_with_entities):\n",
    "    \"\"\"\n",
    "    Create visualizations to understand the graph structure.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Node type distribution\n",
    "    node_counts = {\n",
    "        'Transactions': graph_data['transaction'].num_nodes,\n",
    "        'Users': graph_data['user'].num_nodes,\n",
    "        'Merchants': graph_data['merchant'].num_nodes\n",
    "    }\n",
    "    \n",
    "    axes[0, 0].bar(node_counts.keys(), node_counts.values(), color=['skyblue', 'lightgreen', 'salmon'])\n",
    "    axes[0, 0].set_ylabel('Number of Nodes')\n",
    "    axes[0, 0].set_title('Node Type Distribution')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Edge type distribution\n",
    "    edge_counts = {}\n",
    "    for edge_type in graph_data.edge_types:\n",
    "        edge_counts[f\"{edge_type[0]}-{edge_type[2]}\"] = graph_data[edge_type].edge_index.shape[1]\n",
    "    \n",
    "    axes[0, 1].bar(range(len(edge_counts)), list(edge_counts.values()), color='lightcoral')\n",
    "    axes[0, 1].set_xticks(range(len(edge_counts)))\n",
    "    axes[0, 1].set_xticklabels(list(edge_counts.keys()), rotation=45, ha='right')\n",
    "    axes[0, 1].set_ylabel('Number of Edges')\n",
    "    axes[0, 1].set_title('Edge Type Distribution')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. User transaction distribution\n",
    "    user_transaction_counts = df_with_entities['user_id'].value_counts()\n",
    "    axes[1, 0].hist(user_transaction_counts.values, bins=50, alpha=0.7, color='lightgreen')\n",
    "    axes[1, 0].set_xlabel('Transactions per User')\n",
    "    axes[1, 0].set_ylabel('Number of Users')\n",
    "    axes[1, 0].set_title('User Activity Distribution')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Merchant transaction distribution\n",
    "    merchant_transaction_counts = df_with_entities['merchant_id'].value_counts()\n",
    "    axes[1, 1].hist(merchant_transaction_counts.values, bins=50, alpha=0.7, color='salmon')\n",
    "    axes[1, 1].set_xlabel('Transactions per Merchant')\n",
    "    axes[1, 1].set_ylabel('Number of Merchants')\n",
    "    axes[1, 1].set_title('Merchant Activity Distribution')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nGraph Statistics:\")\n",
    "    print(f\"Average transactions per user: {user_transaction_counts.mean():.1f}\")\n",
    "    print(f\"Average transactions per merchant: {merchant_transaction_counts.mean():.1f}\")\n",
    "    print(f\"Most active user: {user_transaction_counts.max()} transactions\")\n",
    "    print(f\"Most active merchant: {merchant_transaction_counts.max()} transactions\")\n",
    "\n",
    "# Visualize the graph\n",
    "visualize_graph_structure(graph_data, df_with_entities)\n",
    "\n",
    "# Show graph schema\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HETEROGENEOUS GRAPH SCHEMA\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nNode Types:\")\n",
    "for node_type in graph_data.node_types:\n",
    "    print(f\"  - {node_type}: {graph_data[node_type].num_nodes} nodes, {graph_data[node_type].x.shape[1]} features\")\n",
    "    \n",
    "print(\"\\nEdge Types:\")\n",
    "for edge_type in graph_data.edge_types:\n",
    "    print(f\"  - {edge_type[0]} --[{edge_type[1]}]--> {edge_type[2]}: {graph_data[edge_type].edge_index.shape[1]} edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Graph Attention Networks (GAT)\n",
    "\n",
    "### Understanding Graph Attention\n",
    "\n",
    "Graph Attention Networks use attention mechanisms to:\n",
    "- **Focus on important neighbors**: Not all connections are equally important\n",
    "- **Learn relationship weights**: Dynamically determine edge importance\n",
    "- **Handle heterogeneous graphs**: Different attention for different edge types\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "For a node $i$ with neighbors $j \\in N(i)$:\n",
    "\n",
    "1. **Attention coefficient**: $e_{ij} = a(W h_i, W h_j)$\n",
    "2. **Normalized attention**: $\\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k \\in N(i)} \\exp(e_{ik})}$\n",
    "3. **Aggregated features**: $h_i' = \\sigma\\left(\\sum_{j \\in N(i)} \\alpha_{ij} W h_j\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeterogeneousGAT(nn.Module):\n",
    "    \"\"\"\n",
    "    Heterogeneous Graph Attention Network for fraud detection.\n",
    "    \n",
    "    Key components:\n",
    "    1. Input projection layers for each node type\n",
    "    2. Multiple GAT layers with heterogeneous message passing\n",
    "    3. Multi-head attention for capturing different aspects\n",
    "    4. Final classifier for fraud prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, node_types, edge_types, hidden_dim=64, num_heads=4, num_layers=2, dropout=0.2):\n",
    "        super(HeterogeneousGAT, self).__init__()\n",
    "        \n",
    "        self.node_types = node_types\n",
    "        self.edge_types = edge_types\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Input projections for each node type\n",
    "        self.input_projections = nn.ModuleDict()\n",
    "        for node_type, input_dim in node_types.items():\n",
    "            self.input_projections[node_type] = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        # Heterogeneous convolution layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            conv_dict = {}\n",
    "            for edge_type in edge_types:\n",
    "                conv_dict[edge_type] = GATConv(\n",
    "                    hidden_dim, hidden_dim // num_heads, \n",
    "                    heads=num_heads, dropout=dropout, concat=True\n",
    "                )\n",
    "            self.convs.append(HeteroConv(conv_dict, aggr='sum'))\n",
    "        \n",
    "        # Final classifier (for transaction nodes)\n",
    "        self.transaction_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim // 2, 2)  # Binary classification\n",
    "        )\n",
    "        \n",
    "        # Optional: Global attention for graph-level features\n",
    "        self.global_attention = nn.MultiheadAttention(hidden_dim, num_heads, dropout=dropout)\n",
    "        \n",
    "    def forward(self, x_dict, edge_index_dict, batch_dict=None):\n",
    "        \"\"\"\n",
    "        Forward pass through the heterogeneous GAT.\n",
    "        \n",
    "        Args:\n",
    "            x_dict: Dictionary of node features for each node type\n",
    "            edge_index_dict: Dictionary of edge indices for each edge type\n",
    "            batch_dict: Optional batch information\n",
    "        \n",
    "        Returns:\n",
    "            Fraud predictions for transaction nodes\n",
    "        \"\"\"\n",
    "        # Input projections\n",
    "        x_dict = {node_type: self.input_projections[node_type](x) \n",
    "                 for node_type, x in x_dict.items()}\n",
    "        \n",
    "        # Graph convolution layers\n",
    "        for conv in self.convs:\n",
    "            x_dict_new = conv(x_dict, edge_index_dict)\n",
    "            \n",
    "            # Apply residual connections and activation\n",
    "            for node_type in x_dict.keys():\n",
    "                if node_type in x_dict_new:\n",
    "                    x_dict[node_type] = F.relu(x_dict_new[node_type] + x_dict[node_type])\n",
    "        \n",
    "        # Get transaction node embeddings\n",
    "        transaction_embeddings = x_dict['transaction']\n",
    "        \n",
    "        # Classification\n",
    "        fraud_predictions = self.transaction_classifier(transaction_embeddings)\n",
    "        \n",
    "        return fraud_predictions\n",
    "    \n",
    "    def get_embeddings(self, x_dict, edge_index_dict):\n",
    "        \"\"\"\n",
    "        Get node embeddings for visualization and analysis.\n",
    "        \"\"\"\n",
    "        # Input projections\n",
    "        x_dict = {node_type: self.input_projections[node_type](x) \n",
    "                 for node_type, x in x_dict.items()}\n",
    "        \n",
    "        # Graph convolution layers\n",
    "        for conv in self.convs:\n",
    "            x_dict_new = conv(x_dict, edge_index_dict)\n",
    "            \n",
    "            # Apply residual connections and activation\n",
    "            for node_type in x_dict.keys():\n",
    "                if node_type in x_dict_new:\n",
    "                    x_dict[node_type] = F.relu(x_dict_new[node_type] + x_dict[node_type])\n",
    "        \n",
    "        return x_dict\n",
    "\n",
    "# Visualize attention mechanism concept\n",
    "def visualize_attention_concept():\n",
    "    \"\"\"\n",
    "    Visualize how attention works in graph neural networks.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # 1. Traditional aggregation\n",
    "    axes[0].text(0.5, 0.9, 'Traditional Aggregation', ha='center', fontsize=14, fontweight='bold')\n",
    "    axes[0].text(0.5, 0.7, 'All neighbors\\nequal weight', ha='center', fontsize=12)\n",
    "    axes[0].text(0.5, 0.5, 'h\\'ᵢ = Σ Wh_j', ha='center', fontsize=12, family='monospace')\n",
    "    axes[0].text(0.5, 0.3, 'Simple but limited', ha='center', fontsize=10, style='italic')\n",
    "    axes[0].set_xlim(0, 1)\n",
    "    axes[0].set_ylim(0, 1)\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # 2. Attention mechanism\n",
    "    axes[1].text(0.5, 0.9, 'Attention Mechanism', ha='center', fontsize=14, fontweight='bold')\n",
    "    axes[1].text(0.5, 0.7, 'Learned weights\\nfor each neighbor', ha='center', fontsize=12)\n",
    "    axes[1].text(0.5, 0.5, 'h\\'ᵢ = Σ αᵢⱼ Wh_j', ha='center', fontsize=12, family='monospace')\n",
    "    axes[1].text(0.5, 0.3, 'Adaptive and flexible', ha='center', fontsize=10, style='italic')\n",
    "    axes[1].set_xlim(0, 1)\n",
    "    axes[1].set_ylim(0, 1)\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # 3. Multi-head attention\n",
    "    axes[2].text(0.5, 0.9, 'Multi-Head Attention', ha='center', fontsize=14, fontweight='bold')\n",
    "    axes[2].text(0.5, 0.7, 'Multiple attention\\nsubspaces', ha='center', fontsize=12)\n",
    "    axes[2].text(0.5, 0.5, 'h\\'ᵢ = || H attention heads', ha='center', fontsize=12, family='monospace')\n",
    "    axes[2].text(0.5, 0.3, 'Captures different aspects', ha='center', fontsize=10, style='italic')\n",
    "    axes[2].set_xlim(0, 1)\n",
    "    axes[2].set_ylim(0, 1)\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Key Benefits of Attention:\")\n",
    "    print(\"1. Focus on important connections\")\n",
    "    print(\"2. Adaptive to data patterns\")\n",
    "    print(\"3. Interpretable attention weights\")\n",
    "    print(\"4. Better performance on complex graphs\")\n",
    "\n",
    "visualize_attention_concept()\n",
    "\n",
    "# Initialize the model\n",
    "node_types = {\n",
    "    'transaction': graph_data['transaction'].x.shape[1],\n",
    "    'user': graph_data['user'].x.shape[1],\n",
    "    'merchant': graph_data['merchant'].x.shape[1]\n",
    "}\n",
    "\n",
    "edge_types = graph_data.edge_types\n",
    "\n",
    "model = HeterogeneousGAT(\n",
    "    node_types=node_types,\n",
    "    edge_types=edge_types,\n",
    "    hidden_dim=64,\n",
    "    num_heads=4,\n",
    "    num_layers=2,\n",
    "    dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "print(f\"\\nModel initialized with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "print(f\"Hidden dimension: {model.hidden_dim}\")\n",
    "print(f\"Number of attention heads: {model.num_heads}\")\n",
    "print(f\"Number of layers: {model.num_layers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Training the Heterogeneous GNN\n",
    "\n",
    "### Training Strategy\n",
    "\n",
    "Training heterogeneous GNNs requires careful consideration of:\n",
    "- **Class imbalance**: Fraud is rare, need weighted loss\n",
    "- **Graph structure**: Different edge types contribute differently\n",
    "- **Scalability**: Large graphs require efficient training\n",
    "- **Overfitting**: Complex models need regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeterogeneousFraudDetector:\n",
    "    \"\"\"\n",
    "    Complete fraud detection system using heterogeneous GNNs.\n",
    "    \n",
    "    Features:\n",
    "    - Automatic graph construction\n",
    "    - Training with class imbalance handling\n",
    "    - Comprehensive evaluation\n",
    "    - Embedding visualization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, random_state=42):\n",
    "        self.random_state = random_state\n",
    "        self.graph_builder = HeterogeneousGraphBuilder(random_state)\n",
    "        self.model = None\n",
    "        self.graph_data = None\n",
    "        self.df_with_entities = None\n",
    "        self.train_mask = None\n",
    "        self.test_mask = None\n",
    "        self.device = device\n",
    "    \n",
    "    def prepare_data(self, df, test_size=0.2):\n",
    "        \"\"\"\n",
    "        Prepare data for training: build graph and create splits.\n",
    "        \"\"\"\n",
    "        print(\"Preparing data for heterogeneous GNN...\")\n",
    "        \n",
    "        # Build graph\n",
    "        self.graph_data, self.df_with_entities = self.graph_builder.build_heterogeneous_graph(df)\n",
    "        \n",
    "        # Create train/test splits\n",
    "        n_transactions = len(df)\n",
    "        indices = np.arange(n_transactions)\n",
    "        labels = df['Class'].values\n",
    "        \n",
    "        train_indices, test_indices = train_test_split(\n",
    "            indices, test_size=test_size, stratify=labels, random_state=self.random_state\n",
    "        )\n",
    "        \n",
    "        # Create masks\n",
    "        self.train_mask = torch.zeros(n_transactions, dtype=torch.bool)\n",
    "        self.test_mask = torch.zeros(n_transactions, dtype=torch.bool)\n",
    "        \n",
    "        self.train_mask[train_indices] = True\n",
    "        self.test_mask[test_indices] = True\n",
    "        \n",
    "        # Move data to device\n",
    "        self.graph_data = self.graph_data.to(self.device)\n",
    "        self.train_mask = self.train_mask.to(self.device)\n",
    "        self.test_mask = self.test_mask.to(self.device)\n",
    "        \n",
    "        print(f\"Data prepared:\")\n",
    "        print(f\"  - Training transactions: {self.train_mask.sum():,}\")\n",
    "        print(f\"  - Test transactions: {self.test_mask.sum():,}\")\n",
    "        print(f\"  - Training fraud rate: {self.graph_data['transaction'].y[self.train_mask].float().mean():.4f}\")\n",
    "        print(f\"  - Test fraud rate: {self.graph_data['transaction'].y[self.test_mask].float().mean():.4f}\")\n",
    "    \n",
    "    def train_model(self, hidden_dim=64, num_heads=4, num_layers=2, epochs=200, lr=0.01):\n",
    "        \"\"\"\n",
    "        Train the heterogeneous GNN model.\n",
    "        \"\"\"\n",
    "        print(f\"Training heterogeneous GNN...\")\n",
    "        \n",
    "        # Initialize model\n",
    "        node_types = {\n",
    "            'transaction': self.graph_data['transaction'].x.shape[1],\n",
    "            'user': self.graph_data['user'].x.shape[1],\n",
    "            'merchant': self.graph_data['merchant'].x.shape[1]\n",
    "        }\n",
    "        \n",
    "        self.model = HeterogeneousGAT(\n",
    "            node_types=node_types,\n",
    "            edge_types=self.graph_data.edge_types,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_heads=num_heads,\n",
    "            num_layers=num_layers\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Calculate class weights for imbalanced data\n",
    "        train_labels = self.graph_data['transaction'].y[self.train_mask]\n",
    "        pos_weight = (train_labels == 0).sum().float() / (train_labels == 1).sum().float()\n",
    "        \n",
    "        # Loss function and optimizer\n",
    "        criterion = nn.CrossEntropyLoss(weight=torch.tensor([1.0, pos_weight]).to(self.device))\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n",
    "        \n",
    "        # Training loop\n",
    "        history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Training\n",
    "            self.model.train()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            out = self.model(self.graph_data.x_dict, self.graph_data.edge_index_dict)\n",
    "            train_out = out[self.train_mask]\n",
    "            train_labels = self.graph_data['transaction'].y[self.train_mask]\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(train_out, train_labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Calculate training accuracy\n",
    "            with torch.no_grad():\n",
    "                train_pred = train_out.argmax(dim=1)\n",
    "                train_acc = (train_pred == train_labels).float().mean()\n",
    "            \n",
    "            # Validation\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_out = out[self.test_mask]\n",
    "                val_labels = self.graph_data['transaction'].y[self.test_mask]\n",
    "                val_loss = criterion(val_out, val_labels)\n",
    "                val_pred = val_out.argmax(dim=1)\n",
    "                val_acc = (val_pred == val_labels).float().mean()\n",
    "            \n",
    "            # Update scheduler\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Save history\n",
    "            history['train_loss'].append(loss.item())\n",
    "            history['train_acc'].append(train_acc.item())\n",
    "            history['val_loss'].append(val_loss.item())\n",
    "            history['val_acc'].append(val_acc.item())\n",
    "            \n",
    "            # Print progress\n",
    "            if epoch % 25 == 0 or epoch == epochs - 1:\n",
    "                print(f\"Epoch {epoch:3d}: Train Loss={loss:.4f}, Train Acc={train_acc:.4f}, \"\n",
    "                      f\"Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}\")\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def evaluate_model(self):\n",
    "        \"\"\"\n",
    "        Comprehensive evaluation of the trained model.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained yet. Call train_model() first.\")\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Get predictions\n",
    "            out = self.model(self.graph_data.x_dict, self.graph_data.edge_index_dict)\n",
    "            test_out = out[self.test_mask]\n",
    "            test_labels = self.graph_data['transaction'].y[self.test_mask]\n",
    "            \n",
    "            # Get probabilities and predictions\n",
    "            probabilities = F.softmax(test_out, dim=1)[:, 1]  # Fraud probabilities\n",
    "            predictions = test_out.argmax(dim=1)\n",
    "            \n",
    "            # Convert to numpy for sklearn metrics\n",
    "            y_true = test_labels.cpu().numpy()\n",
    "            y_pred = predictions.cpu().numpy()\n",
    "            y_prob = probabilities.cpu().numpy()\n",
    "            \n",
    "            # Calculate metrics\n",
    "            metrics = {\n",
    "                'accuracy': accuracy_score(y_true, y_pred),\n",
    "                'precision': precision_score(y_true, y_pred),\n",
    "                'recall': recall_score(y_true, y_pred),\n",
    "                'f1_score': f1_score(y_true, y_pred),\n",
    "                'roc_auc': roc_auc_score(y_true, y_prob)\n",
    "            }\n",
    "            \n",
    "            # Print results\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"HETEROGENEOUS GNN EVALUATION RESULTS\")\n",
    "            print(\"=\"*50)\n",
    "            print(f\"Accuracy:  {metrics['accuracy']:.4f}\")\n",
    "            print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "            print(f\"Recall:    {metrics['recall']:.4f}\")\n",
    "            print(f\"F1-Score:  {metrics['f1_score']:.4f}\")\n",
    "            print(f\"ROC-AUC:   {metrics['roc_auc']:.4f}\")\n",
    "            print(\"=\"*50)\n",
    "            \n",
    "            # Confusion matrix\n",
    "            cm = confusion_matrix(y_true, y_pred)\n",
    "            print(f\"\\nConfusion Matrix:\")\n",
    "            print(f\"                Predicted\")\n",
    "            print(f\"Actual   Normal  Fraud\")\n",
    "            print(f\"Normal   {cm[0,0]:6d}  {cm[0,1]:5d}\")\n",
    "            print(f\"Fraud    {cm[1,0]:6d}  {cm[1,1]:5d}\")\n",
    "            \n",
    "            # Classification report\n",
    "            print(f\"\\nDetailed Classification Report:\")\n",
    "            print(classification_report(y_true, y_pred, target_names=['Normal', 'Fraud']))\n",
    "            \n",
    "            return metrics, y_true, y_pred, y_prob\n",
    "\n",
    "# Initialize and train the detector\n",
    "detector = HeterogeneousFraudDetector()\n",
    "detector.prepare_data(df.iloc[:50000])  # Use subset for faster training\n",
    "\n",
    "# Train the model\n",
    "training_history = detector.train_model(epochs=100, lr=0.01)\n",
    "\n",
    "# Evaluate the model\n",
    "metrics, y_true, y_pred, y_prob = detector.evaluate_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Advanced Analysis and Visualization\n",
    "\n",
    "### Embedding Analysis\n",
    "\n",
    "Let's analyze what the model learned by examining the node embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_embeddings(detector):\n",
    "    \"\"\"\n",
    "    Analyze and visualize learned embeddings.\n",
    "    \"\"\"\n",
    "    print(\"Analyzing learned embeddings...\")\n",
    "    \n",
    "    # Get embeddings\n",
    "    detector.model.eval()\n",
    "    with torch.no_grad():\n",
    "        embeddings = detector.model.get_embeddings(\n",
    "            detector.graph_data.x_dict, \n",
    "            detector.graph_data.edge_index_dict\n",
    "        )\n",
    "    \n",
    "    # Extract transaction embeddings\n",
    "    transaction_embeddings = embeddings['transaction'].cpu().numpy()\n",
    "    transaction_labels = detector.graph_data['transaction'].y.cpu().numpy()\n",
    "    \n",
    "    # Dimensionality reduction\n",
    "    print(\"Performing dimensionality reduction...\")\n",
    "    pca = PCA(n_components=2)\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    \n",
    "    # Use subset for t-SNE (computationally expensive)\n",
    "    subset_indices = np.random.choice(len(transaction_embeddings), 2000, replace=False)\n",
    "    subset_embeddings = transaction_embeddings[subset_indices]\n",
    "    subset_labels = transaction_labels[subset_indices]\n",
    "    \n",
    "    embeddings_pca = pca.fit_transform(transaction_embeddings)\n",
    "    embeddings_tsne = tsne.fit_transform(subset_embeddings)\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # PCA visualization\n",
    "    normal_mask = transaction_labels == 0\n",
    "    fraud_mask = transaction_labels == 1\n",
    "    \n",
    "    axes[0, 0].scatter(embeddings_pca[normal_mask, 0], embeddings_pca[normal_mask, 1], \n",
    "                      alpha=0.6, s=10, label='Normal', color='blue')\n",
    "    axes[0, 0].scatter(embeddings_pca[fraud_mask, 0], embeddings_pca[fraud_mask, 1], \n",
    "                      alpha=0.8, s=30, label='Fraud', color='red')\n",
    "    axes[0, 0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2f})')\n",
    "    axes[0, 0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2f})')\n",
    "    axes[0, 0].set_title('Transaction Embeddings (PCA)')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # t-SNE visualization\n",
    "    subset_normal_mask = subset_labels == 0\n",
    "    subset_fraud_mask = subset_labels == 1\n",
    "    \n",
    "    axes[0, 1].scatter(embeddings_tsne[subset_normal_mask, 0], embeddings_tsne[subset_normal_mask, 1], \n",
    "                      alpha=0.6, s=10, label='Normal', color='blue')\n",
    "    axes[0, 1].scatter(embeddings_tsne[subset_fraud_mask, 0], embeddings_tsne[subset_fraud_mask, 1], \n",
    "                      alpha=0.8, s=30, label='Fraud', color='red')\n",
    "    axes[0, 1].set_xlabel('t-SNE 1')\n",
    "    axes[0, 1].set_ylabel('t-SNE 2')\n",
    "    axes[0, 1].set_title('Transaction Embeddings (t-SNE)')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # User embeddings\n",
    "    user_embeddings = embeddings['user'].cpu().numpy()\n",
    "    user_pca = PCA(n_components=2).fit_transform(user_embeddings)\n",
    "    \n",
    "    axes[1, 0].scatter(user_pca[:, 0], user_pca[:, 1], alpha=0.7, s=50, color='green')\n",
    "    axes[1, 0].set_xlabel('PC1')\n",
    "    axes[1, 0].set_ylabel('PC2')\n",
    "    axes[1, 0].set_title('User Embeddings (PCA)')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Merchant embeddings\n",
    "    merchant_embeddings = embeddings['merchant'].cpu().numpy()\n",
    "    merchant_pca = PCA(n_components=2).fit_transform(merchant_embeddings)\n",
    "    \n",
    "    axes[1, 1].scatter(merchant_pca[:, 0], merchant_pca[:, 1], alpha=0.7, s=50, color='orange')\n",
    "    axes[1, 1].set_xlabel('PC1')\n",
    "    axes[1, 1].set_ylabel('PC2')\n",
    "    axes[1, 1].set_title('Merchant Embeddings (PCA)')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Embedding statistics\n",
    "    print(\"\\nEmbedding Analysis:\")\n",
    "    print(f\"Transaction embeddings shape: {transaction_embeddings.shape}\")\n",
    "    print(f\"User embeddings shape: {user_embeddings.shape}\")\n",
    "    print(f\"Merchant embeddings shape: {merchant_embeddings.shape}\")\n",
    "    \n",
    "    # Silhouette analysis\n",
    "    from sklearn.metrics import silhouette_score\n",
    "    silhouette_pca = silhouette_score(embeddings_pca, transaction_labels)\n",
    "    silhouette_tsne = silhouette_score(embeddings_tsne, subset_labels)\n",
    "    \n",
    "    print(f\"\\nSeparation Quality:\")\n",
    "    print(f\"PCA silhouette score: {silhouette_pca:.4f}\")\n",
    "    print(f\"t-SNE silhouette score: {silhouette_tsne:.4f}\")\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# Analyze embeddings\n",
    "learned_embeddings = analyze_embeddings(detector)\n",
    "\n",
    "# Visualize training history\n",
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Plot training history.\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    epochs = range(len(history['train_loss']))\n",
    "    \n",
    "    # Loss curves\n",
    "    ax1.plot(epochs, history['train_loss'], 'b-', label='Training Loss', linewidth=2)\n",
    "    ax1.plot(epochs, history['val_loss'], 'r-', label='Validation Loss', linewidth=2)\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy curves\n",
    "    ax2.plot(epochs, history['train_acc'], 'b-', label='Training Accuracy', linewidth=2)\n",
    "    ax2.plot(epochs, history['val_acc'], 'r-', label='Validation Accuracy', linewidth=2)\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_title('Training and Validation Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(training_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Performance Comparison and Analysis\n",
    "\n",
    "Let's compare the heterogeneous GNN with simpler baselines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_with_baselines(detector, y_true, y_pred, y_prob):\n",
    "    \"\"\"\n",
    "    Compare heterogeneous GNN with traditional ML baselines.\n",
    "    \"\"\"\n",
    "    print(\"Comparing with traditional ML baselines...\")\n",
    "    \n",
    "    # Prepare data for traditional ML\n",
    "    X_test = detector.graph_data['transaction'].x[detector.test_mask].cpu().numpy()\n",
    "    y_test = y_true\n",
    "    \n",
    "    # Get training data\n",
    "    X_train = detector.graph_data['transaction'].x[detector.train_mask].cpu().numpy()\n",
    "    y_train = detector.graph_data['transaction'].y[detector.train_mask].cpu().numpy()\n",
    "    \n",
    "    # Train baseline models\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.svm import SVC\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    baselines = {\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'),\n",
    "        'Logistic Regression': LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000),\n",
    "        'SVM': SVC(random_state=42, class_weight='balanced', probability=True)\n",
    "    }\n",
    "    \n",
    "    baseline_results = {}\n",
    "    \n",
    "    for name, model in baselines.items():\n",
    "        print(f\"Training {name}...\")\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred_baseline = model.predict(X_test_scaled)\n",
    "        y_prob_baseline = model.predict_proba(X_test_scaled)[:, 1]\n",
    "        \n",
    "        # Metrics\n",
    "        baseline_results[name] = {\n",
    "            'accuracy': accuracy_score(y_test, y_pred_baseline),\n",
    "            'precision': precision_score(y_test, y_pred_baseline),\n",
    "            'recall': recall_score(y_test, y_pred_baseline),\n",
    "            'f1_score': f1_score(y_test, y_pred_baseline),\n",
    "            'roc_auc': roc_auc_score(y_test, y_prob_baseline)\n",
    "        }\n",
    "    \n",
    "    # Add heterogeneous GNN results\n",
    "    baseline_results['Heterogeneous GNN'] = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred),\n",
    "        'recall': recall_score(y_true, y_pred),\n",
    "        'f1_score': f1_score(y_true, y_pred),\n",
    "        'roc_auc': roc_auc_score(y_true, y_prob)\n",
    "    }\n",
    "    \n",
    "    # Create comparison table\n",
    "    comparison_df = pd.DataFrame(baseline_results).T\n",
    "    comparison_df = comparison_df.round(4)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MODEL COMPARISON RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(comparison_df.to_string())\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1_score']\n",
    "    titles = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "    \n",
    "    for i, (metric, title) in enumerate(zip(metrics, titles)):\n",
    "        ax = axes[i//2, i%2]\n",
    "        \n",
    "        models = list(baseline_results.keys())\n",
    "        values = [baseline_results[model][metric] for model in models]\n",
    "        \n",
    "        colors = ['lightblue', 'lightgreen', 'lightcoral', 'gold']\n",
    "        bars = ax.bar(models, values, color=colors)\n",
    "        \n",
    "        # Highlight best performer\n",
    "        best_idx = np.argmax(values)\n",
    "        bars[best_idx].set_color('darkgreen')\n",
    "        \n",
    "        ax.set_ylabel(title)\n",
    "        ax.set_title(f'{title} Comparison')\n",
    "        ax.set_ylim(0, 1)\n",
    "        \n",
    "        # Add value labels\n",
    "        for j, (model, value) in enumerate(zip(models, values)):\n",
    "            ax.text(j, value + 0.01, f'{value:.3f}', ha='center', va='bottom')\n",
    "        \n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Rotate x-axis labels for better readability\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "# Compare with baselines\n",
    "comparison_results = compare_with_baselines(detector, y_true, y_pred, y_prob)\n",
    "\n",
    "# ROC curve comparison\n",
    "def plot_roc_comparison(detector, y_true, y_prob):\n",
    "    \"\"\"\n",
    "    Plot ROC curves for different models.\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Heterogeneous GNN\n",
    "    fpr_gnn, tpr_gnn, _ = roc_curve(y_true, y_prob)\n",
    "    roc_auc_gnn = auc(fpr_gnn, tpr_gnn)\n",
    "    \n",
    "    plt.plot(fpr_gnn, tpr_gnn, color='red', lw=2, \n",
    "             label=f'Heterogeneous GNN (AUC = {roc_auc_gnn:.3f})')\n",
    "    \n",
    "    # Baseline comparison (Random Forest)\n",
    "    X_test = detector.graph_data['transaction'].x[detector.test_mask].cpu().numpy()\n",
    "    X_train = detector.graph_data['transaction'].x[detector.train_mask].cpu().numpy()\n",
    "    y_train = detector.graph_data['transaction'].y[detector.train_mask].cpu().numpy()\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "    rf.fit(X_train_scaled, y_train)\n",
    "    y_prob_rf = rf.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    fpr_rf, tpr_rf, _ = roc_curve(y_true, y_prob_rf)\n",
    "    roc_auc_rf = auc(fpr_rf, tpr_rf)\n",
    "    \n",
    "    plt.plot(fpr_rf, tpr_rf, color='blue', lw=2, \n",
    "             label=f'Random Forest (AUC = {roc_auc_rf:.3f})')\n",
    "    \n",
    "    # Random classifier\n",
    "    plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--', \n",
    "             label='Random Classifier (AUC = 0.500)')\n",
    "    \n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve Comparison')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nROC-AUC Improvement: {roc_auc_gnn - roc_auc_rf:.4f}\")\n",
    "    print(f\"Relative improvement: {((roc_auc_gnn - roc_auc_rf) / roc_auc_rf * 100):.1f}%\")\n",
    "\n",
    "plot_roc_comparison(detector, y_true, y_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises\n",
    "\n",
    "Now it's your turn! Try these exercises to deepen your understanding:\n",
    "\n",
    "### Exercise 1: Graph Architecture Experiments\n",
    "Modify the heterogeneous GNN architecture:\n",
    "- Try different numbers of attention heads (2, 4, 8)\n",
    "- Experiment with different hidden dimensions (32, 64, 128)\n",
    "- Add more GNN layers (3, 4, 5)\n",
    "\n",
    "How do these changes affect performance and training time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Create multiple models with different architectures\n",
    "# Compare their performance on the same dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Edge Type Ablation Study\n",
    "Study the importance of different edge types:\n",
    "- Remove similarity edges (user-user, merchant-merchant)\n",
    "- Remove temporal edges (transaction-transaction)\n",
    "- Train with only user-transaction and merchant-transaction edges\n",
    "\n",
    "Which edge types are most important for fraud detection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Modify the graph construction to exclude certain edge types\n",
    "# Compare performance with and without each edge type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Attention Weight Analysis\n",
    "Analyze the learned attention weights:\n",
    "- Extract attention weights from the GAT layers\n",
    "- Visualize which relationships the model focuses on\n",
    "- Compare attention patterns for fraud vs normal transactions\n",
    "\n",
    "What patterns does the model learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Modify the forward pass to return attention weights\n",
    "# Analyze the weights for different transaction types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### 1. Graph-Based Fraud Detection\n",
    "- **Relationships Matter**: Fraud detection benefits from understanding connections between entities\n",
    "- **Heterogeneous Graphs**: Multiple node types provide richer representation\n",
    "- **Message Passing**: Information flows through graph structure enhances predictions\n",
    "\n",
    "### 2. Graph Construction Strategies\n",
    "- **Synthetic Entities**: Use clustering to create realistic user/merchant entities\n",
    "- **Multiple Edge Types**: Different relationships capture different aspects of fraud\n",
    "- **Temporal Connections**: Time-based edges capture sequential patterns\n",
    "\n",
    "### 3. Graph Attention Networks\n",
    "- **Attention Mechanism**: Focuses on important connections dynamically\n",
    "- **Multi-head Attention**: Captures different aspects of relationships\n",
    "- **Heterogeneous Message Passing**: Different attention for different edge types\n",
    "\n",
    "### 4. Training Considerations\n",
    "- **Class Imbalance**: Weighted loss functions handle rare fraud cases\n",
    "- **Graph Structure**: Complex graphs require careful regularization\n",
    "- **Scalability**: Large graphs need efficient training strategies\n",
    "\n",
    "### 5. Evaluation and Analysis\n",
    "- **Embedding Visualization**: t-SNE and PCA show learned representations\n",
    "- **Comparative Analysis**: Compare with traditional ML baselines\n",
    "- **Attention Analysis**: Understand what the model learned\n",
    "\n",
    "### 6. Practical Applications\n",
    "- **Financial Networks**: Credit cards, bank transfers, insurance claims\n",
    "- **Social Networks**: Fake accounts, spam detection, recommendation systems\n",
    "- **E-commerce**: Fake reviews, seller verification, transaction monitoring\n",
    "\n",
    "### 7. Production Considerations\n",
    "- **Real-time Inference**: Optimize for low-latency predictions\n",
    "- **Graph Updates**: Handle dynamic graphs with new entities\n",
    "- **Scalability**: Use graph sampling for large-scale deployment\n",
    "- **Interpretability**: Provide explanations for fraud predictions\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next tutorial, we'll explore:\n",
    "- Online learning and streaming fraud detection\n",
    "- Handling concept drift in fraud patterns\n",
    "- Real-time model updates and deployment\n",
    "- Scalable architectures for production systems\n",
    "\n",
    "Remember: Graph neural networks are powerful tools for fraud detection, but they require careful design and understanding of the underlying relationships in your data!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}