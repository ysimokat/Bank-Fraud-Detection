{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced Fraud Detection API with Model Versioning and CI/CD\n",
    "\n",
    "## Tutorial: Building Production-Ready ML APIs with Enterprise Features\n",
    "\n",
    "In this comprehensive tutorial, we'll explore building a production-grade fraud detection API that implements:\n",
    "- Model versioning and management\n",
    "- Request logging to database and S3\n",
    "- CI/CD deployment hooks\n",
    "- Advanced monitoring and observability\n",
    "- Auto-rollback capabilities\n",
    "- Batch prediction endpoints\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will:\n",
    "\n",
    "1. **Understand production ML API requirements**\n",
    "2. **Implement model versioning systems**\n",
    "3. **Build comprehensive request logging**\n",
    "4. **Create CI/CD integration endpoints**\n",
    "5. **Design monitoring and observability features**\n",
    "6. **Implement batch processing for efficiency**\n",
    "7. **Build auto-rollback mechanisms**\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- Completion of previous fraud detection tutorials\n",
    "- Understanding of FastAPI basics\n",
    "- Familiarity with REST API concepts\n",
    "- Basic knowledge of CI/CD principles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting Up the Enhanced API Environment\n",
    "\n",
    "Let's start by understanding the key components and installing necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install fastapi uvicorn pydantic boto3 aiofiles\n",
    "\n",
    "# Core imports\n",
    "from fastapi import FastAPI, HTTPException, Request, BackgroundTasks\n",
    "from pydantic import BaseModel, Field\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import logging\n",
    "import json\n",
    "import sqlite3\n",
    "import hashlib\n",
    "import uuid\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import time\n",
    "from typing import List, Dict, Optional, Any\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"✅ Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building the Request/Response Models\n",
    "\n",
    "Pydantic models provide automatic validation and documentation for our API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request model for fraud prediction\n",
    "class TransactionRequest(BaseModel):\n",
    "    \"\"\"Request model for fraud prediction with comprehensive validation.\"\"\"\n",
    "    transaction_id: str = Field(..., description=\"Unique transaction identifier\")\n",
    "    amount: float = Field(..., ge=0, description=\"Transaction amount\")\n",
    "    time: float = Field(..., ge=0, description=\"Time from first transaction\")\n",
    "    features: Dict[str, float] = Field(..., description=\"V1-V28 features\")\n",
    "    merchant_category: Optional[str] = Field(None, description=\"Merchant category\")\n",
    "    location: Optional[str] = Field(None, description=\"Transaction location\")\n",
    "    \n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"transaction_id\": \"txn_12345\",\n",
    "                \"amount\": 100.50,\n",
    "                \"time\": 3600.0,\n",
    "                \"features\": {\n",
    "                    \"V1\": -1.359807134,\n",
    "                    \"V2\": -0.072781173,\n",
    "                    \"V3\": 2.536346738,\n",
    "                    \"V4\": 1.378155224\n",
    "                },\n",
    "                \"merchant_category\": \"grocery\",\n",
    "                \"location\": \"US\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Response model with comprehensive information\n",
    "class PredictionResponse(BaseModel):\n",
    "    \"\"\"Response model for fraud prediction with explanations.\"\"\"\n",
    "    transaction_id: str\n",
    "    is_fraud: bool\n",
    "    fraud_probability: float\n",
    "    risk_score: float\n",
    "    model_version: str\n",
    "    processing_time_ms: float\n",
    "    explanation: Optional[Dict[str, Any]] = None\n",
    "\n",
    "# Model version information\n",
    "class ModelVersion(BaseModel):\n",
    "    \"\"\"Model version metadata.\"\"\"\n",
    "    version: str\n",
    "    model_hash: str\n",
    "    created_at: datetime\n",
    "    performance_metrics: Dict[str, float]\n",
    "    is_active: bool\n",
    "    description: str\n",
    "\n",
    "print(\"📝 Request/Response models defined!\")\n",
    "\n",
    "# Example usage\n",
    "example_request = TransactionRequest(\n",
    "    transaction_id=\"test_123\",\n",
    "    amount=150.0,\n",
    "    time=1000.0,\n",
    "    features={f\"V{i}\": np.random.normal() for i in range(1, 29)}\n",
    ")\n",
    "print(f\"\\nExample request: {example_request.transaction_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementing Model Version Management\n",
    "\n",
    "Model versioning is crucial for production ML systems. Let's build a comprehensive version manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelVersionManager:\n",
    "    \"\"\"\n",
    "    Manages model versions with automatic rollback and deployment tracking.\n",
    "    \n",
    "    Key features:\n",
    "    - Version tracking with metadata\n",
    "    - Model hash verification\n",
    "    - Automatic rollback capabilities\n",
    "    - Performance-based activation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_path: str = \"model_versions\"):\n",
    "        self.base_path = Path(base_path)\n",
    "        self.base_path.mkdir(exist_ok=True)\n",
    "        self.versions_file = self.base_path / \"versions.json\"\n",
    "        self.active_version = None\n",
    "        self.models = {}\n",
    "        \n",
    "        self._load_versions()\n",
    "    \n",
    "    def _load_versions(self):\n",
    "        \"\"\"Load version metadata from disk.\"\"\"\n",
    "        if self.versions_file.exists():\n",
    "            with open(self.versions_file, 'r') as f:\n",
    "                self.versions = json.load(f)\n",
    "        else:\n",
    "            self.versions = {}\n",
    "    \n",
    "    def _calculate_model_hash(self, model_path: Path) -> str:\n",
    "        \"\"\"Calculate MD5 hash of model file for integrity.\"\"\"\n",
    "        hash_md5 = hashlib.md5()\n",
    "        with open(model_path, \"rb\") as f:\n",
    "            for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "                hash_md5.update(chunk)\n",
    "        return hash_md5.hexdigest()\n",
    "    \n",
    "    def deploy_model(self, model_data: Dict, version: str, \n",
    "                    description: str = \"\",\n",
    "                    performance_metrics: Dict[str, float] = None) -> bool:\n",
    "        \"\"\"\n",
    "        Deploy a new model version.\n",
    "        \n",
    "        Args:\n",
    "            model_data: Dictionary containing models, scaler, etc.\n",
    "            version: Version string (e.g., \"v1.0.0\")\n",
    "            description: Human-readable description\n",
    "            performance_metrics: Model performance metrics\n",
    "        \n",
    "        Returns:\n",
    "            Success status\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\"📦 Deploying model version {version}\")\n",
    "            \n",
    "            # Save model files\n",
    "            version_path = self.base_path / version\n",
    "            version_path.mkdir(exist_ok=True)\n",
    "            \n",
    "            model_file = version_path / \"model.joblib\"\n",
    "            joblib.dump(model_data, model_file)\n",
    "            \n",
    "            # Calculate hash for integrity\n",
    "            model_hash = self._calculate_model_hash(model_file)\n",
    "            \n",
    "            # Update version metadata\n",
    "            self.versions[version] = {\n",
    "                \"version\": version,\n",
    "                \"model_hash\": model_hash,\n",
    "                \"created_at\": datetime.now().isoformat(),\n",
    "                \"performance_metrics\": performance_metrics or {},\n",
    "                \"is_active\": False,\n",
    "                \"description\": description,\n",
    "                \"model_file\": str(model_file)\n",
    "            }\n",
    "            \n",
    "            self._save_versions()\n",
    "            logger.info(f\"✅ Model version {version} deployed successfully\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Failed to deploy model version {version}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def activate_version(self, version: str) -> bool:\n",
    "        \"\"\"\n",
    "        Activate a specific model version.\n",
    "        \n",
    "        This implements blue-green deployment pattern.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if version not in self.versions:\n",
    "                logger.error(f\"Version {version} not found\")\n",
    "                return False\n",
    "            \n",
    "            # Deactivate current version\n",
    "            if self.active_version:\n",
    "                self.versions[self.active_version][\"is_active\"] = False\n",
    "            \n",
    "            # Activate new version\n",
    "            self.versions[version][\"is_active\"] = True\n",
    "            self.active_version = version\n",
    "            \n",
    "            # Load model into memory\n",
    "            model_file = self.versions[version][\"model_file\"]\n",
    "            self.models[version] = joblib.load(model_file)\n",
    "            \n",
    "            self._save_versions()\n",
    "            logger.info(f\"🚀 Activated model version {version}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to activate version {version}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def rollback_to_previous(self) -> bool:\n",
    "        \"\"\"\n",
    "        Rollback to the previous stable version.\n",
    "        \n",
    "        Critical for production stability.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            versions = self.list_versions()\n",
    "            if len(versions) < 2:\n",
    "                logger.warning(\"No previous version available for rollback\")\n",
    "                return False\n",
    "            \n",
    "            # Find previous version\n",
    "            previous_version = None\n",
    "            for version in versions[1:]:\n",
    "                if not version.is_active:\n",
    "                    previous_version = version.version\n",
    "                    break\n",
    "            \n",
    "            if previous_version:\n",
    "                logger.info(f\"🔄 Rolling back to version {previous_version}\")\n",
    "                return self.activate_version(previous_version)\n",
    "            else:\n",
    "                logger.warning(\"No previous version found for rollback\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Rollback failed: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def _save_versions(self):\n",
    "        \"\"\"Save version metadata.\"\"\"\n",
    "        with open(self.versions_file, 'w') as f:\n",
    "            json.dump(self.versions, f, indent=2, default=str)\n",
    "    \n",
    "    def list_versions(self) -> List[ModelVersion]:\n",
    "        \"\"\"List all model versions.\"\"\"\n",
    "        versions = []\n",
    "        for version, metadata in self.versions.items():\n",
    "            versions.append(ModelVersion(\n",
    "                version=metadata[\"version\"],\n",
    "                model_hash=metadata[\"model_hash\"],\n",
    "                created_at=datetime.fromisoformat(metadata[\"created_at\"]),\n",
    "                performance_metrics=metadata[\"performance_metrics\"],\n",
    "                is_active=metadata[\"is_active\"],\n",
    "                description=metadata[\"description\"]\n",
    "            ))\n",
    "        return sorted(versions, key=lambda x: x.created_at, reverse=True)\n",
    "\n",
    "# Demo model version manager\n",
    "print(\"🔧 Model Version Manager implemented!\")\n",
    "\n",
    "# Create and test version manager\n",
    "version_manager = ModelVersionManager()\n",
    "\n",
    "# Deploy a test model\n",
    "test_model_data = {\n",
    "    \"models\": {\"test\": \"dummy_model\"},\n",
    "    \"scaler\": \"dummy_scaler\",\n",
    "    \"metadata\": {\"test\": True}\n",
    "}\n",
    "\n",
    "version_manager.deploy_model(\n",
    "    test_model_data,\n",
    "    \"v0.1.0-test\",\n",
    "    \"Test deployment for tutorial\",\n",
    "    {\"accuracy\": 0.95, \"f1_score\": 0.85}\n",
    ")\n",
    "\n",
    "print(\"\\nDeployed versions:\")\n",
    "for v in version_manager.list_versions():\n",
    "    print(f\"  - {v.version}: {v.description} (Active: {v.is_active})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implementing Comprehensive Request Logging\n",
    "\n",
    "Request logging is essential for debugging, auditing, and compliance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RequestLogger:\n",
    "    \"\"\"\n",
    "    Logs requests to database and optionally S3.\n",
    "    \n",
    "    Features:\n",
    "    - SQLite for local storage\n",
    "    - S3 for long-term archival\n",
    "    - Async logging for performance\n",
    "    - Comprehensive request/response capture\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, db_path: str = \"request_logs.db\"):\n",
    "        self.db_path = db_path\n",
    "        self._init_database()\n",
    "    \n",
    "    def _init_database(self):\n",
    "        \"\"\"Initialize SQLite database for request logging.\"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            conn.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS request_logs (\n",
    "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                    request_id TEXT UNIQUE NOT NULL,\n",
    "                    transaction_id TEXT,\n",
    "                    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,\n",
    "                    endpoint TEXT,\n",
    "                    method TEXT,\n",
    "                    model_version TEXT,\n",
    "                    request_data TEXT,\n",
    "                    response_data TEXT,\n",
    "                    processing_time_ms REAL,\n",
    "                    client_ip TEXT,\n",
    "                    user_agent TEXT,\n",
    "                    status_code INTEGER\n",
    "                )\n",
    "            \"\"\")\n",
    "            \n",
    "            # Create indexes for performance\n",
    "            conn.execute(\"\"\"\n",
    "                CREATE INDEX IF NOT EXISTS idx_timestamp \n",
    "                ON request_logs(timestamp)\n",
    "            \"\"\")\n",
    "            \n",
    "            conn.execute(\"\"\"\n",
    "                CREATE INDEX IF NOT EXISTS idx_transaction_id \n",
    "                ON request_logs(transaction_id)\n",
    "            \"\"\")\n",
    "            \n",
    "            conn.execute(\"\"\"\n",
    "                CREATE INDEX IF NOT EXISTS idx_status_code \n",
    "                ON request_logs(status_code)\n",
    "            \"\"\")\n",
    "    \n",
    "    def log_request(self, \n",
    "                   transaction_id: str,\n",
    "                   endpoint: str,\n",
    "                   method: str,\n",
    "                   model_version: str,\n",
    "                   request_data: Dict,\n",
    "                   response_data: Dict,\n",
    "                   processing_time_ms: float,\n",
    "                   status_code: int,\n",
    "                   client_ip: str = \"localhost\",\n",
    "                   user_agent: str = \"test\"):\n",
    "        \"\"\"\n",
    "        Log request to database.\n",
    "        \n",
    "        In production, this would be async and include S3 upload.\n",
    "        \"\"\"\n",
    "        request_id = str(uuid.uuid4())\n",
    "        timestamp = datetime.now()\n",
    "        \n",
    "        try:\n",
    "            with sqlite3.connect(self.db_path) as conn:\n",
    "                conn.execute(\"\"\"\n",
    "                    INSERT INTO request_logs \n",
    "                    (request_id, transaction_id, timestamp, endpoint, method, \n",
    "                     model_version, request_data, response_data, processing_time_ms,\n",
    "                     client_ip, user_agent, status_code)\n",
    "                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "                \"\"\", (\n",
    "                    request_id,\n",
    "                    transaction_id,\n",
    "                    timestamp,\n",
    "                    endpoint,\n",
    "                    method,\n",
    "                    model_version,\n",
    "                    json.dumps(request_data),\n",
    "                    json.dumps(response_data),\n",
    "                    processing_time_ms,\n",
    "                    client_ip,\n",
    "                    user_agent,\n",
    "                    status_code\n",
    "                ))\n",
    "                \n",
    "            logger.info(f\"📝 Logged request {request_id} for transaction {transaction_id}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to log request: {e}\")\n",
    "    \n",
    "    def get_request_logs(self, \n",
    "                        limit: int = 100, \n",
    "                        transaction_id: str = None,\n",
    "                        status_code: int = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Retrieve request logs with filtering.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with sqlite3.connect(self.db_path) as conn:\n",
    "                conn.row_factory = sqlite3.Row\n",
    "                \n",
    "                # Build query with filters\n",
    "                query = \"SELECT * FROM request_logs WHERE 1=1\"\n",
    "                params = []\n",
    "                \n",
    "                if transaction_id:\n",
    "                    query += \" AND transaction_id = ?\"\n",
    "                    params.append(transaction_id)\n",
    "                \n",
    "                if status_code:\n",
    "                    query += \" AND status_code = ?\"\n",
    "                    params.append(status_code)\n",
    "                \n",
    "                query += \" ORDER BY timestamp DESC LIMIT ?\"\n",
    "                params.append(limit)\n",
    "                \n",
    "                cursor = conn.execute(query, params)\n",
    "                return [dict(row) for row in cursor.fetchall()]\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to retrieve logs: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def get_metrics(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Get request metrics for monitoring.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with sqlite3.connect(self.db_path) as conn:\n",
    "                # Total requests\n",
    "                total = conn.execute(\n",
    "                    \"SELECT COUNT(*) FROM request_logs\"\n",
    "                ).fetchone()[0]\n",
    "                \n",
    "                # Success rate\n",
    "                successful = conn.execute(\n",
    "                    \"SELECT COUNT(*) FROM request_logs WHERE status_code = 200\"\n",
    "                ).fetchone()[0]\n",
    "                \n",
    "                # Average processing time\n",
    "                avg_time = conn.execute(\n",
    "                    \"SELECT AVG(processing_time_ms) FROM request_logs\"\n",
    "                ).fetchone()[0] or 0\n",
    "                \n",
    "                # Requests by endpoint\n",
    "                endpoint_stats = conn.execute(\"\"\"\n",
    "                    SELECT endpoint, COUNT(*) as count \n",
    "                    FROM request_logs \n",
    "                    GROUP BY endpoint\n",
    "                \"\"\").fetchall()\n",
    "                \n",
    "                return {\n",
    "                    \"total_requests\": total,\n",
    "                    \"successful_requests\": successful,\n",
    "                    \"success_rate\": successful / total if total > 0 else 0,\n",
    "                    \"average_processing_time_ms\": avg_time,\n",
    "                    \"requests_by_endpoint\": dict(endpoint_stats)\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to get metrics: {e}\")\n",
    "            return {}\n",
    "\n",
    "# Demo request logger\n",
    "print(\"📊 Request Logger implemented!\")\n",
    "\n",
    "request_logger = RequestLogger()\n",
    "\n",
    "# Log some test requests\n",
    "for i in range(5):\n",
    "    request_logger.log_request(\n",
    "        transaction_id=f\"test_{i}\",\n",
    "        endpoint=\"/api/v1/predict\",\n",
    "        method=\"POST\",\n",
    "        model_version=\"v0.1.0-test\",\n",
    "        request_data={\"amount\": 100 + i * 50},\n",
    "        response_data={\"is_fraud\": i % 2 == 0},\n",
    "        processing_time_ms=10 + i * 2,\n",
    "        status_code=200\n",
    "    )\n",
    "\n",
    "# Display metrics\n",
    "metrics = request_logger.get_metrics()\n",
    "print(\"\\n📈 Request Metrics:\")\n",
    "for key, value in metrics.items():\n",
    "    print(f\"  - {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Building the Enhanced Fraud Predictor\n",
    "\n",
    "Let's create a predictor that integrates with our version manager and provides explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FraudPredictor:\n",
    "    \"\"\"\n",
    "    Enhanced fraud predictor with model versioning and explanations.\n",
    "    \n",
    "    Features:\n",
    "    - Automatic model version selection\n",
    "    - Prediction explanations\n",
    "    - Risk scoring\n",
    "    - Performance tracking\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, version_manager: ModelVersionManager):\n",
    "        self.version_manager = version_manager\n",
    "        self.prediction_cache = {}  # Simple cache for demo\n",
    "    \n",
    "    def predict(self, transaction: TransactionRequest) -> Dict:\n",
    "        \"\"\"\n",
    "        Make fraud prediction for a single transaction.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Check cache first\n",
    "        cache_key = self._get_cache_key(transaction)\n",
    "        if cache_key in self.prediction_cache:\n",
    "            logger.info(f\"🎯 Cache hit for transaction {transaction.transaction_id}\")\n",
    "            cached_result = self.prediction_cache[cache_key].copy()\n",
    "            cached_result[\"cache_hit\"] = True\n",
    "            return cached_result\n",
    "        \n",
    "        # Get active model\n",
    "        model_data, version = self.version_manager.get_active_model()\n",
    "        \n",
    "        if not model_data:\n",
    "            # For demo, create dummy prediction\n",
    "            logger.warning(\"No active model, using dummy prediction\")\n",
    "            version = \"dummy-v1.0\"\n",
    "            prediction = 0\n",
    "            fraud_probability = 0.1\n",
    "        else:\n",
    "            # Prepare features\n",
    "            features = self._prepare_features(transaction)\n",
    "            \n",
    "            # In production, would use actual model\n",
    "            # For demo, simulate prediction\n",
    "            prediction = int(transaction.amount > 500)  # Simple rule\n",
    "            fraud_probability = min(0.9, transaction.amount / 1000)\n",
    "        \n",
    "        # Calculate risk score (0-100)\n",
    "        risk_score = self._calculate_risk_score(transaction, fraud_probability)\n",
    "        \n",
    "        # Generate explanation\n",
    "        explanation = self._generate_explanation(transaction, fraud_probability)\n",
    "        \n",
    "        processing_time = (time.time() - start_time) * 1000  # ms\n",
    "        \n",
    "        result = {\n",
    "            \"transaction_id\": transaction.transaction_id,\n",
    "            \"is_fraud\": bool(prediction),\n",
    "            \"fraud_probability\": float(fraud_probability),\n",
    "            \"risk_score\": float(risk_score),\n",
    "            \"model_version\": version,\n",
    "            \"processing_time_ms\": processing_time,\n",
    "            \"explanation\": explanation,\n",
    "            \"cache_hit\": False\n",
    "        }\n",
    "        \n",
    "        # Cache result\n",
    "        self.prediction_cache[cache_key] = result.copy()\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _prepare_features(self, transaction: TransactionRequest) -> List[float]:\n",
    "        \"\"\"Prepare feature vector from transaction.\"\"\"\n",
    "        features = [transaction.amount, transaction.time]\n",
    "        \n",
    "        # Add V features (V1-V28)\n",
    "        for i in range(1, 29):\n",
    "            v_key = f\"V{i}\"\n",
    "            features.append(transaction.features.get(v_key, 0.0))\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _calculate_risk_score(self, transaction: TransactionRequest, \n",
    "                             fraud_probability: float) -> float:\n",
    "        \"\"\"\n",
    "        Calculate comprehensive risk score.\n",
    "        \n",
    "        Considers:\n",
    "        - Fraud probability\n",
    "        - Transaction amount\n",
    "        - Feature anomalies\n",
    "        - Merchant category risk\n",
    "        \"\"\"\n",
    "        base_score = fraud_probability * 100\n",
    "        \n",
    "        # Amount-based adjustment\n",
    "        if transaction.amount > 1000:\n",
    "            base_score = min(100, base_score * 1.2)\n",
    "        elif transaction.amount < 10:\n",
    "            base_score = min(100, base_score * 1.1)\n",
    "        \n",
    "        # Feature anomaly adjustment\n",
    "        v_features = [transaction.features.get(f\"V{i}\", 0) for i in range(1, 5)]\n",
    "        anomaly_score = sum(1 for v in v_features if abs(v) > 3) * 5\n",
    "        base_score = min(100, base_score + anomaly_score)\n",
    "        \n",
    "        # Merchant category risk\n",
    "        high_risk_categories = ['online_gambling', 'cryptocurrency', 'wire_transfer']\n",
    "        if transaction.merchant_category in high_risk_categories:\n",
    "            base_score = min(100, base_score * 1.3)\n",
    "        \n",
    "        return base_score\n",
    "    \n",
    "    def _generate_explanation(self, transaction: TransactionRequest, \n",
    "                            fraud_probability: float) -> Dict:\n",
    "        \"\"\"\n",
    "        Generate human-readable explanation for the prediction.\n",
    "        \"\"\"\n",
    "        explanation = {\n",
    "            \"risk_factors\": [],\n",
    "            \"protective_factors\": [],\n",
    "            \"confidence\": \"high\" if abs(fraud_probability - 0.5) > 0.3 else \"medium\",\n",
    "            \"recommendation\": \"\"\n",
    "        }\n",
    "        \n",
    "        # Amount-based factors\n",
    "        if transaction.amount > 1000:\n",
    "            explanation[\"risk_factors\"].append({\n",
    "                \"factor\": \"High transaction amount\",\n",
    "                \"impact\": \"high\",\n",
    "                \"value\": f\"${transaction.amount:.2f}\"\n",
    "            })\n",
    "        elif transaction.amount < 10:\n",
    "            explanation[\"risk_factors\"].append({\n",
    "                \"factor\": \"Unusually low amount\",\n",
    "                \"impact\": \"medium\",\n",
    "                \"value\": f\"${transaction.amount:.2f}\"\n",
    "            })\n",
    "        else:\n",
    "            explanation[\"protective_factors\"].append({\n",
    "                \"factor\": \"Normal transaction amount\",\n",
    "                \"impact\": \"medium\",\n",
    "                \"value\": f\"${transaction.amount:.2f}\"\n",
    "            })\n",
    "        \n",
    "        # Feature anomalies\n",
    "        v_features = [transaction.features.get(f\"V{i}\", 0) for i in range(1, 5)]\n",
    "        anomalies = sum(1 for v in v_features if abs(v) > 3)\n",
    "        if anomalies > 0:\n",
    "            explanation[\"risk_factors\"].append({\n",
    "                \"factor\": \"Unusual transaction pattern\",\n",
    "                \"impact\": \"high\" if anomalies > 2 else \"medium\",\n",
    "                \"value\": f\"{anomalies} anomalous features\"\n",
    "            })\n",
    "        \n",
    "        # Recommendation\n",
    "        if fraud_probability > 0.8:\n",
    "            explanation[\"recommendation\"] = \"Block transaction and investigate\"\n",
    "        elif fraud_probability > 0.5:\n",
    "            explanation[\"recommendation\"] = \"Additional verification required\"\n",
    "        else:\n",
    "            explanation[\"recommendation\"] = \"Approve transaction\"\n",
    "        \n",
    "        return explanation\n",
    "    \n",
    "    def _get_cache_key(self, transaction: TransactionRequest) -> str:\n",
    "        \"\"\"Generate cache key for transaction.\"\"\"\n",
    "        # Simple hash of transaction data\n",
    "        data_str = f\"{transaction.amount}_{transaction.time}_{hash(str(transaction.features))}\"\n",
    "        return hashlib.md5(data_str.encode()).hexdigest()\n",
    "\n",
    "# Demo fraud predictor\n",
    "print(\"🔮 Enhanced Fraud Predictor implemented!\")\n",
    "\n",
    "fraud_predictor = FraudPredictor(version_manager)\n",
    "\n",
    "# Test predictions\n",
    "test_transactions = [\n",
    "    TransactionRequest(\n",
    "        transaction_id=\"low_risk_001\",\n",
    "        amount=50.0,\n",
    "        time=1000.0,\n",
    "        features={f\"V{i}\": np.random.normal(0, 1) for i in range(1, 29)}\n",
    "    ),\n",
    "    TransactionRequest(\n",
    "        transaction_id=\"high_risk_001\",\n",
    "        amount=2500.0,\n",
    "        time=5000.0,\n",
    "        features={f\"V{i}\": np.random.normal(0, 3) for i in range(1, 29)},\n",
    "        merchant_category=\"online_gambling\"\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"\\n🎯 Test Predictions:\")\n",
    "for txn in test_transactions:\n",
    "    result = fraud_predictor.predict(txn)\n",
    "    print(f\"\\n Transaction: {txn.transaction_id}\")\n",
    "    print(f\"  - Amount: ${txn.amount}\")\n",
    "    print(f\"  - Is Fraud: {result['is_fraud']}\")\n",
    "    print(f\"  - Risk Score: {result['risk_score']:.1f}/100\")\n",
    "    print(f\"  - Recommendation: {result['explanation']['recommendation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Creating the FastAPI Application\n",
    "\n",
    "Now let's build the complete API with all enterprise features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: In a real implementation, this would be in a separate file\n",
    "# Here we'll demonstrate the key API endpoints\n",
    "\n",
    "from typing import Callable\n",
    "import asyncio\n",
    "\n",
    "class MockFastAPI:\n",
    "    \"\"\"Mock FastAPI for demonstration purposes.\"\"\"\n",
    "    \n",
    "    def __init__(self, title: str, version: str):\n",
    "        self.title = title\n",
    "        self.version = version\n",
    "        self.routes = {}\n",
    "    \n",
    "    def post(self, path: str):\n",
    "        def decorator(func: Callable):\n",
    "            self.routes[f\"POST {path}\"] = func\n",
    "            return func\n",
    "        return decorator\n",
    "    \n",
    "    def get(self, path: str):\n",
    "        def decorator(func: Callable):\n",
    "            self.routes[f\"GET {path}\"] = func\n",
    "            return func\n",
    "        return decorator\n",
    "\n",
    "# Create API instance\n",
    "app = MockFastAPI(\n",
    "    title=\"Enhanced Fraud Detection API\",\n",
    "    version=\"2.0.0\"\n",
    ")\n",
    "\n",
    "# API Endpoints\n",
    "\n",
    "@app.post(\"/api/v1/predict\")\n",
    "async def predict_fraud(transaction: TransactionRequest):\n",
    "    \"\"\"\n",
    "    Predict fraud for a single transaction.\n",
    "    \n",
    "    This endpoint:\n",
    "    - Validates input\n",
    "    - Makes prediction\n",
    "    - Logs request\n",
    "    - Returns comprehensive response\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Make prediction\n",
    "        result = fraud_predictor.predict(transaction)\n",
    "        \n",
    "        # Log request (in production, this would be async)\n",
    "        request_logger.log_request(\n",
    "            transaction_id=transaction.transaction_id,\n",
    "            endpoint=\"/api/v1/predict\",\n",
    "            method=\"POST\",\n",
    "            model_version=result[\"model_version\"],\n",
    "            request_data=transaction.dict(),\n",
    "            response_data=result,\n",
    "            processing_time_ms=result[\"processing_time_ms\"],\n",
    "            status_code=200\n",
    "        )\n",
    "        \n",
    "        return PredictionResponse(**result)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Prediction failed: {e}\")\n",
    "        processing_time = (time.time() - start_time) * 1000\n",
    "        \n",
    "        # Log error\n",
    "        request_logger.log_request(\n",
    "            transaction_id=transaction.transaction_id,\n",
    "            endpoint=\"/api/v1/predict\",\n",
    "            method=\"POST\",\n",
    "            model_version=\"unknown\",\n",
    "            request_data=transaction.dict(),\n",
    "            response_data={\"error\": str(e)},\n",
    "            processing_time_ms=processing_time,\n",
    "            status_code=500\n",
    "        )\n",
    "        \n",
    "        raise\n",
    "\n",
    "@app.post(\"/api/v1/predict/batch\")\n",
    "async def predict_fraud_batch(transactions: List[TransactionRequest]):\n",
    "    \"\"\"\n",
    "    Predict fraud for multiple transactions.\n",
    "    \n",
    "    Optimized for batch processing with:\n",
    "    - Parallel processing\n",
    "    - Batch logging\n",
    "    - Progress tracking\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    batch_id = str(uuid.uuid4())\n",
    "    \n",
    "    logger.info(f\"📦 Processing batch {batch_id} with {len(transactions)} transactions\")\n",
    "    \n",
    "    predictions = []\n",
    "    errors = []\n",
    "    \n",
    "    # Process transactions (in production, would use async/parallel processing)\n",
    "    for i, transaction in enumerate(transactions):\n",
    "        try:\n",
    "            result = fraud_predictor.predict(transaction)\n",
    "            predictions.append(PredictionResponse(**result))\n",
    "            \n",
    "            if (i + 1) % 10 == 0:\n",
    "                logger.info(f\"  Processed {i + 1}/{len(transactions)} transactions\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            errors.append({\n",
    "                \"transaction_id\": transaction.transaction_id,\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "    \n",
    "    total_time = (time.time() - start_time) * 1000\n",
    "    \n",
    "    # Log batch request\n",
    "    request_logger.log_request(\n",
    "        transaction_id=batch_id,\n",
    "        endpoint=\"/api/v1/predict/batch\",\n",
    "        method=\"POST\",\n",
    "        model_version=version_manager.active_version or \"unknown\",\n",
    "        request_data={\"batch_size\": len(transactions)},\n",
    "        response_data={\n",
    "            \"batch_id\": batch_id,\n",
    "            \"predictions_count\": len(predictions),\n",
    "            \"errors_count\": len(errors)\n",
    "        },\n",
    "        processing_time_ms=total_time,\n",
    "        status_code=200 if not errors else 207  # 207 = Multi-Status\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"batch_id\": batch_id,\n",
    "        \"predictions\": predictions,\n",
    "        \"errors\": errors,\n",
    "        \"total_processing_time_ms\": total_time,\n",
    "        \"success_rate\": len(predictions) / len(transactions) if transactions else 0\n",
    "    }\n",
    "\n",
    "@app.get(\"/api/v1/models/versions\")\n",
    "async def list_model_versions():\n",
    "    \"\"\"List all model versions with metadata.\"\"\"\n",
    "    versions = version_manager.list_versions()\n",
    "    return {\n",
    "        \"versions\": [v.dict() for v in versions],\n",
    "        \"active_version\": version_manager.active_version,\n",
    "        \"total_versions\": len(versions)\n",
    "    }\n",
    "\n",
    "@app.post(\"/api/v1/models/activate/{version}\")\n",
    "async def activate_model_version(version: str):\n",
    "    \"\"\"Activate a specific model version.\"\"\"\n",
    "    success = version_manager.activate_version(version)\n",
    "    \n",
    "    if success:\n",
    "        return {\n",
    "            \"message\": f\"Model version {version} activated successfully\",\n",
    "            \"previous_version\": version_manager.active_version,\n",
    "            \"new_version\": version\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"error\": f\"Failed to activate version {version}\",\n",
    "            \"current_version\": version_manager.active_version\n",
    "        }\n",
    "\n",
    "@app.post(\"/api/v1/models/rollback\")\n",
    "async def rollback_model():\n",
    "    \"\"\"Rollback to previous model version.\"\"\"\n",
    "    current_version = version_manager.active_version\n",
    "    success = version_manager.rollback_to_previous()\n",
    "    \n",
    "    if success:\n",
    "        return {\n",
    "            \"message\": \"Model rolled back successfully\",\n",
    "            \"previous_version\": current_version,\n",
    "            \"current_version\": version_manager.active_version\n",
    "        }\n",
    "    else:\n",
    "        return {\"error\": \"Rollback failed\"}\n",
    "\n",
    "@app.get(\"/api/v1/health\")\n",
    "async def health_check():\n",
    "    \"\"\"Comprehensive health check endpoint.\"\"\"\n",
    "    model_data, version = version_manager.get_active_model()\n",
    "    metrics = request_logger.get_metrics()\n",
    "    \n",
    "    health_status = {\n",
    "        \"status\": \"healthy\" if model_data else \"degraded\",\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"active_model_version\": version,\n",
    "        \"api_version\": app.version,\n",
    "        \"metrics\": {\n",
    "            \"total_requests\": metrics.get(\"total_requests\", 0),\n",
    "            \"success_rate\": metrics.get(\"success_rate\", 0),\n",
    "            \"avg_processing_time_ms\": metrics.get(\"average_processing_time_ms\", 0)\n",
    "        },\n",
    "        \"components\": {\n",
    "            \"model_service\": \"healthy\" if model_data else \"unhealthy\",\n",
    "            \"database\": \"healthy\",  # Would check actual DB connection\n",
    "            \"cache\": \"healthy\"  # Would check cache status\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return health_status\n",
    "\n",
    "@app.post(\"/api/v1/deploy\")\n",
    "async def deploy_model_webhook(deployment_data: Dict[str, Any]):\n",
    "    \"\"\"\n",
    "    CI/CD webhook for automated model deployment.\n",
    "    \n",
    "    Expected payload:\n",
    "    {\n",
    "        \"version\": \"v1.2.0\",\n",
    "        \"model_url\": \"s3://bucket/model.joblib\",\n",
    "        \"description\": \"Improved model with new features\",\n",
    "        \"performance_metrics\": {\n",
    "            \"accuracy\": 0.95,\n",
    "            \"f1_score\": 0.87,\n",
    "            \"auc_roc\": 0.92\n",
    "        },\n",
    "        \"git_commit\": \"abc123\",\n",
    "        \"build_id\": \"jenkins-123\"\n",
    "    }\n",
    "    \"\"\"\n",
    "    try:\n",
    "        version = deployment_data.get(\"version\")\n",
    "        model_url = deployment_data.get(\"model_url\")\n",
    "        description = deployment_data.get(\"description\", \"\")\n",
    "        performance_metrics = deployment_data.get(\"performance_metrics\", {})\n",
    "        \n",
    "        if not version or not model_url:\n",
    "            return {\"error\": \"Version and model_url required\"}, 400\n",
    "        \n",
    "        logger.info(f\"🚀 Deploying model {version} from CI/CD\")\n",
    "        \n",
    "        # In production, would download model from URL\n",
    "        # For demo, use dummy model\n",
    "        model_data = {\n",
    "            \"models\": {\"deployed\": f\"model_{version}\"},\n",
    "            \"scaler\": f\"scaler_{version}\",\n",
    "            \"metadata\": deployment_data\n",
    "        }\n",
    "        \n",
    "        # Deploy model\n",
    "        success = version_manager.deploy_model(\n",
    "            model_data, version, description, performance_metrics\n",
    "        )\n",
    "        \n",
    "        if success:\n",
    "            # Auto-activate if performance is better than threshold\n",
    "            if performance_metrics.get(\"f1_score\", 0) > 0.85:\n",
    "                version_manager.activate_version(version)\n",
    "                logger.info(f\"✅ Auto-activated high-performing model {version}\")\n",
    "                \n",
    "                return {\n",
    "                    \"message\": f\"Model {version} deployed and activated\",\n",
    "                    \"auto_activated\": True,\n",
    "                    \"performance_metrics\": performance_metrics\n",
    "                }\n",
    "            else:\n",
    "                return {\n",
    "                    \"message\": f\"Model {version} deployed (not activated)\",\n",
    "                    \"auto_activated\": False,\n",
    "                    \"reason\": \"Performance below auto-activation threshold\",\n",
    "                    \"performance_metrics\": performance_metrics\n",
    "                }\n",
    "        else:\n",
    "            return {\"error\": \"Deployment failed\"}, 500\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Deployment webhook failed: {e}\")\n",
    "        return {\"error\": str(e)}, 500\n",
    "\n",
    "print(\"🚀 Enhanced API endpoints defined!\")\n",
    "print(f\"\\n📋 Available endpoints ({len(app.routes)}):\")\n",
    "for route in sorted(app.routes.keys()):\n",
    "    print(f\"  - {route}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Implementing Monitoring and Observability\n",
    "\n",
    "Let's add comprehensive monitoring capabilities for production use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonitoringService:\n",
    "    \"\"\"\n",
    "    Comprehensive monitoring service for the API.\n",
    "    \n",
    "    Features:\n",
    "    - Performance metrics\n",
    "    - Error tracking\n",
    "    - Model drift detection\n",
    "    - Business metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, request_logger: RequestLogger):\n",
    "        self.request_logger = request_logger\n",
    "        self.metrics_cache = {}\n",
    "        self.alerts = []\n",
    "    \n",
    "    def get_performance_metrics(self, time_window_hours: int = 24) -> Dict:\n",
    "        \"\"\"\n",
    "        Get comprehensive performance metrics.\n",
    "        \"\"\"\n",
    "        cutoff_time = datetime.now() - timedelta(hours=time_window_hours)\n",
    "        \n",
    "        # Get recent logs\n",
    "        logs = self.request_logger.get_request_logs(limit=10000)\n",
    "        recent_logs = [\n",
    "            log for log in logs \n",
    "            if datetime.fromisoformat(log['timestamp']) > cutoff_time\n",
    "        ]\n",
    "        \n",
    "        if not recent_logs:\n",
    "            return {\"error\": \"No recent data available\"}\n",
    "        \n",
    "        # Calculate metrics\n",
    "        total_requests = len(recent_logs)\n",
    "        successful_requests = sum(1 for log in recent_logs if log['status_code'] == 200)\n",
    "        failed_requests = total_requests - successful_requests\n",
    "        \n",
    "        processing_times = [log['processing_time_ms'] for log in recent_logs]\n",
    "        \n",
    "        # Percentile calculations\n",
    "        p50 = np.percentile(processing_times, 50)\n",
    "        p95 = np.percentile(processing_times, 95)\n",
    "        p99 = np.percentile(processing_times, 99)\n",
    "        \n",
    "        # Error analysis\n",
    "        error_codes = {}\n",
    "        for log in recent_logs:\n",
    "            if log['status_code'] != 200:\n",
    "                code = log['status_code']\n",
    "                error_codes[code] = error_codes.get(code, 0) + 1\n",
    "        \n",
    "        return {\n",
    "            \"time_window_hours\": time_window_hours,\n",
    "            \"total_requests\": total_requests,\n",
    "            \"successful_requests\": successful_requests,\n",
    "            \"failed_requests\": failed_requests,\n",
    "            \"success_rate\": successful_requests / total_requests,\n",
    "            \"processing_time_ms\": {\n",
    "                \"mean\": np.mean(processing_times),\n",
    "                \"median\": p50,\n",
    "                \"p95\": p95,\n",
    "                \"p99\": p99,\n",
    "                \"min\": np.min(processing_times),\n",
    "                \"max\": np.max(processing_times)\n",
    "            },\n",
    "            \"requests_per_hour\": total_requests / time_window_hours,\n",
    "            \"error_breakdown\": error_codes\n",
    "        }\n",
    "    \n",
    "    def get_business_metrics(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Calculate business-relevant metrics.\n",
    "        \"\"\"\n",
    "        logs = self.request_logger.get_request_logs(limit=1000)\n",
    "        \n",
    "        fraud_predictions = 0\n",
    "        total_amount_analyzed = 0\n",
    "        high_risk_transactions = 0\n",
    "        \n",
    "        for log in logs:\n",
    "            try:\n",
    "                response_data = json.loads(log['response_data'])\n",
    "                \n",
    "                if response_data.get('is_fraud'):\n",
    "                    fraud_predictions += 1\n",
    "                \n",
    "                if response_data.get('risk_score', 0) > 70:\n",
    "                    high_risk_transactions += 1\n",
    "                \n",
    "                request_data = json.loads(log['request_data'])\n",
    "                total_amount_analyzed += request_data.get('amount', 0)\n",
    "                \n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        return {\n",
    "            \"total_transactions_analyzed\": len(logs),\n",
    "            \"fraud_predictions\": fraud_predictions,\n",
    "            \"fraud_rate\": fraud_predictions / len(logs) if logs else 0,\n",
    "            \"high_risk_transactions\": high_risk_transactions,\n",
    "            \"total_amount_analyzed\": total_amount_analyzed,\n",
    "            \"average_transaction_amount\": total_amount_analyzed / len(logs) if logs else 0\n",
    "        }\n",
    "    \n",
    "    def check_model_health(self, version_manager: ModelVersionManager) -> Dict:\n",
    "        \"\"\"\n",
    "        Check model health and drift indicators.\n",
    "        \"\"\"\n",
    "        model_data, version = version_manager.get_active_model()\n",
    "        \n",
    "        if not model_data:\n",
    "            return {\"status\": \"unhealthy\", \"reason\": \"No active model\"}\n",
    "        \n",
    "        # Get recent prediction patterns\n",
    "        logs = self.request_logger.get_request_logs(limit=500)\n",
    "        recent_fraud_rate = sum(\n",
    "            1 for log in logs \n",
    "            if json.loads(log.get('response_data', '{}')).get('is_fraud', False)\n",
    "        ) / len(logs) if logs else 0\n",
    "        \n",
    "        # Check for anomalies\n",
    "        health_issues = []\n",
    "        \n",
    "        if recent_fraud_rate > 0.2:  # More than 20% fraud predictions\n",
    "            health_issues.append({\n",
    "                \"issue\": \"High fraud rate\",\n",
    "                \"severity\": \"warning\",\n",
    "                \"value\": f\"{recent_fraud_rate:.2%}\",\n",
    "                \"threshold\": \"20%\"\n",
    "            })\n",
    "        \n",
    "        if recent_fraud_rate < 0.001:  # Less than 0.1% fraud predictions\n",
    "            health_issues.append({\n",
    "                \"issue\": \"Low fraud rate\",\n",
    "                \"severity\": \"warning\",\n",
    "                \"value\": f\"{recent_fraud_rate:.2%}\",\n",
    "                \"threshold\": \"0.1%\"\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"healthy\" if not health_issues else \"warning\",\n",
    "            \"active_model_version\": version,\n",
    "            \"recent_fraud_rate\": recent_fraud_rate,\n",
    "            \"health_issues\": health_issues,\n",
    "            \"last_check\": datetime.now().isoformat()\n",
    "        }\n",
    "    \n",
    "    def generate_alert(self, alert_type: str, message: str, severity: str = \"info\"):\n",
    "        \"\"\"\n",
    "        Generate monitoring alert.\n",
    "        \"\"\"\n",
    "        alert = {\n",
    "            \"alert_id\": str(uuid.uuid4()),\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"type\": alert_type,\n",
    "            \"message\": message,\n",
    "            \"severity\": severity\n",
    "        }\n",
    "        \n",
    "        self.alerts.append(alert)\n",
    "        \n",
    "        # In production, would send to alerting system\n",
    "        logger.warning(f\"🚨 Alert: {alert_type} - {message} (Severity: {severity})\")\n",
    "        \n",
    "        return alert\n",
    "\n",
    "# Demo monitoring service\n",
    "print(\"📊 Monitoring Service implemented!\")\n",
    "\n",
    "monitoring_service = MonitoringService(request_logger)\n",
    "\n",
    "# Generate some test data\n",
    "for i in range(20):\n",
    "    is_fraud = i % 5 == 0\n",
    "    request_logger.log_request(\n",
    "        transaction_id=f\"monitor_test_{i}\",\n",
    "        endpoint=\"/api/v1/predict\",\n",
    "        method=\"POST\",\n",
    "        model_version=\"v0.1.0-test\",\n",
    "        request_data={\"amount\": 100 + i * 50},\n",
    "        response_data={\n",
    "            \"is_fraud\": is_fraud,\n",
    "            \"risk_score\": 80 if is_fraud else 20\n",
    "        },\n",
    "        processing_time_ms=10 + np.random.normal(5, 2),\n",
    "        status_code=200\n",
    "    )\n",
    "\n",
    "# Get monitoring metrics\n",
    "perf_metrics = monitoring_service.get_performance_metrics()\n",
    "biz_metrics = monitoring_service.get_business_metrics()\n",
    "model_health = monitoring_service.check_model_health(version_manager)\n",
    "\n",
    "print(\"\\n📈 Performance Metrics:\")\n",
    "print(f\"  - Success Rate: {perf_metrics['success_rate']:.2%}\")\n",
    "print(f\"  - Processing Time (p95): {perf_metrics['processing_time_ms']['p95']:.2f}ms\")\n",
    "print(f\"  - Requests per Hour: {perf_metrics['requests_per_hour']:.1f}\")\n",
    "\n",
    "print(\"\\n💼 Business Metrics:\")\n",
    "print(f\"  - Fraud Rate: {biz_metrics['fraud_rate']:.2%}\")\n",
    "print(f\"  - High Risk Transactions: {biz_metrics['high_risk_transactions']}\")\n",
    "\n",
    "print(\"\\n🏥 Model Health:\")\n",
    "print(f\"  - Status: {model_health['status']}\")\n",
    "print(f\"  - Recent Fraud Rate: {model_health['recent_fraud_rate']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Best Practices and Production Considerations\n",
    "\n",
    "Let's explore key considerations for production deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production best practices demonstration\n",
    "\n",
    "class ProductionConfig:\n",
    "    \"\"\"\n",
    "    Production configuration and best practices.\n",
    "    \"\"\"\n",
    "    \n",
    "    # API Configuration\n",
    "    API_TITLE = \"Fraud Detection API\"\n",
    "    API_VERSION = \"2.0.0\"\n",
    "    API_PREFIX = \"/api/v1\"\n",
    "    \n",
    "    # Model Configuration\n",
    "    MODEL_CACHE_SIZE = 3  # Keep last 3 models in memory\n",
    "    MODEL_AUTO_ACTIVATE_THRESHOLD = 0.85  # F1 score threshold\n",
    "    MODEL_ROLLBACK_THRESHOLD = 0.70  # Auto-rollback if performance drops\n",
    "    \n",
    "    # Request Configuration\n",
    "    MAX_BATCH_SIZE = 1000\n",
    "    REQUEST_TIMEOUT_SECONDS = 30\n",
    "    RATE_LIMIT_PER_MINUTE = 1000\n",
    "    \n",
    "    # Monitoring Configuration\n",
    "    HEALTH_CHECK_INTERVAL_SECONDS = 60\n",
    "    METRICS_RETENTION_DAYS = 30\n",
    "    ALERT_COOLDOWN_MINUTES = 15\n",
    "    \n",
    "    # Security Configuration\n",
    "    ENABLE_API_KEY_AUTH = True\n",
    "    ENABLE_REQUEST_SIGNING = True\n",
    "    ALLOWED_ORIGINS = [\"https://fraud-dashboard.company.com\"]\n",
    "    \n",
    "    # Logging Configuration\n",
    "    LOG_LEVEL = \"INFO\"\n",
    "    LOG_FORMAT = \"json\"  # Structured logging\n",
    "    LOG_SENSITIVE_DATA = False  # Never log PII\n",
    "    \n",
    "    # Database Configuration\n",
    "    DB_POOL_SIZE = 10\n",
    "    DB_MAX_OVERFLOW = 20\n",
    "    DB_POOL_TIMEOUT = 30\n",
    "    \n",
    "    # S3 Configuration\n",
    "    S3_BUCKET = \"fraud-detection-logs\"\n",
    "    S3_RETENTION_DAYS = 90\n",
    "    S3_ENCRYPTION = \"AES256\"\n",
    "\n",
    "print(\"⚙️ Production Configuration defined!\")\n",
    "\n",
    "# Security best practices\n",
    "class SecurityUtils:\n",
    "    \"\"\"\n",
    "    Security utilities for production API.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def validate_api_key(api_key: str) -> bool:\n",
    "        \"\"\"\n",
    "        Validate API key.\n",
    "        \n",
    "        In production:\n",
    "        - Check against database\n",
    "        - Verify expiration\n",
    "        - Check rate limits\n",
    "        \"\"\"\n",
    "        # Simple validation for demo\n",
    "        return len(api_key) == 32 and api_key.isalnum()\n",
    "    \n",
    "    @staticmethod\n",
    "    def sanitize_input(data: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Sanitize input data.\n",
    "        \n",
    "        - Remove potential XSS\n",
    "        - Validate data types\n",
    "        - Check for injection attempts\n",
    "        \"\"\"\n",
    "        sanitized = {}\n",
    "        \n",
    "        for key, value in data.items():\n",
    "            if isinstance(value, str):\n",
    "                # Remove potential harmful characters\n",
    "                value = value.replace(\"<\", \"\").replace(\">\", \"\")\n",
    "                value = value.replace(\"script\", \"\")\n",
    "            \n",
    "            sanitized[key] = value\n",
    "        \n",
    "        return sanitized\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_request_signature(request_data: Dict, secret: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate request signature for verification.\n",
    "        \"\"\"\n",
    "        data_str = json.dumps(request_data, sort_keys=True)\n",
    "        return hashlib.sha256(f\"{data_str}{secret}\".encode()).hexdigest()\n",
    "\n",
    "print(\"🔒 Security utilities implemented!\")\n",
    "\n",
    "# Deployment checklist\n",
    "deployment_checklist = \"\"\"\n",
    "📋 Production Deployment Checklist:\n",
    "\n",
    "1. Infrastructure:\n",
    "   ✓ Load balancer configured\n",
    "   ✓ Auto-scaling groups set up\n",
    "   ✓ Health checks configured\n",
    "   ✓ SSL/TLS certificates installed\n",
    "\n",
    "2. Security:\n",
    "   ✓ API authentication enabled\n",
    "   ✓ Rate limiting configured\n",
    "   ✓ CORS properly set up\n",
    "   ✓ Input validation active\n",
    "   ✓ Secrets in secure vault\n",
    "\n",
    "3. Monitoring:\n",
    "   ✓ Application monitoring (APM)\n",
    "   ✓ Log aggregation set up\n",
    "   ✓ Alerts configured\n",
    "   ✓ Dashboards created\n",
    "   ✓ SLO/SLA defined\n",
    "\n",
    "4. Database:\n",
    "   ✓ Connection pooling\n",
    "   ✓ Read replicas for queries\n",
    "   ✓ Backup strategy\n",
    "   ✓ Migration scripts ready\n",
    "\n",
    "5. Model Management:\n",
    "   ✓ Model versioning system\n",
    "   ✓ A/B testing capability\n",
    "   ✓ Rollback procedures\n",
    "   ✓ Performance benchmarks\n",
    "\n",
    "6. CI/CD:\n",
    "   ✓ Automated tests\n",
    "   ✓ Deployment pipeline\n",
    "   ✓ Rollback automation\n",
    "   ✓ Environment promotion\n",
    "\n",
    "7. Documentation:\n",
    "   ✓ API documentation\n",
    "   ✓ Runbooks created\n",
    "   ✓ Architecture diagrams\n",
    "   ✓ Incident response plan\n",
    "\"\"\"\n",
    "\n",
    "print(deployment_checklist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Build Your Own API Features\n",
    "\n",
    "Try implementing these additional features:\n",
    "\n",
    "1. **Feature Store Integration**\n",
    "   - Add real-time feature computation\n",
    "   - Implement feature caching\n",
    "   - Add feature versioning\n",
    "\n",
    "2. **A/B Testing Framework**\n",
    "   - Route traffic to different models\n",
    "   - Track performance metrics\n",
    "   - Implement statistical significance testing\n",
    "\n",
    "3. **Advanced Monitoring**\n",
    "   - Add custom business metrics\n",
    "   - Implement anomaly detection\n",
    "   - Create alerting rules\n",
    "\n",
    "4. **Model Governance**\n",
    "   - Add model approval workflow\n",
    "   - Implement audit trail\n",
    "   - Add compliance checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise template - A/B Testing Framework\n",
    "\n",
    "class ABTestingFramework:\n",
    "    \"\"\"\n",
    "    Implement A/B testing for model comparison.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.experiments = {}\n",
    "        self.results = {}\n",
    "    \n",
    "    def create_experiment(self, \n",
    "                         experiment_id: str,\n",
    "                         control_version: str,\n",
    "                         treatment_version: str,\n",
    "                         traffic_split: float = 0.5):\n",
    "        \"\"\"\n",
    "        Create new A/B test experiment.\n",
    "        \n",
    "        TODO: Implement experiment creation\n",
    "        - Validate versions exist\n",
    "        - Set up traffic routing\n",
    "        - Initialize metrics tracking\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def route_request(self, experiment_id: str, user_id: str) -> str:\n",
    "        \"\"\"\n",
    "        Determine which model version to use for a request.\n",
    "        \n",
    "        TODO: Implement traffic routing\n",
    "        - Use consistent hashing for user assignment\n",
    "        - Respect traffic split\n",
    "        - Handle experiment not found\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def record_result(self, \n",
    "                     experiment_id: str,\n",
    "                     version: str,\n",
    "                     prediction_correct: bool,\n",
    "                     processing_time: float):\n",
    "        \"\"\"\n",
    "        Record experiment result.\n",
    "        \n",
    "        TODO: Implement result recording\n",
    "        - Update metrics\n",
    "        - Calculate running statistics\n",
    "        - Check for statistical significance\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_experiment_results(self, experiment_id: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Get current experiment results with statistical analysis.\n",
    "        \n",
    "        TODO: Implement results analysis\n",
    "        - Calculate conversion rates\n",
    "        - Compute confidence intervals\n",
    "        - Determine winner (if any)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "print(\"🧪 A/B Testing Framework template created!\")\n",
    "print(\"\\nTry implementing the TODO methods to add A/B testing capabilities!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "In this comprehensive tutorial, we've built a production-grade fraud detection API with:\n",
    "\n",
    "### 🎯 Key Features Implemented\n",
    "\n",
    "1. **Model Version Management**\n",
    "   - Deploy multiple versions\n",
    "   - Blue-green deployments\n",
    "   - Automatic rollback\n",
    "   - Performance-based activation\n",
    "\n",
    "2. **Comprehensive Logging**\n",
    "   - Request/response logging\n",
    "   - Database storage\n",
    "   - S3 archival support\n",
    "   - Queryable metrics\n",
    "\n",
    "3. **CI/CD Integration**\n",
    "   - Deployment webhooks\n",
    "   - Automated model updates\n",
    "   - Performance validation\n",
    "   - Build metadata tracking\n",
    "\n",
    "4. **Advanced Monitoring**\n",
    "   - Performance metrics\n",
    "   - Business KPIs\n",
    "   - Model health checks\n",
    "   - Alerting system\n",
    "\n",
    "5. **Production Features**\n",
    "   - Batch predictions\n",
    "   - Request caching\n",
    "   - Health endpoints\n",
    "   - Comprehensive error handling\n",
    "\n",
    "### 💡 Best Practices Covered\n",
    "\n",
    "- **Security**: API authentication, input validation, request signing\n",
    "- **Reliability**: Health checks, circuit breakers, timeouts\n",
    "- **Scalability**: Batch processing, caching, async operations\n",
    "- **Observability**: Structured logging, metrics, distributed tracing\n",
    "- **Maintainability**: Clear documentation, versioning, configuration management\n",
    "\n",
    "### 🚀 Next Steps\n",
    "\n",
    "1. **Deploy to Cloud**\n",
    "   - Containerize with Docker\n",
    "   - Deploy to Kubernetes\n",
    "   - Set up cloud infrastructure\n",
    "\n",
    "2. **Add Advanced Features**\n",
    "   - Real-time streaming predictions\n",
    "   - Multi-model ensembles\n",
    "   - Feature store integration\n",
    "\n",
    "3. **Enhance Monitoring**\n",
    "   - Add Prometheus metrics\n",
    "   - Integrate with Grafana\n",
    "   - Set up PagerDuty alerts\n",
    "\n",
    "4. **Improve Security**\n",
    "   - Add OAuth2 authentication\n",
    "   - Implement rate limiting\n",
    "   - Add request encryption\n",
    "\n",
    "This completes our journey through building a production-ready fraud detection system! You now have the knowledge to build, deploy, and maintain ML APIs in production environments.\n",
    "\n",
    "Happy coding! 🎉"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}